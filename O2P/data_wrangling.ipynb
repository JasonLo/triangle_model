{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R9fTvY-sMpyh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/tf/O2P\n",
      "basicOP_master.ipynb   data_wrangling.py  models\t\t __pycache__\n",
      "basicOSP_master.ipynb  input\t\t  my_eval.py\n",
      "custom_layer.py        misc.py\t\t  pho_task_master.ipynb\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "%cd '/home/jupyter/tf/O2P'\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_wrangling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_wrangling.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def gen_pkey(p_file=\"patterns/mapping_v2.txt\"):\n",
    "    \n",
    "    # read phonological patterns from the mapping file\n",
    "    # See Harm & Seidenberg PDF file\n",
    "    mapping = pd.read_table(p_file, header=None, delim_whitespace=True)\n",
    "    m_dict = mapping.set_index(0).T.to_dict('list')\n",
    "    return m_dict\n",
    "\n",
    "def data_wrangling():\n",
    "\n",
    "    # Read training file\n",
    "    train_file = 'patterns/6ktraining.dict'\n",
    "\n",
    "    strain_file = 'patterns/strain.txt'  \n",
    "    strain_key_file='patterns/strain_key.txt'\n",
    "\n",
    "    grain_file = 'patterns/grain_nws.dict'\n",
    "    grain_key_file='patterns/grain_key.txt'\n",
    "\n",
    "\n",
    "    train = pd.read_csv(train_file,sep='\\t', header=None, names=['word', 'ort', 'pho', 'wf'])\n",
    "\n",
    "    strain = pd.read_csv(strain_file,sep='\\t', header=None, names=['word', 'ort', 'pho', 'wf'])\n",
    "    strain_key = pd.read_table(strain_key_file, header=None, delim_whitespace=True, names=['word', 'frequency', 'pho_consistency', 'imageability'])\n",
    "    df_strain = pd.merge(strain, strain_key)\n",
    "\n",
    "    grain = pd.read_csv(grain_file,sep='\\t', header=None, names=['word', 'ort', 'pho_large', 'pho_small'])\n",
    "    grain_key = pd.read_table(grain_key_file, header=None, delim_whitespace=True, names=['word', 'condition'])\n",
    "    grain_key['condition'] = np.where(grain_key['condition']=='critical', 'ambiguous', 'unambiguous')\n",
    "    df_grain = pd.merge(grain, grain_key)\n",
    "\n",
    "    def prepDF(t):\n",
    "        # The first bit and last 3 bits are empty in this dataset\n",
    "        t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "        return t\n",
    "\n",
    "    df_train = prepDF(train)\n",
    "    df_strain = prepDF(df_strain)\n",
    "    df_grain = prepDF(df_grain)\n",
    "\n",
    "    df_train.to_csv('input/df_train.csv')\n",
    "    df_strain.to_csv('input/df_strain.csv')\n",
    "    df_grain.to_csv('input/df_grain.csv')\n",
    "\n",
    "    print(df_train.head)\n",
    "    print(df_strain.head)\n",
    "    print(df_grain.head)\n",
    "\n",
    "    # Encode orthographic representation\n",
    "    def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "        # Replicating support.py (o_char)\n",
    "        # This function wrap tokenizer.texts_to_matrix to fit on multiple \n",
    "        # independent slot-based input\n",
    "        # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "        nSlot = len(o_col[0])\n",
    "        nWord = len(o_col)\n",
    "\n",
    "        slotData = nWord*[None]\n",
    "        binData = pd.DataFrame()\n",
    "\n",
    "        for slotId in range(nSlot):\n",
    "            for wordId in range(nWord):\n",
    "                slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "            t = Tokenizer(filters='', lower=False)\n",
    "            t.fit_on_texts(slotData)\n",
    "            seqData = t.texts_to_sequences(slotData)  # Maybe just use sequence data later\n",
    "            \n",
    "            # Triming first bit in each slot\n",
    "            if trimMode == True:\n",
    "                tmp = t.texts_to_matrix(slotData)\n",
    "                thisSlotBinData = tmp[:,1::]   # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "            elif trimMode == False:\n",
    "                thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "            # Print dictionary details\n",
    "            if verbose == True:\n",
    "                print('In slot ', slotId, '\\t')\n",
    "                print('token count:', t.word_counts)\n",
    "                print('word count:', t.document_count)\n",
    "                print('dictionary:', t.word_index)\n",
    "                print('token appear in how many words:', t.word_docs)\n",
    "\n",
    "            # Put binary data into a dataframe\n",
    "            binData = pd.concat([binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True)\n",
    "\n",
    "        return binData\n",
    "\n",
    "    def ort2bin_v2(o_col):\n",
    "        # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "        from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "        t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "        t.fit_on_texts(o_col)\n",
    "        print('dictionary:', t.word_index)\n",
    "        return t.texts_to_matrix(o_col)\n",
    "\n",
    "    # Merge all 3 ortho representation\n",
    "    all_ort = pd.concat([df_train.ort, df_strain.ort, df_grain.ort], ignore_index=True)\n",
    "\n",
    "    # Encoding orthographic representation\n",
    "    all_ort_bin     = ort2bin(all_ort, verbose=False)\n",
    "    splitId_strain  = len(df_train)\n",
    "    splitId_grain   = len(df_train) + len(df_strain)\n",
    "\n",
    "    x_train     = np.array(all_ort_bin[0:splitId_strain])\n",
    "    x_strain    = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "    x_grain     = np.array(all_ort_bin[splitId_grain::])\n",
    "\n",
    "    # Save to disk\n",
    "    np.savez_compressed('input/x_train.npz', data = x_train)\n",
    "    np.savez_compressed('input/x_strain.npz', data = x_strain)\n",
    "    np.savez_compressed('input/x_grain.npz', data = x_grain)\n",
    "\n",
    "    print('==========Orthographic representation==========')\n",
    "    print('all shape:', all_ort_bin.shape)\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('x_strain shape:', x_strain.shape)\n",
    "    print('x_grain shape:', x_grain.shape)\n",
    "\n",
    "    # Encode phonological representation\n",
    "\n",
    "    def pho2bin(p_col, p_key):\n",
    "        # Was called p_pattern in original support script\n",
    "        phoneme_len = len(p_key[\"p\"])  # just chose an item at random for the length as they are all equal\n",
    "        p_output = np.zeros((len(p_col), len(p_col[0])*phoneme_len))  # same true for item 0 here\n",
    "\n",
    "        # iterate through items\n",
    "        for i in range(0, len(p_col)):\n",
    "            word_now = list(p_col[i])  # separate input into characters\n",
    "            whole_word = []\n",
    "\n",
    "            # loop through characters\n",
    "            for j in word_now:  # convert char to mapping\n",
    "                slot = p_key[j]\n",
    "                whole_word.append(slot)  # append to the item\n",
    "\n",
    "            p_output[i] = np.concatenate(whole_word)\n",
    "        return p_output\n",
    "\n",
    "    def pho2bin_v2(p_col, p_key):\n",
    "        # Vectorize for performance (that no one ask for... )\n",
    "        binLength = len(p_key['_'])\n",
    "        n = len(p_col)\n",
    "        nPhoChar = len(p_col[0])\n",
    "\n",
    "        p_output = np.empty([n, binLength*nPhoChar])\n",
    "\n",
    "        for slot in range(len(p_col[0])):\n",
    "            slotSeries = p_col.str.slice(start=slot, stop=slot+1)\n",
    "            outSeries = slotSeries.map(p_key)\n",
    "            p_output[:,range(slot*25, (slot+1)*25)] = outSeries.to_list()\n",
    "        return p_output\n",
    "    \n",
    "    phon_key = gen_pkey()\n",
    "    y_train = pho2bin_v2(train.pho, phon_key)\n",
    "    y_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "    y_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "    y_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "\n",
    "    # Save to disk\n",
    "    np.savez_compressed('input/y_train.npz', data = y_train)\n",
    "    np.savez_compressed('input/y_strain.npz', data = y_strain)\n",
    "    np.savez_compressed('input/y_large_grain.npz', data = y_large_grain)\n",
    "    np.savez_compressed('input/y_small_grain.npz', data = y_small_grain)\n",
    "\n",
    "    print('\\n==========Phonological representation==========')\n",
    "    print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_strain shape:', y_strain.shape)\n",
    "    print('y_large_grain shape:', y_large_grain.shape)\n",
    "    print('y_small_grain shape:', y_small_grain.shape)\n",
    "\n",
    "# data_wrangling()\n",
    "class wfManager():\n",
    "    import numpy as np\n",
    "    # Note: the probability must sum to 1 when passing it to np.random.choice()\n",
    "    def __init__(self, wf):\n",
    "        self.wf = np.array(wf)\n",
    "\n",
    "    def wf(self): return self.wf\n",
    "\n",
    "    def to_p(self, x): return x/np.sum(x)\n",
    "\n",
    "    def samp_termf(self):\n",
    "        return self.to_p(self.wf)\n",
    "\n",
    "    def samp_log(self):\n",
    "        log = np.log(1+self.wf)\n",
    "        return self.to_p(log)\n",
    "\n",
    "    def samp_hs04(self):\n",
    "        root = np.sqrt(self.wf)/np.sqrt(30000)\n",
    "        clip = root.clip(0.05,1.0)\n",
    "        return self.to_p(clip)\n",
    "\n",
    "    def samp_jay(self):\n",
    "        cap = self.wf.clip(0, 10000)\n",
    "        root = np.sqrt(cap)\n",
    "        return self.to_p(root)\n",
    "\n",
    "def sampleGenerator(x_set, y_set, n_timesteps, batch_size, sample_p, rng_seed):\n",
    "# Get <batch_size> of data from <x_set>, <y_set> based on the probability of <sample_p>\n",
    "    np.random.seed(rng_seed)\n",
    "    while 1:\n",
    "        idx = np.random.choice(range(len(sample_p)), batch_size, p=sample_p)\n",
    "        batch_x = x_set[idx]\n",
    "        batch_y = []\n",
    "        \n",
    "        for i in range(n_timesteps):\n",
    "            batch_y.append(y_set[idx])\n",
    "        yield (batch_x, batch_y)\n",
    "\n",
    "def sample_generator_with_semantic(cfg, data):\n",
    "    # Get <batch_size> of data from <x_set>, <y_set> based on the probability of <sample_p>\n",
    "    from data_wrangling import semantic_influence\n",
    "    import tensorflow as tf \n",
    "    \n",
    "    np.random.seed(cfg.sample_rng_seed)\n",
    "    t = 0\n",
    "    batch = 0\n",
    "    \n",
    "    while True:\n",
    "        batch += 1\n",
    "        idx = np.random.choice(range(len(data.sample_p)), cfg.batch_size, p=data.sample_p)\n",
    "        batch_x = data.x_train[idx]\n",
    "        \n",
    "        batch_y = []\n",
    "        for i in range(cfg.n_timesteps):\n",
    "            batch_y.append(data.y_train[idx])\n",
    "            \n",
    "        input_s_cell = semantic_influence(t, data.wf[idx])\n",
    "        input_s = tf.tile(tf.expand_dims(input_s_cell, 1), [1, cfg.pho_units])\n",
    "        if batch % cfg.steps_per_epoch == 0: t += 1  # Recording time for semantic ramp up\n",
    "        # print('Time = {}'.format(t)) \n",
    "        \n",
    "        yield ([batch_x, input_s], batch_y)\n",
    "        \n",
    "        \n",
    "class my_data():\n",
    "    def __init__(self, cfg):\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "\n",
    "        self.sample_name = cfg.sample_name\n",
    "\n",
    "        self.df_train = pd.read_csv('input/df_train.csv', index_col=0)\n",
    "        self.df_strain = pd.read_csv('input/df_strain.csv', index_col=0)\n",
    "        self.df_grain = pd.read_csv('input/df_grain.csv', index_col=0)\n",
    "\n",
    "        self.x_train = np.load('input/x_train.npz')['data']\n",
    "        self.x_strain = np.load('input/x_strain.npz')['data']\n",
    "        self.x_grain = np.load('input/x_grain.npz')['data']\n",
    "        self.x_grain_wf = np.array(self.df_strain['wf'])\n",
    "        \n",
    "        self.y_train = np.load('input/y_train.npz')['data']\n",
    "        self.y_strain = np.load('input/y_strain.npz')['data']\n",
    "        self.y_large_grain = np.load('input/y_large_grain.npz')['data']\n",
    "        self.y_small_grain = np.load('input/y_small_grain.npz')['data']\n",
    "\n",
    "        from data_wrangling import gen_pkey\n",
    "        self.phon_key = gen_pkey('input/mapping_v2.txt')\n",
    "\n",
    "        print('==========Orthographic representation==========')\n",
    "        print('x_train shape:', self.x_train.shape)\n",
    "        print('x_strain shape:', self.x_strain.shape)\n",
    "        print('x_grain shape:', self.x_grain.shape)\n",
    "\n",
    "        print('\\n==========Phonological representation==========')\n",
    "        print(len(self.phon_key), ' phonemes: ', self.phon_key.keys())\n",
    "        print('y_train shape:', self.y_train.shape)\n",
    "        print('y_strain shape:', self.y_strain.shape)\n",
    "        print('y_large_grain shape:', self.y_large_grain.shape)\n",
    "        print('y_small_grain shape:', self.y_small_grain.shape)\n",
    "\n",
    "        self.gen_sample_p()\n",
    "        self.wf = np.array(self.df_train['wf'], dtype='float32')\n",
    "\n",
    "    def gen_sample_p(self):\n",
    "        from data_wrangling import wfManager\n",
    "        wf = wfManager(self.df_train['wf'])\n",
    "\n",
    "        if self.sample_name == 'hs04': self.sample_p = wf.samp_hs04()\n",
    "        if self.sample_name == 'jay': self.sample_p = wf.samp_jay()\n",
    "        if self.sample_name == 'log': self.sample_p = wf.samp_log()\n",
    "\n",
    "            \n",
    "def bq_conn(pid='idyllic-web-267716'):\n",
    "    from google.oauth2 import service_account\n",
    "    project_id = pid\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "                 '../bq_credential.json')\n",
    "    \n",
    "def write_cfg_to_bq(cfg):\n",
    "    import pandas as pd\n",
    "    import pandas_gbq\n",
    "\n",
    "    pandas_gbq.to_gbq(pd.DataFrame([cfg.cfg_dict]), \n",
    "                      destination_table='batch_test.cfg2', \n",
    "                      project_id='idyllic-web-267716', \n",
    "                      if_exists='append')\n",
    "        \n",
    "def semantic_influence(t, f, g=5., k=2000.):\n",
    "    import tensorflow as tf\n",
    "    lt = tf.math.multiply(tf.math.log(tf.math.add(f,2.)),t)\n",
    "    return tf.math.divide(tf.math.multiply(g,lt),tf.math.add(lt,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('input/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "from data_wrangling import wfManager\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "sortwf = wfManager(plot_f['wf'])\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(sortwf.wf,  sortwf.samp_log(), label='HS04')\n",
    "line2, = ax.plot(sortwf.wf,  sortwf.samp_hs04(), label='JAY')\n",
    "line3, = ax.plot(sortwf.wf,  sortwf.samp_jay(), label='LOG')\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
