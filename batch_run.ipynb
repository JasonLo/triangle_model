{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import os, multiprocessing, time, papermill, sys\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sys.path.append(\"/home/jupyter/tf/src\")\n",
    "from meta import ModelConfig, make_batch_cfg, parse_batch_results\n",
    "\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = \"chang_ssr_replicate\"\n",
    "batch_output_dir = f\"models/batch_run/{batch_name}/\"\n",
    "\n",
    "# Gen OG random seed: [int(random.random() * 1e5) for x in range(10)]\n",
    "\n",
    "code_name = \"chang19_ssr_1k_test\"\n",
    "\n",
    "param_grid = {\n",
    "    \"oral_vocab_size\": [1000, 2000, 3000, 4000, 5000, 6000],\n",
    "    \"n_mil_sample\": [1.0, 2.0, 3.0, 4.0, 5.0, 6.0],\n",
    "}\n",
    "\n",
    "static_hpar = {\n",
    "    \"tf_root\": \"/home/jupyter/tf\",\n",
    "    \"ort_units\": 119,\n",
    "    \"pho_units\": 250,\n",
    "    \"sem_units\": 2446,\n",
    "    \"hidden_os_units\": 500,\n",
    "    \"hidden_op_units\": 100,\n",
    "    \"hidden_ps_units\": 500,\n",
    "    \"hidden_sp_units\": 500,\n",
    "    \"pho_cleanup_units\": 50,\n",
    "    \"sem_cleanup_units\": 50,\n",
    "    \"pho_noise_level\": 0.0,\n",
    "    \"sem_noise_level\": 0.0,\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"tau\": 1 / 3,\n",
    "    \"max_unit_time\": 4.0,\n",
    "    \"output_ticks\": 4,\n",
    "    \"rng_seed\": 53797,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"batch_size\": 100,\n",
    "    \"save_freq\": 10,\n",
    "    \"batch_name\": batch_name,\n",
    "}\n",
    "\n",
    "batch_cfgs = make_batch_cfg(\n",
    "    batch_name, batch_output_dir, static_hpar, param_grid, \"hs04_master.ipynb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel run\n",
    "- Can run on Jupyter Lab\n",
    "- Cannot run on VSCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "def run_batch(cfg):\n",
    "    \"\"\"\n",
    "    Using papermill to run parameterized notebook\n",
    "    To prevent overwriting, set default overwrite to False if needed\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running model {}\".format(cfg[\"sn\"]))\n",
    "    os.makedirs(cfg[\"model_folder\"], exist_ok=True)\n",
    "    papermill.execute_notebook(\n",
    "        cfg[\"in_notebook\"],\n",
    "        cfg[\"out_notebook\"],\n",
    "        parameters=cfg[\"params\"],\n",
    "        prepare_only=True\n",
    "    )\n",
    "    \n",
    "    in_notebook = cfg[\"out_notebook\"]\n",
    "    out_dir = cfg[\"model_folder\"]\n",
    "    !jupyter nbconvert --ExecutePreprocessor.timeout=300000 --output-dir $out_dir --to notebook --execute $in_notebook\n",
    "    !jupyter nbconvert --output-dir $out_dir --to html_toc $in_notebook\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "# Run in parallel pool\n",
    "with multiprocessing.Pool(2) as pool:\n",
    "    pool.map(run_batch, batch_cfgs)\n",
    "\n",
    "# for cfg in tqdm(batch_cfgs):\n",
    "#     run_batch(cfg)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot execute together with parallel run for some reason... maybe due to nest_asynio... forgot how to fix...\n",
    "# cfgs, df = parse_batch_results(batch_cfgs)\n",
    "# df.to_csv(batch_output_dir + \"bcdf.csv\")\n",
    "# cfgs.to_csv(batch_output_dir + \"cfgs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shutdown compute engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "print('shuting down')\n",
    "!sudo poweroff"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
