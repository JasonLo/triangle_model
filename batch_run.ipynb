{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Batch run models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os, multiprocessing, time, papermill, sys, sqlite3\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "from meta import make_batch_cfg, csv_to_bigquery\n",
    "import pandas as pd\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "# Gen OG random seed: [int(random.random() * 1e5) for x in range(10)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make configs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Latest workflow: Use this to generate batch cfg\n",
    "Run with quick_run_papermill.py and set which gpu to run on\n",
    "Run up to 4 instance on Uconn server\n",
    "May split GPU since there are a lot of head room "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "batch_name = \"lesion_batchsize_OS30\"\n",
    "batch_output_dir = f\"models/{batch_name}/\"\n",
    "\n",
    "param_grid = {\n",
    "    \"tasks_ps\": [\n",
    "        # (0.2, 0.2, 0.05, 0.05, 0.1, 0.1, 0.3),\n",
    "        # (0.2, 0.2, 0.05, 0.05, 0.1, 0.15, 0.25),\n",
    "        # (0.2, 0.2, 0.05, 0.05, 0.1, 0.2, 0.2),\n",
    "        (0.2, 0.2, 0.05, 0.05, 0.1, 0.3, 0.1)\n",
    "    ],\n",
    "    \"batch_size\": [1, 2, 4, 16, 64],\n",
    "}\n",
    "\n",
    "static_hpar = {\n",
    "    \"tf_root\": \"/home/jal21012/triangle_model\",\n",
    "    \"ort_units\": 119,\n",
    "    \"pho_units\": 250,\n",
    "    \"sem_units\": 2446,\n",
    "    \"hidden_os_units\": 500,\n",
    "    \"hidden_op_units\": 100,\n",
    "    \"hidden_ps_units\": 500,\n",
    "    \"hidden_sp_units\": 500,\n",
    "    \"pho_cleanup_units\": 50,\n",
    "    \"sem_cleanup_units\": 50,\n",
    "    \"pho_noise_level\": 0.0,\n",
    "    \"sem_noise_level\": 0.0,\n",
    "    \"activation\": \"sigmoid\",\n",
    "    \"tau\": 1 / 3,\n",
    "    \"max_unit_time\": 4.0,\n",
    "    \"inject_error_ticks\": 11,\n",
    "    \"output_ticks\": 13,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"zero_error_radius\": 0.1,\n",
    "    \"save_freq\": 20,\n",
    "    \"n_mil_sample\": 2.0,\n",
    "    \"wf_compression\": \"log\",\n",
    "    \"wf_clip_low\": 0,\n",
    "    \"wf_clip_high\": 999_999_999,\n",
    "    \"task_names\": (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"ort_pho\", \"ort_sem\", \"triangle\"),\n",
    "    \"batch_name\": batch_name,\n",
    "    \"total_sample\": 10_000_000,\n",
    "    \"rng_seed\": 2021\n",
    "}\n",
    "\n",
    "batch_cfgs = make_batch_cfg(\n",
    "    batch_name, batch_output_dir, static_hpar, param_grid, \"master.ipynb\"\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "UUID not found, regenerating.\n",
      "Saved config json to /home/jal21012/triangle_model/models/lesion_batchsize_OS30/lesion_batchsize_OS30_r0000/model_config.json\n",
      "UUID not found, regenerating.\n",
      "Saved config json to /home/jal21012/triangle_model/models/lesion_batchsize_OS30/lesion_batchsize_OS30_r0001/model_config.json\n",
      "UUID not found, regenerating.\n",
      "Saved config json to /home/jal21012/triangle_model/models/lesion_batchsize_OS30/lesion_batchsize_OS30_r0002/model_config.json\n",
      "UUID not found, regenerating.\n",
      "Saved config json to /home/jal21012/triangle_model/models/lesion_batchsize_OS30/lesion_batchsize_OS30_r0003/model_config.json\n",
      "UUID not found, regenerating.\n",
      "Saved config json to /home/jal21012/triangle_model/models/lesion_batchsize_OS30/lesion_batchsize_OS30_r0004/model_config.json\n",
      "Batch config saved to models/batch_run/lesion_batchsize_OS30/\n",
      "There are 5 models in this batch\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Push config to database"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cfg_df = pd.DataFrame()\n",
    "\n",
    "for i, cfg in enumerate(batch_cfgs):\n",
    "    cfg_df = pd.concat([cfg_df, pd.DataFrame(cfg[\"params\"], index=[i])])\n",
    "\n",
    "\n",
    "sqlite_file = os.path.join(batch_output_dir, \"batch_results.sqlite\")\n",
    "con = sqlite3.connect(sqlite_file)\n",
    "cur = con.cursor()\n",
    "cfg_df.to_sql(\"batch_config\", con, if_exists=\"replace\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run batch\n",
    "- Can run on Jupyter Lab\n",
    "- Cannot run on VSCode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run\n",
    "def run_batch(cfg):\n",
    "    \"\"\"\n",
    "    Using papermill to run parameterized notebook\n",
    "    To prevent overwriting, set default overwrite to False if needed\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Running model {}\".format(cfg[\"sn\"]))\n",
    "    os.makedirs(cfg[\"model_folder\"], exist_ok=True)\n",
    "    papermill.execute_notebook(\n",
    "        cfg[\"in_notebook\"],\n",
    "        cfg[\"out_notebook\"],\n",
    "        parameters=cfg[\"params\"],\n",
    "    )\n",
    "\n",
    "    clear_output()\n",
    "\n",
    "\n",
    "# Run in parallel pool\n",
    "\n",
    "# with multiprocessing.Pool(2) as pool:\n",
    "#     pool.map(run_batch, batch_cfgs)\n",
    "\n",
    "\n",
    "# Run in series\n",
    "for cfg in tqdm(batch_cfgs):\n",
    "    run_batch(cfg)\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compile and save results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Re-evaluation snippet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "# print(cur.fetchall())\n",
    "# con.execute('DROP TABLE strain')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\n",
    "import meta, data_wrangling, modeling, evaluate\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "\n",
    "    code_name = cfg_df.code_name[i]\n",
    "    cfg = meta.Config.from_json(\n",
    "        os.path.join(batch_output_dir, code_name, \"model_config.json\")\n",
    "    )\n",
    "\n",
    "\n",
    "    # Push csv to BQ\n",
    "    csv_file = os.path.join(\n",
    "        batch_output_dir, code_name, \"eval\", \"train_item_df.csv\"\n",
    "    )\n",
    "    csv_to_bigquery(\n",
    "        csv_file, dataset_name=batch_name, table_name=\"train\"\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shutdown compute engine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "time.sleep(30)\n",
    "print('shuting down')\n",
    "!sudo poweroff"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false,
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}