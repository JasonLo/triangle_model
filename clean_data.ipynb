{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprecess training set\n",
    "This notebook preprocesses the training set from the raw data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from data_wrangling import (\n",
    "    get_used_slots, \n",
    "    trim_unused_slots, \n",
    "    get_duplicates, \n",
    "    ort_to_binary,\n",
    "    pho_to_binary,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training set raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>____a_________</td>\n",
       "      <td>___^______</td>\n",
       "      <td>1100290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ace</td>\n",
       "      <td>____a_ce______</td>\n",
       "      <td>___es_____</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ache</td>\n",
       "      <td>____a_che_____</td>\n",
       "      <td>___ek_____</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ached</td>\n",
       "      <td>____a_ched____</td>\n",
       "      <td>___ekt____</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aches</td>\n",
       "      <td>____a_ches____</td>\n",
       "      <td>___eks____</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>zoo</td>\n",
       "      <td>___zoo________</td>\n",
       "      <td>__zu______</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>zoom</td>\n",
       "      <td>___zoom_______</td>\n",
       "      <td>__zum_____</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>zoomed</td>\n",
       "      <td>___zoomed_____</td>\n",
       "      <td>__zumd____</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5830</th>\n",
       "      <td>zooms</td>\n",
       "      <td>___zooms______</td>\n",
       "      <td>__zumz____</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5831</th>\n",
       "      <td>zoos</td>\n",
       "      <td>___zoos_______</td>\n",
       "      <td>__zuz_____</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5832 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word             ort         pho         wf\n",
       "0          a  ____a_________  ___^______  1100290.0\n",
       "1        ace  ____a_ce______  ___es_____      117.0\n",
       "2       ache  ____a_che_____  ___ek_____       27.0\n",
       "3      ached  ____a_ched____  ___ekt____        3.0\n",
       "4      aches  ____a_ches____  ___eks____       23.0\n",
       "...      ...             ...         ...        ...\n",
       "5827     zoo  ___zoo________  __zu______      172.0\n",
       "5828    zoom  ___zoom_______  __zum_____       38.0\n",
       "5829  zoomed  ___zoomed_____  __zumd____       44.0\n",
       "5830   zooms  ___zooms______  __zumz____       18.0\n",
       "5831    zoos  ___zoos_______  __zuz_____       23.0\n",
       "\n",
       "[5832 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\n",
    "    \"issues/preprocessing/6ktraining_v2.dict\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,\n",
    ")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unused slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Removing unused slots: [0, 11, 12, 13]\n",
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: []\n"
     ]
    }
   ],
   "source": [
    "train['ort'] = trim_unused_slots(train.ort)\n",
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some slot that are rarely used, check which ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'_', 's'},\n",
       " 1: {'S', 'T', '_', 'b', 'd', 'f', 'g', 'k', 'p', 's', 't'},\n",
       " 2: {'C',\n",
       "  'D',\n",
       "  'J',\n",
       "  'S',\n",
       "  'T',\n",
       "  '_',\n",
       "  'b',\n",
       "  'd',\n",
       "  'f',\n",
       "  'g',\n",
       "  'h',\n",
       "  'k',\n",
       "  'l',\n",
       "  'm',\n",
       "  'n',\n",
       "  'p',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'v',\n",
       "  'w',\n",
       "  'y',\n",
       "  'z'},\n",
       " 3: {'@', 'A', 'E', 'I', 'O', 'U', 'W', 'Y', '^', 'a', 'e', 'i', 'o', 'u'},\n",
       " 4: {'C',\n",
       "  'D',\n",
       "  'J',\n",
       "  'S',\n",
       "  'T',\n",
       "  'Z',\n",
       "  '_',\n",
       "  'b',\n",
       "  'd',\n",
       "  'f',\n",
       "  'g',\n",
       "  'k',\n",
       "  'l',\n",
       "  'm',\n",
       "  'n',\n",
       "  'p',\n",
       "  'r',\n",
       "  's',\n",
       "  't',\n",
       "  'v',\n",
       "  'z'},\n",
       " 5: {'C',\n",
       "  'J',\n",
       "  'S',\n",
       "  'T',\n",
       "  '_',\n",
       "  'b',\n",
       "  'd',\n",
       "  'f',\n",
       "  'g',\n",
       "  'k',\n",
       "  'l',\n",
       "  'm',\n",
       "  'n',\n",
       "  'p',\n",
       "  's',\n",
       "  't',\n",
       "  'v',\n",
       "  'z'},\n",
       " 6: {'J', 'T', '_', 'd', 's', 't', 'z'},\n",
       " 7: {'_', 's', 't', 'z'},\n",
       " 8: {'E', '_'},\n",
       " 9: {'_', 's'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{slot: set([x[slot] for x in train.pho]) for slot in range(len(train.pho[0]))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slot 0, 8, 9 are somewhat rarely use (only one possible phoneme other than empty). They are pending for removal. Before removing, let's make sure it won't affects too many words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap in _broC___Es at {8, 9}\n",
      "Overlap in _kloz___Es at {8, 9}\n",
      "Overlap in skr@m_____ at {0}\n",
      "Overlap in skr@p_____ at {0}\n",
      "Overlap in skrep_____ at {0}\n",
      "Overlap in skrept____ at {0}\n",
      "Overlap in skreps____ at {0}\n",
      "Overlap in skr@ps____ at {0}\n",
      "Overlap in skr@tC____ at {0}\n",
      "Overlap in skr@tCt___ at {0}\n",
      "Overlap in skral_____ at {0}\n",
      "Overlap in skrald____ at {0}\n",
      "Overlap in skralz____ at {0}\n",
      "Overlap in skrim_____ at {0}\n",
      "Overlap in skrimd____ at {0}\n",
      "Overlap in skrimz____ at {0}\n",
      "Overlap in skri______ at {0}\n",
      "Overlap in skriC_____ at {0}\n",
      "Overlap in skrid_____ at {0}\n",
      "Overlap in skrin_____ at {0}\n",
      "Overlap in skrinz____ at {0}\n",
      "Overlap in skru______ at {0}\n",
      "Overlap in skruz_____ at {0}\n",
      "Overlap in skrAb_____ at {0}\n",
      "Overlap in skrImp____ at {0}\n",
      "Overlap in skrIp_____ at {0}\n",
      "Overlap in skrIpt____ at {0}\n",
      "Overlap in skrIpts___ at {0}\n",
      "Overlap in skrWnJ____ at {0}\n",
      "Overlap in skr^b_____ at {0}\n",
      "Overlap in skr^bd____ at {0}\n",
      "Overlap in skr^bz____ at {0}\n",
      "Overlap in skr^f_____ at {0}\n",
      "Overlap in skr^m_____ at {0}\n",
      "Overlap in skr^nC____ at {0}\n",
      "Overlap in spl@S_____ at {0}\n",
      "Overlap in spl@St____ at {0}\n",
      "Overlap in sple______ at {0}\n",
      "Overlap in splin_____ at {0}\n",
      "Overlap in splinz____ at {0}\n",
      "Overlap in splAs_____ at {0}\n",
      "Overlap in splIt_____ at {0}\n",
      "Overlap in splatC____ at {0}\n",
      "Overlap in splatCt___ at {0}\n",
      "Overlap in spl^rJ____ at {0}\n",
      "Overlap in spren_____ at {0}\n",
      "Overlap in sprend____ at {0}\n",
      "Overlap in sprenz____ at {0}\n",
      "Overlap in spr@t_____ at {0}\n",
      "Overlap in spral_____ at {0}\n",
      "Overlap in sprald____ at {0}\n",
      "Overlap in spralz____ at {0}\n",
      "Overlap in spre______ at {0}\n",
      "Overlap in sprez_____ at {0}\n",
      "Overlap in sprEd_____ at {0}\n",
      "Overlap in spri______ at {0}\n",
      "Overlap in spriz_____ at {0}\n",
      "Overlap in sprIg_____ at {0}\n",
      "Overlap in sprIgz____ at {0}\n",
      "Overlap in sprIng____ at {0}\n",
      "Overlap in sprIngz___ at {0}\n",
      "Overlap in sprInt____ at {0}\n",
      "Overlap in sprAt_____ at {0}\n",
      "Overlap in sprAts____ at {0}\n",
      "Overlap in sprWt_____ at {0}\n",
      "Overlap in sprus_____ at {0}\n",
      "Overlap in skwab_____ at {0}\n",
      "Overlap in skwad_____ at {0}\n",
      "Overlap in skwadz____ at {0}\n",
      "Overlap in skwal_____ at {0}\n",
      "Overlap in skwalz____ at {0}\n",
      "Overlap in skwer_____ at {0}\n",
      "Overlap in skwerz____ at {0}\n",
      "Overlap in skwaS_____ at {0}\n",
      "Overlap in skwat_____ at {0}\n",
      "Overlap in skwa______ at {0}\n",
      "Overlap in skwak_____ at {0}\n",
      "Overlap in skwaz_____ at {0}\n",
      "Overlap in skwik_____ at {0}\n",
      "Overlap in skwil_____ at {0}\n",
      "Overlap in skwiz_____ at {0}\n",
      "Overlap in skwElC____ at {0}\n",
      "Overlap in skwIb_____ at {0}\n",
      "Overlap in skwId_____ at {0}\n",
      "Overlap in skwInt____ at {0}\n",
      "Overlap in skwAr_____ at {0}\n",
      "Overlap in skwArz____ at {0}\n",
      "Overlap in skw^rm____ at {0}\n",
      "Overlap in skw^rt____ at {0}\n",
      "Overlap in stret_____ at {0}\n",
      "Overlap in stren_____ at {0}\n",
      "Overlap in strenz____ at {0}\n",
      "Overlap in stret_____ at {0}\n",
      "Overlap in strets____ at {0}\n",
      "Overlap in str@nd____ at {0}\n",
      "Overlap in str@ndz___ at {0}\n",
      "Overlap in strenJ____ at {0}\n",
      "Overlap in str@p_____ at {0}\n",
      "Overlap in str@pt____ at {0}\n",
      "Overlap in str@ps____ at {0}\n",
      "Overlap in stra______ at {0}\n",
      "Overlap in straz_____ at {0}\n",
      "Overlap in stre______ at {0}\n",
      "Overlap in strik_____ at {0}\n",
      "Overlap in striks____ at {0}\n",
      "Overlap in strim_____ at {0}\n",
      "Overlap in strimz____ at {0}\n",
      "Overlap in strit_____ at {0}\n",
      "Overlap in strits____ at {0}\n",
      "Overlap in strEngT___ at {0}\n",
      "Overlap in strEngTs__ at {0}\n",
      "Overlap in strEs_____ at {0}\n",
      "Overlap in strEtC____ at {0}\n",
      "Overlap in strEtCt___ at {0}\n",
      "Overlap in stru______ at {0}\n",
      "Overlap in strIkt____ at {0}\n",
      "Overlap in strAd_____ at {0}\n",
      "Overlap in strAdz____ at {0}\n",
      "Overlap in strAf_____ at {0}\n",
      "Overlap in strAfs____ at {0}\n",
      "Overlap in strAk_____ at {0}\n",
      "Overlap in strAks____ at {0}\n",
      "Overlap in strIng____ at {0}\n",
      "Overlap in strIngz___ at {0}\n",
      "Overlap in strIp_____ at {0}\n",
      "Overlap in strAp_____ at {0}\n",
      "Overlap in strAps____ at {0}\n",
      "Overlap in strIps____ at {0}\n",
      "Overlap in strAv_____ at {0}\n",
      "Overlap in strAvz____ at {0}\n",
      "Overlap in strok_____ at {0}\n",
      "Overlap in stroks____ at {0}\n",
      "Overlap in strol_____ at {0}\n",
      "Overlap in strold____ at {0}\n",
      "Overlap in strolz____ at {0}\n",
      "Overlap in strang____ at {0}\n",
      "Overlap in strap_____ at {0}\n",
      "Overlap in strapt____ at {0}\n",
      "Overlap in straps____ at {0}\n",
      "Overlap in strov_____ at {0}\n",
      "Overlap in str^k_____ at {0}\n",
      "Overlap in str^m_____ at {0}\n",
      "Overlap in str^t_____ at {0}\n"
     ]
    }
   ],
   "source": [
    "remove_phos = []\n",
    "\n",
    "for pho in train.pho:\n",
    "    used_slots = get_used_slots(pho)\n",
    "    clashed_slots = used_slots.intersection({0, 8, 9})\n",
    "    if clashed_slots:\n",
    "        remove_phos.append(pho)\n",
    "        print(f\"Overlap in {pho} at {clashed_slots}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slot 8, 9 had only occupied by 2 words:\n",
    "- Overlap in _broC___Es at {8, 9}\n",
    "- Overlap in _kloz___Es at {8, 9}\n",
    "\n",
    "Slot 0 had occupied by 141 words, for example:\n",
    "- Overlap in skr@m_____ at {0}\n",
    "- Overlap in skr@p_____ at {0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be safe, remove the words that had used slots 0, 8, 9. Then the data set will have new unused slots (in terms of the entire data set), we can use the same function `trim_unused_slots` to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: [0, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "train = train.loc[~train.pho.isin(remove_phos)].copy()\n",
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, 0, 8, 9 are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wordnet_dict(dict_file):\n",
    "    \"\"\"Load wordnet from source file.\"\"\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        dict_file,\n",
    "        sep=r\"[\\t| ]\",\n",
    "        header=None,\n",
    "        names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "        na_filter=False,  # needed to avoid treating null as empty string\n",
    "        engine=\"python\",\n",
    "    )\n",
    "\n",
    "    df[\"wn_idx\"] = df.index\n",
    "\n",
    "    return df\n",
    "\n",
    "wn_dict = load_wordnet_dict(\"issues/preprocessing/6kdict\")\n",
    "wn_repr = np.genfromtxt(\"issues/preprocessing/wordNet_6229.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bass': 2,\n",
       " 'beat': 2,\n",
       " 'bet': 2,\n",
       " 'cast': 2,\n",
       " 'close': 2,\n",
       " 'closed': 2,\n",
       " 'corps': 2,\n",
       " 'cut': 2,\n",
       " 'deer': 2,\n",
       " 'elk': 2,\n",
       " 'fish': 2,\n",
       " 'fit': 2,\n",
       " 'fowl': 2,\n",
       " 'french': 2,\n",
       " 'hit': 2,\n",
       " 'hops': 3,\n",
       " 'hurt': 2,\n",
       " 'knit': 2,\n",
       " 'leaves': 2,\n",
       " 'left': 2,\n",
       " 'let': 2,\n",
       " 'lives': 2,\n",
       " 'moose': 2,\n",
       " 'nuts': 2,\n",
       " 'pants': 3,\n",
       " 'put': 2,\n",
       " 'quit': 2,\n",
       " 'red': 2,\n",
       " 'scours': 2,\n",
       " 'set': 2,\n",
       " 'shed': 2,\n",
       " 'sheep': 2,\n",
       " 'shrimp': 2,\n",
       " 'shut': 2,\n",
       " 'sioux': 2,\n",
       " 'squash': 2,\n",
       " 'thrust': 2,\n",
       " 'was': 2,\n",
       " 'wed': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homographs = get_duplicates(wn_dict)\n",
    "homographs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "39 homographs is found in wordnet. We will merge it into the training set, then adjust for the duplicated word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(wn_dict[['word', 'wn_idx']], on=\"word\", how=\"left\")\n",
    "train = train.dropna(subset=[\"wn_idx\"])\n",
    "train = train.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show an example to make sure the wordnet is merged correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "      <th>wn_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>4189</td>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>_S^t___</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>4496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>4190</td>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>_S^t___</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>4497.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  word         ort      pho      wf  wn_idx\n",
       "4181   4189  shut  _shu_t____  _S^t___  1316.0  4496.0\n",
       "4182   4190  shut  _shu_t____  _S^t___  1316.0  4497.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.word=='shut']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, proceed to adjusting word frequency to avoid over sampling from the duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wf(row):\n",
    "    \"\"\"Adjust word frequencies.\"\"\"\n",
    "    if row.word in homographs.keys():\n",
    "        return(row.wf / homographs[row.word])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['adjusted_wf'] = train.apply(adjust_wf, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some example to make sure it is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "      <th>wn_idx</th>\n",
       "      <th>adjusted_wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>2842</td>\n",
       "      <td>man</td>\n",
       "      <td>__ma_n____</td>\n",
       "      <td>_m@n___</td>\n",
       "      <td>8417.0</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>8417.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4181</th>\n",
       "      <td>4189</td>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>_S^t___</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>4496.0</td>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>4190</td>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>_S^t___</td>\n",
       "      <td>1316.0</td>\n",
       "      <td>4497.0</td>\n",
       "      <td>658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4813</th>\n",
       "      <td>4821</td>\n",
       "      <td>swim</td>\n",
       "      <td>_swi_m____</td>\n",
       "      <td>swIm___</td>\n",
       "      <td>120.0</td>\n",
       "      <td>5265.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  word         ort      pho      wf  wn_idx  adjusted_wf\n",
       "2836   2842   man  __ma_n____  _m@n___  8417.0  3014.0       8417.0\n",
       "4181   4189  shut  _shu_t____  _S^t___  1316.0  4496.0        658.0\n",
       "4182   4190  shut  _shu_t____  _S^t___  1316.0  4497.0        658.0\n",
       "4813   4821  swim  _swi_m____  swIm___   120.0  5265.0        120.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.word.isin(['shut', 'man', 'swim'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for packaging the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('dataset/df_train_211209.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: defaultdict(<class 'int'>, {'_': 5641, 'c': 4, 'p': 2, 's': 32, 't': 40})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 3525, 'b': 185, 'c': 334, 'h': 70, 'd': 71, 'f': 166, 'g': 165, 'k': 35, 'p': 147, 'q': 49, 'r': 4, 's': 700, 't': 173, 'w': 95})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 202, 'b': 287, 'l': 752, 'r': 927, 'c': 283, 'h': 620, 'z': 24, 'd': 215, 'w': 290, 'f': 202, 'g': 164, 'n': 197, 'j': 89, 'k': 77, 'm': 267, 'p': 354, 's': 225, 'u': 54, 't': 356, 'v': 74, 'y': 60})\n",
      "Token count: defaultdict(<class 'int'>, {'a': 1507, 'e': 1048, 'i': 1029, 'o': 1386, 'u': 691, 'y': 58})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 3988, 'i': 255, 'u': 231, 'w': 210, 'y': 79, 'a': 404, 'e': 332, 'h': 3, 'o': 217})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 206, 'c': 288, 'd': 325, 'f': 141, 'g': 249, 'l': 644, 'm': 349, 'r': 721, 's': 548, 'n': 847, 'p': 319, 't': 511, 'e': 34, 'x': 34, 'b': 156, 'k': 170, 'u': 4, 'z': 48, 'v': 115, 'q': 4, 'h': 4, 'w': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 1386, 'e': 1026, 'h': 282, 't': 487, 'd': 205, 's': 862, 'l': 188, 'm': 60, 'p': 194, 'g': 159, 'c': 159, 'k': 424, 'b': 57, 'n': 79, 'f': 66, 'q': 7, 'z': 9, 'r': 11, 'u': 14, 'x': 7, 'v': 36, 'a': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 3829, 'e': 485, 's': 1015, 'd': 163, 'h': 152, 't': 63, 'u': 9, 'n': 1, 'z': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 5321, 'd': 277, 's': 80, 't': 1, 'e': 35, 'h': 4, 'g': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 5689, 'd': 24, 's': 5, 'e': 1})\n"
     ]
    }
   ],
   "source": [
    "train_package = {\n",
    "    \"id\": train.word.index.tolist(),\n",
    "    \"item\": train.word.tolist(),\n",
    "    \"wf\": train.adjusted_wf.tolist(),\n",
    "    \"ort\": tf.convert_to_tensor(ort_to_binary(train.ort), dtype=tf.float32),\n",
    "    \"pho\": tf.convert_to_tensor(pho_to_binary(train.pho), dtype=tf.float32),\n",
    "    \"sem\": tf.convert_to_tensor(wn_repr[[int(x) for x in train.wn_idx],], dtype=tf.float32),\n",
    "    \"graphems\": train.ort.tolist(),\n",
    "    \"phonemes\": train.pho.tolist(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify dimensions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5719\n",
      "5719\n",
      "5719\n",
      "(5719, 119)\n",
      "(5719, 175)\n",
      "(5719, 2446)\n",
      "5719\n",
      "5719\n"
     ]
    }
   ],
   "source": [
    "print(len(train_package[\"id\"]))\n",
    "print(len(train_package[\"item\"]))\n",
    "print(len(train_package[\"wf\"]))\n",
    "print(train_package[\"ort\"].shape)\n",
    "print(train_package[\"pho\"].shape)\n",
    "print(train_package[\"sem\"].shape)\n",
    "print(len(train_package[\"graphems\"]))\n",
    "print(len(train_package[\"graphems\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "\n",
    "with gzip.open(\"dataset/train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_package, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Make sparase tensor later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "%reload_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Read training file\n",
    "train_file = \"issues/preprocessing/6ktraining_v2.dict\"\n",
    "\n",
    "strain_file = \"issues/preprocessing/strain.txt\"\n",
    "strain_key_file = \"issues/preprocessing/strain_key.txt\"\n",
    "\n",
    "grain_file = \"issues/preprocessing/grain_nws.dict\"\n",
    "grain_key_file = \"issues/preprocessing/grain_key.txt\"\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "\n",
    "# Zeno norm\n",
    "zeno = pd.read_csv(\"issues/preprocessing/EWFG.csv\", na_values=\"\", keep_default_na=False)\n",
    "zeno[\"gr14\"] = pd.to_numeric(zeno.f, errors=\"coerce\")  # Stage 14 is adult frequency\n",
    "[zeno.pop(v) for v in [\"sfi\", \"d\", \"u\", \"f\"]]\n",
    "clear_output()\n",
    "\n",
    "# Chang training set (from Chang 2019 github)\n",
    "y_wordnet = np.genfromtxt(\"issues/preprocessing/wordNet_6229.csv\", delimiter=\",\")\n",
    "\n",
    "wordnet_dict = pd.read_csv(\n",
    "    \"issues/preprocessing/6kdict\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "## Copy index for key\n",
    "wordnet_dict[\"wn_idx\"] = wordnet_dict.index\n",
    "\n",
    "## Drop wordnet duplicates (Do not drop)\n",
    "## HS04: There were 39 words in which a single spelling was associated with two or more meanings (mainly words such as SHEEP, FISH, or HIT, whose plural or past tense morphological inflection involves no change from the stem).\n",
    "# wordnet_dict.drop_duplicates(subset=['word'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Zeno and IMG into train\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "train = pd.merge(train, zeno, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Assume Zeno missing = 0\n",
    "for x in range(14):\n",
    "    variable_name = \"gr\" + str(x + 1)\n",
    "    train[variable_name] = train[variable_name].map(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "train = pd.merge(train, img_map, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "\n",
    "# Merge Chang\n",
    "wnid = wordnet_dict.loc[:, [\"word\", \"wn_idx\"]]\n",
    "train = train.merge(wnid, how=\"inner\", on=\"word\")\n",
    "print(f\"Words in training set: {len(train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "strain = pd.read_csv(\n",
    "    strain_file, sep=\"\\t\", header=None, names=[\"word\", \"ort\", \"pho\", \"wf\"]\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=[\"word\", \"frequency\", \"pho_consistency\", \"imageability\"],\n",
    ")\n",
    "\n",
    "strain = pd.merge(strain, strain_key)\n",
    "strain = pd.merge(strain, img_map, on=\"word\", how=\"left\")\n",
    "strain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain.groupby(\"frequency\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "grain = pd.merge(grain, grain_key)\n",
    "\n",
    "grain['img'] = 0\n",
    "grain['wf'] = 0\n",
    "grain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban = pd.read_csv(\"issues/preprocessing/taraban.csv\")\n",
    "taraban.columns = [\"id\", \"cond\", \"word\", \"ort\", \"pho\", \"wf\"]\n",
    "taraban = pd.merge(taraban, img_map, on=\"word\", how=\"left\")\n",
    "taraban.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glushko = pd.read_csv(\"issues/preprocessing/glushko_nonword.csv\")\n",
    "glushko.columns = [\"id\", \"cond\", \"word\", \"pho\", \"ort\"]\n",
    "\n",
    "glushko[\"img\"] = 0\n",
    "glushko[\"wf\"] = 0\n",
    "glushko.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check raw data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all represtation follow 14 ort, 10 pho format\n",
    "assert all([len(x) == 14 for x in train.ort])\n",
    "assert all([len(x) == 14 for x in strain.ort])\n",
    "assert all([len(x) == 14 for x in grain.ort])\n",
    "assert all([len(x) == 14 for x in taraban.ort])\n",
    "assert all([len(x) == 14 for x in glushko.ort])\n",
    "\n",
    "assert all([len(x) == 10 for x in train.pho])\n",
    "assert all([len(x) == 10 for x in strain.pho])\n",
    "assert all([len(x) == 10 for x in grain.pho_small])\n",
    "assert all([len(x) == 10 for x in grain.pho_large])\n",
    "assert all([len(x) == 10 for x in taraban.pho])\n",
    "\n",
    "from ast import literal_eval\n",
    "for pho in glushko.pho:\n",
    "    ps = literal_eval(pho)\n",
    "    for p in ps:\n",
    "        assert len(p) == 10\n",
    "\n",
    "# Check all fufill trim_ort criteria\n",
    "locs = [0, 11, 12, 13]\n",
    "\n",
    "for l in locs:\n",
    "    assert all([x == '_' for x in train.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in strain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in grain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in taraban.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in glushko.ort.str.get(l)])\n",
    "\n",
    "# No missing data in critical variables\n",
    "assert sum(train.ort.isna()) == 0\n",
    "assert sum(train.pho.isna()) == 0\n",
    "assert sum(train.wf.isna()) == 0\n",
    "\n",
    "assert sum(strain.ort.isna()) == 0\n",
    "assert sum(strain.pho.isna()) == 0\n",
    "assert sum(strain.wf.isna()) == 0\n",
    "\n",
    "assert sum(grain.ort.isna()) == 0\n",
    "assert sum(grain.pho_small.isna()) == 0\n",
    "assert sum(grain.pho_large.isna()) == 0\n",
    "\n",
    "assert sum(taraban.ort.isna()) == 0\n",
    "assert sum(taraban.pho.isna()) == 0\n",
    "\n",
    "assert sum(glushko.ort.isna()) == 0\n",
    "assert sum(glushko.pho.isna()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def trim_ort(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = trim_ort(train)\n",
    "df_strain = trim_ort(strain)\n",
    "df_grain = trim_ort(grain)\n",
    "df_taraban = trim_ort(taraban)\n",
    "df_glushko = trim_ort(glushko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imageability missing data replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_missing(df, var):\n",
    "    print(\n",
    "        '{} missing in {}: {}/{}'.format(\n",
    "            var, df, sum(globals()[df][var].isna()), len((globals()[df]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "chk_missing('df_train', 'img')\n",
    "chk_missing('df_strain', 'img')\n",
    "chk_missing('df_grain', 'img')\n",
    "chk_missing('df_taraban', 'img')\n",
    "chk_missing('df_glushko', 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train['img'] = df_train.img.fillna(mean_img)\n",
    "\n",
    "# Fill missing value to condition mean img rating\n",
    "mean_strain_hi_img = df_strain.loc[df_strain.imageability == \"HI\", 'img'].mean()\n",
    "mean_strain_lo_img = df_strain.loc[df_strain.imageability == \"LI\", 'img'].mean()\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"HI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"HI\",\n",
    "                                     \"img\"].fillna(mean_strain_hi_img)\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"LI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"LI\",\n",
    "                                     \"img\"].fillna(mean_strain_lo_img)\n",
    "\n",
    "# Since taraban do not maniputate img, just replace by training set mean\n",
    "df_taraban['img'] = df_taraban.img.fillna(mean_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle homograph\n",
    "\n",
    "From Jay (201217):\n",
    "I'm guessing they just split the real frequency in two.  If it's possible to check that, even approximately, that would be good.  (If we don't have the WJ frequencies independently of these training sets, we could just ball park it--do the frequencies in the file look comparable, or half of, the frequencies of words that are more-or-less the same frequency in some other norms.  (I'm not sure if that's clear.  If not, we can talk about it.)\n",
    "\n",
    "- Split all frequency into n_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary for looking up word:n_dup\n",
    "\n",
    "tmp_count = train.groupby('word').agg('count').reset_index().loc[:,['word','wn_idx']]\n",
    "tmp_dups = tmp_count.loc[tmp_count.wn_idx>1,]\n",
    "dups_dict = dict(zip(tmp_dups.word, tmp_dups.wn_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dups in testset\n",
    "strain.loc[strain.word.isin(dups_dict.keys()),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban.loc[taraban.word.isin(dups_dict.keys()),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wf(row):\n",
    "    if row.word in dups_dict.keys():\n",
    "        return(row.wf / dups_dict[row.word])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['wf'] = train.apply(adjust_wf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('dataset/df_train.csv')\n",
    "df_strain.to_csv('dataset/df_strain.csv')\n",
    "df_grain.to_csv('dataset/df_grain.csv')\n",
    "df_taraban.to_csv('dataset/df_taraban.csv')\n",
    "df_glushko.to_csv('dataset/df_glushko.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export wordnet semantic representation (n=5821) for training set\n",
    "sem_train = y_wordnet[train.wn_idx,]\n",
    "print(f'Shape of selected semantic representation: {sem_train.shape}')\n",
    "np.savez_compressed('dataset/sem_train.npz', data=sem_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Strain semantic\n",
    "strain_word_idx = [df_train.loc[df_train.word==w,].index[0] for w in strain.word]\n",
    "sem_strain = sem_train[strain_word_idx,]\n",
    "np.savez_compressed('dataset/sem_strain.npz', data=sem_strain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print(\n",
    "                'Slot {} (n = {}, unique token = {}) {} \\n'.format(\n",
    "                    slotId, t.document_count, len(t.word_index.items()),\n",
    "                    t.word_docs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "        \n",
    "    return binData\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_word = pd.concat(\n",
    "    [\n",
    "        df_train.word, df_strain.word, df_grain.word, df_taraban.word,\n",
    "        df_glushko.word\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort, df_taraban.ort, df_glushko.ort],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "splitId_strain = len(df_train)\n",
    "splitId_grain = splitId_strain + len(df_strain)\n",
    "splitId_taraban = splitId_grain + len(df_grain)\n",
    "splitId_glushko = splitId_taraban + len(df_taraban)\n",
    "\n",
    "ort_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "ort_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "ort_grain = np.array(all_ort_bin[splitId_grain:splitId_taraban])\n",
    "ort_taraban = np.array(all_ort_bin[splitId_taraban:splitId_glushko])\n",
    "ort_glushko = np.array(all_ort_bin[splitId_glushko::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/ort_train.npz', data=ort_train)\n",
    "np.savez_compressed('dataset/ort_strain.npz', data=ort_strain)\n",
    "np.savez_compressed('dataset/ort_grain.npz', data=ort_grain)\n",
    "np.savez_compressed('dataset/ort_taraban.npz', data=ort_taraban)\n",
    "np.savez_compressed('dataset/ort_glushko.npz', data=ort_glushko)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('ort_train shape:', ort_train.shape)\n",
    "print('ort_strain shape:', ort_strain.shape)\n",
    "print('ort_grain shape:', ort_grain.shape)\n",
    "print('ort_taraban shape:', ort_taraban.shape)\n",
    "print('ort_glushko shape:', ort_glushko.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([len(p_col), binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        out = slotSeries.map(p_key).to_list()\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = out\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from src.data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "pho_train = pho2bin_v2(train.pho, phon_key)\n",
    "pho_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "pho_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "pho_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "pho_taraban = pho2bin_v2(taraban.pho, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/pho_train.npz', data=pho_train)\n",
    "np.savez_compressed('dataset/pho_strain.npz', data=pho_strain)\n",
    "np.savez_compressed('dataset/pho_large_grain.npz', data=pho_large_grain)\n",
    "np.savez_compressed('dataset/pho_small_grain.npz', data=pho_small_grain)\n",
    "np.savez_compressed('dataset/pho_taraban.npz', data=pho_taraban)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('pho_train shape:', pho_train.shape)\n",
    "print('pho_strain shape:', pho_strain.shape)\n",
    "print('pho_large_grain shape:', pho_large_grain.shape)\n",
    "print('pho_small_grain shape:', pho_small_grain.shape)\n",
    "print('pho_taraban shape:', pho_taraban.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import get_all_pronunciations_fast as get_p\n",
    "assert all(get_p(pho_train, phon_key) == df_train.pho)\n",
    "assert all(get_p(pho_strain, phon_key) == df_strain.pho)\n",
    "assert all(get_p(pho_large_grain, phon_key) == df_grain.pho_large)\n",
    "assert all(get_p(pho_small_grain, phon_key) == df_grain.pho_small)\n",
    "assert all(get_p(pho_taraban, phon_key) == df_taraban.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special format for Glushko PHO (due to multiple correct answer with different length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, pickle\n",
    "\n",
    "# Glushko pho dictionary\n",
    "phonology_glushko = {\n",
    "    x: ast.literal_eval(df_glushko.loc[i, 'pho'])\n",
    "    for i, x in enumerate(df_glushko.word)\n",
    "}\n",
    "\n",
    "# Glushko one-hot encoded output dictionary\n",
    "pho_glushko = {}\n",
    "for k, v in phonology_glushko.items():\n",
    "    ys = []\n",
    "    for pho in v:\n",
    "        y = []\n",
    "        for char in pho:\n",
    "            y += phon_key[char]\n",
    "        ys.append(y)\n",
    "    pho_glushko[k] = ys\n",
    "\n",
    "with open('dataset/pho_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(pho_glushko, f)\n",
    "\n",
    "print('y_glushko dimension: {}'.format(len(pho_glushko['beed'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_train = pd.read_csv(\"dataset/df_train.csv\", index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "import data_wrangling\n",
    "\n",
    "plot_f = df_train.sort_values(\"wf\")\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "(line1,) = ax.plot(\n",
    "    plot_f.wf,\n",
    "    data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"),\n",
    "    label=\"Log\",\n",
    ")\n",
    "(line2,) = ax.plot(\n",
    "    plot_f.wf,\n",
    "    data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"),\n",
    "    label=\"HS04\",\n",
    ")\n",
    "(line3,) = ax.plot(\n",
    "    plot_f.wf,\n",
    "    data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"),\n",
    "    label=\"JAY\",\n",
    ")\n",
    "\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Word frequency\")\n",
    "# plt.xlim((0, 200))\n",
    "# plt.ylim((0, .0006))\n",
    "plt.ylabel(\"Sampling probability\")\n",
    "# plt.xlim([0,100])\n",
    "plt.title(\"Tested sampling p vs. word frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "plt.xlim((0, 100))\n",
    "plt.ylim((0, .0002))\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new dictionary style representation (hash table)\n",
    "each word contains a maximum of 3 representations\n",
    "- orthography(ort): trimed slot-based one-hot letters encoding \n",
    "- phonology(pho): phonological features from Jason Zevin based on HS04 and Harm, 1998\n",
    "- semantic(sem): based on wordnet, clone from [Chang, 19 JML](https://github.com/JasonLo/Chang_Monaghan_Welbourne_AoA_Paper_for_JML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_representation_mapping = dict(zip(train.word, train.index))\n",
    "\n",
    "ort, pho, sem = {}, {}, {}\n",
    "\n",
    "for word in df_train.word:\n",
    "    word_idx = word_representation_mapping[word]\n",
    "    ort[word] = ort_train[word_idx]\n",
    "    pho[word] = pho_train[word_idx]\n",
    "    sem[word] = sem_train[word_idx]\n",
    "\n",
    "representation = {\"ort\": ort, \"pho\": pho, \"sem\": sem}\n",
    "print(f\"Total no. of training items: {len(ort.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: representation[\"ort\" or \"pho\" or \"sem\"][\"word\"]\n",
    "print(f'Representations of \"cat\" are:\\n')\n",
    "print(f'ort: {representation[\"ort\"][\"cat\"]} \\n with shape = {representation[\"ort\"][\"cat\"].shape}\\n')\n",
    "print(f'pho: {representation[\"pho\"][\"cat\"]} \\n with shape = {representation[\"pho\"][\"cat\"].shape}\\n')\n",
    "print(f'sem: {representation[\"sem\"][\"cat\"]} \\n with shape = {representation[\"sem\"][\"cat\"].shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pkl.gz\n",
    "import pickle, gzip\n",
    "\n",
    "with gzip.open('dataset/representation_dictionary.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(representation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New test set pickle format\n",
    "a dictionary with 4 keys\n",
    "- item: maybe word or nonword string, easy to human eye\n",
    "- ort: orthgraphic representation\n",
    "- pho: phonological representation\n",
    "- sem: semantic representation\n",
    "\n",
    "If all item within training set, use data_wrangling.MyData.create_testset_from_train_idx() to create dictionary. \n",
    "\n",
    "Otherwise, create it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_wrangling\n",
    "import gzip, pickle\n",
    "from importlib import reload\n",
    "\n",
    "reload(data_wrangling)\n",
    "data = data_wrangling.MyData()\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain (all)\n",
    "strain_items = data.df_strain.word.unique()\n",
    "strain_items = strain_items[strain_items != \"cut\"]  # Remove homographs\n",
    "strain_items_idx = list(data.df_train.loc[data.df_train.word.isin(strain_items)].index)\n",
    "strain_dict = data.create_testset_from_train_idx(strain_items_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_159 = data.df_strain.loc[data.df_strain.word != \"cut\"]\n",
    "strain_dict[\"cond\"] = list(\n",
    "    strain_159.frequency\n",
    "    + \"_\"\n",
    "    + strain_159.pho_consistency\n",
    "    + \"_\"\n",
    "    + strain_159.imageability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain_dict[\"freq\"] = list(strain_159.frequency)\n",
    "strain_dict[\"cons\"] = list(strain_159.pho_consistency)\n",
    "strain_dict[\"img\"] = list(strain_159.imageability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"dataset/testsets/strain.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(strain_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(x) for x in strain_dict.values()]  # All should be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain by each condition\n",
    "\n",
    "def make_strain_sub_testsets(df, f, c, i, save_file):\n",
    "\n",
    "    words = df.loc[\n",
    "        (df.frequency == f) & (df.pho_consistency == c) & (df.imageability == i),\n",
    "        \"word\",\n",
    "    ].unique()\n",
    "\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(words)].index)\n",
    "    testset_dict = data.create_testset_from_train_idx(idx)\n",
    "    with gzip.open(f\"dataset/testsets/{save_file}.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset_dict, f)\n",
    "\n",
    "\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"HI\", save_file=\"strain_hf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"LI\", save_file=\"strain_hf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"HI\", save_file=\"strain_hf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"LI\", save_file=\"strain_hf_inc_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"HI\", save_file=\"strain_lf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"LI\", save_file=\"strain_lf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"HI\", save_file=\"strain_lf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"LI\", save_file=\"strain_lf_inc_li\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (Removed all one ort, multi sem words)\n",
    "train_count = data.df_train.groupby(\"word\").count().reset_index()\n",
    "word_with_dup = list(train_count.loc[train_count.pho > 1, \"word\"])\n",
    "train_no_dup_idx = list(data.df_train.loc[~data.df_train.word.isin(word_with_dup)].index)\n",
    "train_dict = data.create_testset_from_train_idx(train_no_dup_idx)\n",
    "with gzip.open(\"dataset/testsets/train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(train_dict['item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_path = '/home/jupyter/tf/dataset'\n",
    "o = \n",
    "p_large = \n",
    "p_small = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain all\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "d = {\n",
    "    \"item\": list(data.df_grain.word),\n",
    "    \"cond\": list(data.df_grain.condition),\n",
    "    \"ort\": np.load(os.path.join(input_path, \"ort_grain.npz\"))[\"data\"],\n",
    "    \"pho_large_grain\": np.load(os.path.join(input_path, \"pho_large_grain.npz\"))[\"data\"],\n",
    "    \"pho_small_grain\": np.load(os.path.join(input_path, \"pho_small_grain.npz\"))[\"data\"],\n",
    "    \"sem\": np.zeros((len(data.df_grain.word), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(d, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain ambiguous\n",
    "import numpy as np\n",
    "\n",
    "nw_amb_idx = list(data.df_grain.loc[data.df_grain.condition==\"ambiguous\"].index)\n",
    "\n",
    "grain_ambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_amb_idx]),\n",
    "    \"ort\": data.ort_grain[nw_amb_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_amb_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_amb_idx],\n",
    "    \"sem\": np.zeros((len(nw_amb_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_ambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_ambiguous_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain unambiguous\n",
    "nw_un_idx = list(data.df_grain.loc[data.df_grain.condition==\"unambiguous\"].index)\n",
    "\n",
    "grain_unambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_un_idx]),\n",
    "    \"ort\": data.ort_grain[nw_un_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_un_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_un_idx],\n",
    "    \"sem\": np.zeros((len(nw_un_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_unambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_unambiguous_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train img (3 groups)\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "img_map[\"3gp\"] = pd.qcut(img_map.img, 3, [\"low\", \"med\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [\"low\", \"med\", \"high\"]:\n",
    "    x = img_map.loc[img_map[\"3gp\"]==g, \"word\"]\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(x)].index)\n",
    "    testset = data.create_testset_from_train_idx(idx)\n",
    "    \n",
    "    with gzip.open(f\"dataset/testsets/cortese_3gp_{g}_img.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low imageability (median split by Cortese rating)\n",
    "low_img_cortese_word = list(img_map.loc[img_map.img < 4, \"word\"])\n",
    "low_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(low_img_cortese_word)].index\n",
    ")\n",
    "testset_low_img_cortest = data.create_testset_from_train_idx(low_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_low_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_low_img_cortest, f)\n",
    "\n",
    "# Hi imageability (median split by Cortese rating)\n",
    "hi_img_cortese_word = list(img_map.loc[img_map.img >= 4, \"word\"])\n",
    "hi_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(hi_img_cortese_word)].index\n",
    ")\n",
    "testset_hi_img_cortest = data.create_testset_from_train_idx(hi_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_hi_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_hi_img_cortest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taraban\n",
    "taraban_name_map = {\n",
    "    \"High-frequency exception\": \"HF-EXC\",\n",
    "    \"High-frequency regular-inconsistent\": \"HF-REG-INC\",\n",
    "    \"Low-frequency exception\": \"LF-EXC\",\n",
    "    \"Low-frequency regular-inconsistent\": \"LF-REG-INC\",\n",
    "    \"Regular control for High-frequency exception\": \"CTRL-HF-EXC\",\n",
    "    \"Regular control for High-frequency regular-inconsistent\": \"CTRL-HF-REG-INC\",\n",
    "    \"Regular control for Low-frequency exception\": \"CTRL-LF-EXC\",\n",
    "    \"Regular control for Low-frequency regular-inconsistent\": \"CTRL-LF-REG-INC\",\n",
    "}\n",
    "\n",
    "\n",
    "for c in taraban.cond.unique():\n",
    "    idx = list(\n",
    "        data.df_train.loc[\n",
    "            data.df_train.word.isin(taraban.loc[taraban.cond == c, \"word\"]),\n",
    "        ].index\n",
    "    )\n",
    "    print(idx)\n",
    "\n",
    "    with gzip.open(\n",
    "        f\"dataset/testsets/taraban_{taraban_name_map[c].lower()}.pkl.gz\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(data.create_testset_from_train_idx(idx), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grain multi pho (add multiple answer to axis 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "import pickle, gzip\n",
    "\n",
    "def convert_grain(file):\n",
    "    testset = data_wrangling.load_testset(file)\n",
    "    s = testset['pho_small_grain']\n",
    "    l = testset['pho_large_grain']\n",
    "    es = np.expand_dims(s, axis = 1)\n",
    "    el = np.expand_dims(l, axis = 1)\n",
    "    testset['pho'] = np.concatenate((es, el), axis=1)\n",
    "\n",
    "    print(f\"Merged large and small grain pho into multi-answer format, with dim (items, ans, pho units)={testset['pho'].shape}\")\n",
    "    with gzip.open(file, \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "\n",
    "names = ('grain', 'grain_ambiguous', 'grain_unambiguous')\n",
    "files = [os.path.join(\"/home/jupyter/tf/dataset/testsets\", f\"{x}.pkl.gz\") for x in names]\n",
    "[convert_grain(f) for f in files]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import pandas as pd \n",
    "\n",
    "testset_file = \"dataset/testsets/grain_unambiguous.pkl.gz\"\n",
    "\n",
    " \n",
    "with gzip.open(testset_file, \"rb\") as f:\n",
    "    testset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['pho_large_grain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get output [epoch, tick, y, item]\n",
    "2. Get ans key [y, item]\n",
    "3. Parallel eval is possible, cast to [epoch, tick]\n",
    "4. Eval routine: acc, sse\n",
    "5. Eval extra: out1, out0, other diagnostic\n",
    "6. Combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create 300 words test set for 3 levels of difficulties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def sample_within_difficulty(low:float, high:float) -> [List[int], List[str]]:\n",
    "    \"\"\"Sample 100 words and returns its index within a range of difficulty levels\n",
    "    args:\n",
    "        low: lower bound of difficulty in percetile rank\n",
    "        high: upper bound of difficult in percetile rank\n",
    "    \"\"\"\n",
    "\n",
    "    data = data_wrangling.MyData()\n",
    "    pct = data.df_train.wf.rank(pct=True, ascending=False)\n",
    "    sel = pct.loc[(pct >= low) & (pct <= high)].sample(100)\n",
    "    words = data.df_train.loc[sel.index, \"word\"]\n",
    "    return sel.index.values, list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "\n",
    "def create_testset(idx: List[int], filename:str) -> dict:\n",
    "    \"\"\"Create a testset for a given list of indices\n",
    "    args:\n",
    "        idx: list of indices\n",
    "    \"\"\"\n",
    "    data = data_wrangling.MyData()\n",
    "    testset = data.create_testset_from_train_idx(idx)\n",
    "    \n",
    "    # Add condition labels\n",
    "    testset['cond'] = ['low'] * 100 + ['mid'] * 100 + ['hi'] * 100\n",
    "    \n",
    "    full_pickle_file_path = f\"dataset/testsets/{filename}.pkl.gz\"\n",
    "\n",
    "    with gzip.open(full_pickle_file_path, \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "\n",
    "    return testset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_idx, low_words = sample_within_difficulty(low=0.0, high=0.2)\n",
    "mid_idx, mid_words = sample_within_difficulty(low=0.4, high=0.6)\n",
    "hi_idx, hi_words = sample_within_difficulty(low=0.8, high=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty_testset_idx = np.concatenate([low_idx, mid_idx, hi_idx])\n",
    "create_testset(idx=difficulty_testset_idx, filename=\"train_r300_difficulty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add difficulty label to random 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r100 = data_wrangling.load_testset('train_r100')\n",
    "data = data_wrangling.MyData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_wf = data.df_train.wf.median()\n",
    "df = data.df_train[['word', 'wf']].copy()\n",
    "df['group'] = df.wf.apply(lambda x: 'lf' if x < median_wf else 'hf')\n",
    "mapper = {word: group for word, group in zip(df.word, df.group)}\n",
    "r100['cond'] = [mapper[word] for word in r100['item']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, pickle\n",
    "full_pickle_file_path = f\"dataset/testsets/{'train_r100'}.pkl.gz\"\n",
    "\n",
    "with gzip.open(full_pickle_file_path, \"wb\") as f:\n",
    "        pickle.dump(r100, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train testset into batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "import tensorflow as tf\n",
    "import helper\n",
    "\n",
    "data = data_wrangling.MyData()\n",
    "train = data_wrangling.load_testset('train')\n",
    "n = len(train['item'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def subset_train(train:dict, begin:int, size:int) -> dict:\n",
    "    \"\"\"Create New Dataset by Subsetting on Train.\"\"\"\n",
    "    new = {}\n",
    "\n",
    "    n_train = len(train['item'])\n",
    "    assert 0 <= begin < n_train\n",
    "    \n",
    "    # Prevent out of range size\n",
    "    if begin + size > n_train:\n",
    "        size = n_train - begin\n",
    "        \n",
    "    for k in train.keys():\n",
    "        if k == 'item':\n",
    "            # 1D array\n",
    "            new[k] = train[k][begin:begin+size]\n",
    "        else:\n",
    "            # 2D array\n",
    "            dim_n = train[k].shape[1]\n",
    "            new[k] = tf.slice(train[k], begin=[begin, 0], size=[size, dim_n])\n",
    "\n",
    "    new['cond'] = None\n",
    "    new['phoneme'] = helper.get_batch_pronunciations_fast(new['pho'])\n",
    "    return new\n",
    "\n",
    "batch_size = 500\n",
    "for i in range((n//batch_size) + 1):\n",
    "    print(f\"{i}: {i*batch_size}, {min(n, (i+1)*batch_size)}\")\n",
    "    tmp = subset_train(train, batch_size*i, batch_size)\n",
    "    data_wrangling.save_testset(tmp, f\"dataset/testsets/train_batch_{i}.pkl.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repackage homophony testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nh = data_wrangling.load_testset('non_homophone')\n",
    "h = data_wrangling.load_testset('homophone')\n",
    "\n",
    "# Sample 100 from each condition\n",
    "h_sel_id = sample(range(len(h['item'])), 100)\n",
    "nh_sel_id = sample(range(len(nh['item'])), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophony = {}\n",
    "\n",
    "homophony['item'] = []\n",
    "homophony['cond'] = []\n",
    "\n",
    "for i in h_sel_id:\n",
    "    homophony['item'].append(h['item'][i])\n",
    "    homophony['cond'].append('homophone')\n",
    "\n",
    "for i in nh_sel_id:\n",
    "    homophony['item'].append(nh['item'][i])\n",
    "    homophony['cond'].append('non_homophone')\n",
    "\n",
    "homophony['ort'] = np.concatenate([h['ort'][h_sel_id], nh['ort'][nh_sel_id]], axis=0)\n",
    "homophony['pho'] = np.concatenate([h['pho'][h_sel_id], nh['pho'][nh_sel_id]], axis=0)\n",
    "homophony['sem'] = np.concatenate([h['sem'][h_sel_id], nh['sem'][nh_sel_id]], axis=0)\n",
    "\n",
    "homophony['phoneme'] = helper.get_batch_pronunciations_fast(homophony['pho'])\n",
    "\n",
    "data_wrangling.save_testset(homophony, 'dataset/testsets/homophony.pkl.gz')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
