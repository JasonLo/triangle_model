{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine a single word  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/triangle_model')\n",
    "import troubleshooting \n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intactive plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_name = 'OP'\n",
    "# code_name = 'Refrac3_local'\n",
    "# code_name = 'Refrac3_after6'\n",
    "# code_name = 'Refrac3_after6_nodc'\n",
    "# code_name = 'Refrac3_after6_init_act0'\n",
    "# code_name = 'Refrac3_after6_informative'\n",
    "# code_name = 'OS_ff'\n",
    "code_name = 'triangle_nodc_osop'\n",
    "testset_name = 'train_r100'\n",
    "d = troubleshooting.Diagnosis(code_name)\n",
    "d.eval(testset_name, task='triangle', epoch=200)\n",
    "\n",
    "@interact(\n",
    "    sel_word=d.testset_package['item'], \n",
    "    layer=['pho', 'sem'], \n",
    "    task=['exp_os_ff', 'triangle', 'ort_pho', 'exp_osp', 'ort_sem', 'exp_ops'], \n",
    "    epoch=(10, d.cfg.total_number_of_epoch + 1, d.cfg.save_freq)\n",
    "    )\n",
    "def interactive_plot(sel_word, layer, task, epoch):\n",
    "    d = troubleshooting.Diagnosis(code_name)\n",
    "    d.eval(testset_name, task=task, epoch=epoch)\n",
    "    d.set_target_word(sel_word)\n",
    "    print(f\"Output phoneme over timeticks: {d.list_output_phoneme}\")\n",
    "    return d.plot_one_layer(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In an intact OP model, ignoring the non informative ticks, the general pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We found a very weird SS-1 having a very high input in off nodes as compared to on nodes\n",
    "- This pattern is also exist in PHO layer\n",
    "Consider PHO layer in a OP model\n",
    "\n",
    "1) Tick 1 is non-informative (No information from O had propagated to P yet) --> tick 1 is fixed across words\n",
    "2) All input is coming from Act P at 0 (=0.5) matmul w_pp. This fixed value is summarized in below density plot\n",
    "3) Functionally it serve as the knowledge (obtain from pretraining) embedded in w_pp, reflecting some degree of direct (1-to-1) coherence coactivation in the phonological layer\n",
    "4) Put in the more general context, bias, PP, and CP reflects probably(?) reflects -> independent activation likelihood in each node, immediate coherence coactivation (1-to-1), and slightly more complex (because it goes through a hidden layer) (but compressed) coherence coactivation???\n",
    "5) Their relative strength is summarized in below density plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tick 1 input density at SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hos_hs = d.get_weight(name=\"w_hos_hs\")\n",
    "w_ss = d.get_weight(name=\"w_ss\")\n",
    "bias_s = d.get_weight(name=\"bias_s\")\n",
    "w_cs = d.get_weight(name=\"w_cs\")\n",
    "\n",
    "data = {\n",
    "    'ss1': 0.5 * np.sum(w_ss, axis=0), # Lazy matmul. \n",
    "    'cs1': 0.5 * np.sum(w_cs, axis=0),\n",
    "    'os1': 0.5 * np.sum(w_hos_hs, axis=0),\n",
    "    'bias1': bias_s\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,6))\n",
    "for i, k in enumerate(data.keys()):\n",
    "    df[k].plot.density(ax=ax[i], title=k)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tick 1 input density at PHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_hop_hp = d.get_weight(name=\"w_hop_hp\")\n",
    "w_pp = d.get_weight(name=\"w_pp\")\n",
    "bias_p = d.get_weight(name=\"bias_p\")\n",
    "w_cp = d.get_weight(name=\"w_cp\")\n",
    "\n",
    "data = {\n",
    "    'pp1': 0.5 * np.sum(w_pp, axis=0), # Lazy matmul. \n",
    "    'cp1': 0.5 * np.sum(w_cp, axis=0),\n",
    "    'op1': 0.5 * np.sum(w_hop_hp, axis=0),\n",
    "    'bias1': bias_p\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(15,6))\n",
    "for i, k in enumerate(data.keys()):\n",
    "    df[k].plot.density(ax=ax[i], title=k)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
