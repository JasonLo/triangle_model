{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suspect of too low accuracy in Lesion model\n",
    "Vocabulary: \n",
    "- boardcasting: when adding tensor with different dimension, for example input_p (dim = [batch_size, 250]) and bias_p (dim = [1, 250]), bias_p will automatically \"boardcast\" to (batch_size, 250) during the elementwise addition\n",
    "\n",
    "When input information have not reach a layer, it will assume the first axis (batch_size) = 1 \n",
    "however, the batch_size won't matter... since tf.matmul batch_axis is independent with each other...  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import os, random\n",
    "import meta, modeling, data_wrangling, evaluate\n",
    "from importlib import reload\n",
    "reload(modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDiagnosis:\n",
    "    \"\"\"A diagnoistic bundle to trouble shot activation and input in semantic layer\"\"\"\n",
    "    def __init__(self, code_name:str):\n",
    "        self.cfg = meta.ModelConfig.from_json(\n",
    "            os.path.join(\"models\", code_name, \"model_config.json\")\n",
    "        )\n",
    "        \n",
    "    def eval(self, testset_name:str, task:str, epoch:int):\n",
    "        self.testset_package = data_wrangling.load_testset(os.path.join('dataset', 'testsets', f\"{testset_name}.pkl.gz\"))\n",
    "        \n",
    "        # Force the \n",
    "        self.cfg.output_ticks = 13\n",
    "        self.model = modeling.MyModel(self.cfg)\n",
    "        self.model.load_weights(self.cfg.saved_weights_fstring.format(epoch=epoch))\n",
    "        self.model.set_active_task(\"triangle\")\n",
    "        self.y_pred = self.model([self.testset_package['ort']] * self.cfg.n_timesteps)\n",
    "\n",
    "        # Get data for evaluate object\n",
    "        self.testset = evaluate.TestSet(self.cfg)\n",
    "        self.df = self.testset.eval(testset_name, task)\n",
    "\n",
    "    def set_target_word(self, word:str):\n",
    "        self.word_df = self.make_semantic_output_diagnostic_df(word)\n",
    "\n",
    "    @property\n",
    "    def all_outputs(self) -> list:\n",
    "        return list(self.y_pred.keys())\n",
    "\n",
    "    @property\n",
    "    def all_weights(self) -> list:\n",
    "        return [w.name for w in self.model.weights]\n",
    "\n",
    "    @property\n",
    "    def all_words(self) -> list:\n",
    "        return list(self.df.word.unique())\n",
    "\n",
    "    def make_semantic_output_diagnostic_df(self, target_word:str) -> pd.DataFrame:\n",
    "        \"\"\"Output all Semantic related input and activation in a word\"\"\"\n",
    "        \n",
    "        target_word_idx = self.testset_package['item'].index(target_word)\n",
    "\n",
    "        # Time invariant elements\n",
    "        df_dict = {}\n",
    "        df_dict['target_act'] = self.testset_package['sem'][target_word_idx, :].numpy()\n",
    "        df_dict['bias'] = [w.numpy() for w in self.model.weights if w.name.endswith('bias_s:0')][0]\n",
    "        df_time_invar = pd.DataFrame.from_dict(df_dict)\n",
    "        df_time_invar['unit'] = df_time_invar.index\n",
    "        df_time_invar['word'] = target_word\n",
    "\n",
    "        # Time varying elements\n",
    "        df_time_varying = pd.DataFrame()\n",
    "        NAME_MAP = {'input_hps_hs':'PS', 'input_css_cs':'CS', 'input_sem_ss':'SS', 'input_hos_hs':'OS', 'sem':'act_sem'}\n",
    "        \n",
    "        for i, input_name in enumerate(('input_hps_hs', 'input_css_cs', 'input_sem_ss', 'input_hos_hs', 'sem')):\n",
    "            this_input_df = pd.DataFrame()\n",
    "            for t in range(13):\n",
    "                df_dict = {}\n",
    "                df_dict[NAME_MAP[input_name]] = self.y_pred[input_name][t,target_word_idx,:].numpy()\n",
    "                this_step_df = pd.DataFrame.from_dict(df_dict)\n",
    "                this_step_df['timetick'] = t\n",
    "                this_step_df['unit'] = this_step_df.index\n",
    "                this_input_df = pd.concat([this_input_df, this_step_df])\n",
    "\n",
    "            if i == 0:\n",
    "                df_time_varying = this_input_df\n",
    "            else:\n",
    "                df_time_varying = pd.merge(df_time_varying, this_input_df, on=['timetick', 'unit'])\n",
    "\n",
    "        # Merge and export\n",
    "        df = df_time_varying.merge(df_time_invar, on='unit', how='left')\n",
    "        return df[['word', 'unit', 'timetick', 'target_act', 'act_sem', 'bias', 'OS', 'PS', 'CS', 'SS']]\n",
    "\n",
    "    \n",
    "    def plot_diagnosis(self, target_act: int) -> alt.Chart:\n",
    "        \"\"\"Plot all in one diagnosis\"\"\"\n",
    "\n",
    "        df = self.word_df.loc[self.word_df.target_act == target_act]\n",
    "        df = df.melt(id_vars=['word', 'unit', 'timetick', 'target_act'], value_vars=['act_sem', 'CS', 'OS', 'PS', 'SS', 'bias'])\n",
    "        sel_units = list(df.unit.unique())\n",
    "\n",
    "        # Random sample if there are too many nodes\n",
    "        if len(sel_units) > 20:\n",
    "            sample_units = random.sample(sel_units, 20)\n",
    "            df = df.loc[df.unit.isin(sample_units)]\n",
    "\n",
    "        sel_var = alt.selection_multi(fields=[\"variable\"], bind=\"legend\")\n",
    "        sel_unit = alt.selection_multi(fields=[\"unit\"], bind=\"legend\")\n",
    "\n",
    "        plot_input_pathway = (\n",
    "            alt.Chart(df.loc[df.variable != \"act_sem\"])\n",
    "            .mark_line(point=True)\n",
    "            .encode(\n",
    "                y=\"mean(value):Q\",\n",
    "                x=\"timetick\",\n",
    "                color=\"variable\",\n",
    "                opacity=alt.condition(sel_var, alt.value(1), alt.value(0.1)),\n",
    "            )\n",
    "            .add_selection(sel_var)\n",
    "        )\n",
    "\n",
    "        plot_input_unit = (\n",
    "            plot_input_pathway.encode(\n",
    "                color=\"unit:N\",\n",
    "                opacity=alt.condition(sel_unit, alt.value(1), alt.value(0.1)),\n",
    "            )\n",
    "            .transform_filter(sel_var)\n",
    "            .add_selection(sel_unit)\n",
    "        )\n",
    "\n",
    "        plot_act_unit = (\n",
    "            alt.Chart(df.loc[df.variable == \"act_sem\"])\n",
    "            .mark_line(point=True)\n",
    "            .encode(\n",
    "                y=alt.Y(\"value:Q\", scale=alt.Scale(domain=(0, 1))),\n",
    "                x=\"timetick\",\n",
    "                color=alt.Color(\"unit:N\", legend=None),\n",
    "                opacity=alt.condition(sel_unit, alt.value(1), alt.value(0.1)),\n",
    "                tooltip=[\"unit\"],\n",
    "            )\n",
    "            .add_selection(sel_unit)\n",
    "            .properties(title=\"Activation time course in each unit (with target node 1)\")\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            plot_input_pathway.properties(\n",
    "                title=\"Input time course in each pathway (with target node 1)\"\n",
    "            )\n",
    "            | plot_input_unit.properties(\n",
    "                title=\"Input time course in each unit (with target node 1)\"\n",
    "            )\n",
    "        ).resolve_scale(color=\"independent\", y=\"shared\") | plot_act_unit.resolve_scale(y=\"independent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem = SemanticDiagnosis(code_name='Refrac_5M_fix')\n",
    "sem.eval(testset_name='train_r100', task='triangle', epoch=400)\n",
    "sem.set_target_word('staffs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.plot_diagnosis(target_act=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem.plot_diagnosis(target_act=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_unit_input(tensor, mask=None):\n",
    "    \"\"\"Mask with output target, reduce sum on items (axis1), mean on units (axis2)\"\"\"\n",
    "    if mask is not None:\n",
    "        tensor = tf.multiply(tensor, mask)\n",
    "    return tf.reduce_mean(tf.reduce_mean(tensor, axis=-1), axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_dict = {k: mean_unit_input(globals()[k], train100['sem']) for k in ('os', 'ps', 'cs', 'ss')}\n",
    "results_dict['op'] = mean_unit_input(op, train100['pho'])\n",
    "dol_df = pd.DataFrame.from_dict(results_dict)\n",
    "dol_df.plot(title=\"one target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {k: mean_unit_input(globals()[k], 1-train100['sem']) for k in ('os', 'ps', 'cs', 'ss')}\n",
    "results_dict['op'] = mean_unit_input(op, 1-train100['pho'])\n",
    "dol_df = pd.DataFrame.from_dict(results_dict)\n",
    "dol_df.plot(title=\"zero target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_s = [w for w in model.weights if w.name == \"my_model/bias_s:0\"]\n",
    "bias_p = [w for w in model.weights if w.name == \"my_model/bias_p:0\"]\n",
    "print(f\"mean bias S: {tf.reduce_mean(bias_s)}; P: {tf.reduce_mean(bias_p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kind of neutral P bias\n",
    "- Positive bias in S??? is counter int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
