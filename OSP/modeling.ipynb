{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modeling.py\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, initializers, regularizers, Model\n",
    "from tensorflow.keras.layers import Layer, Input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Older Semantic\n",
    "# def input_s(e, t, f, i,\n",
    "#             gf=4, gi=1,\n",
    "#             kf=100, ki=100,\n",
    "#             hf=5, hi=5,\n",
    "#             tmax=3.8,\n",
    "#             mf=4.4743, sf=2.4578,\n",
    "#             mi=4.1988, si=1.0078):\n",
    "\n",
    "#     zf = (np.log(f + 2) - mf) / sf\n",
    "#     numer_f = gf * e * (zf + hf)\n",
    "#     denom_f = e * (zf + hf) + kf\n",
    "\n",
    "#     zi = (i - mi) / si\n",
    "#     numer_i = gi * e * (zi + hi)\n",
    "#     denom_i = e * (zi + hi) + ki\n",
    "\n",
    "#     return (t / tmax) * ((numer_f / denom_f) + (numer_i / denom_i))\n",
    "\n",
    "def input_s(e, t, f, i, gf, gi, kf, ki, tmax=3.8,\n",
    "            mf=4.4743, sf=2.4578, mi=4.1988, si=1.0078, hf=0, hi=0):\n",
    "    # Semantic refresh V1\n",
    "\n",
    "    numer_f = gf * e * np.log(f+2)\n",
    "    denom_f = e * np.log(f+2) + kf\n",
    "\n",
    "    return (t/tmax)*(numer_f/denom_f)\n",
    "\n",
    "# def input_s(e,\n",
    "#             t,\n",
    "#             f,\n",
    "#             i,\n",
    "#             gf,\n",
    "#             gi,\n",
    "#             kf,\n",
    "#             ki,\n",
    "#             tmax=3.8,\n",
    "#             mf=4.4743,\n",
    "#             sf=2.4578,\n",
    "#             mi=4.1988,\n",
    "#             si=1.0078,\n",
    "#             hf=5,\n",
    "#             hi=5):\n",
    "#     # Semantic refresh V2\n",
    "#     numer_f = gf * np.sqrt(e) * np.log(f + 2)\n",
    "#     denom_f = np.sqrt(e) * np.log(f + 2) + kf\n",
    "\n",
    "#     return (t / tmax) * (numer_f / denom_f)\n",
    "\n",
    "\n",
    "# def input_s(\n",
    "#     e,\n",
    "#     t,\n",
    "#     f,\n",
    "#     i,\n",
    "#     gf,\n",
    "#     gi,\n",
    "#     kf,\n",
    "#     ki,\n",
    "#     tmax=3.8,\n",
    "#     mf=4.4743,\n",
    "#     sf=2.4578,\n",
    "#     mi=4.1988,\n",
    "#     si=1.0078,\n",
    "#     hf=5,\n",
    "#     hi=5\n",
    "# ):\n",
    "#     # Semantic refresh V3\n",
    "#     numer_e = gf * e\n",
    "#     denom_e = e + kf\n",
    "\n",
    "#     return (t / tmax) * ((numer_e / denom_e) + 0.1 * np.log(f + 2))\n",
    "\n",
    "\n",
    "class rnn(Layer):\n",
    "    # Use keras rnn layer seems more efficient, maybe upgrade later...\n",
    "    def __init__(self, cfg, input_p_dignostic=False, name='rnn', **kwargs):\n",
    "\n",
    "        super(rnn, self).__init__(**kwargs)\n",
    "\n",
    "        self.cfg = cfg\n",
    "        self.input_p_dignostic = input_p_dignostic\n",
    "\n",
    "        self.rnn_activation = activations.get(self.cfg.rnn_activation)\n",
    "        self.weight_regularizer = regularizers.l2(cfg.regularizer_const)\n",
    "\n",
    "        self.w_oh = self.add_weight(\n",
    "            name='w_oh',\n",
    "            shape=(self.cfg.o_input_dim, self.cfg.hidden_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_hp = self.add_weight(\n",
    "            name='w_hp',\n",
    "            shape=(self.cfg.hidden_units, self.cfg.pho_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_pp = self.add_weight(\n",
    "            name='w_pp',\n",
    "            shape=(self.cfg.pho_units, self.cfg.pho_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_pc = self.add_weight(\n",
    "            name='w_pc',\n",
    "            shape=(self.cfg.pho_units, self.cfg.cleanup_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_cp = self.add_weight(\n",
    "            name='w_cp',\n",
    "            shape=(self.cfg.cleanup_units, self.cfg.pho_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.bias_h = self.add_weight(\n",
    "            shape=(self.cfg.hidden_units, ),\n",
    "            name='bias_h',\n",
    "            initializer='zeros',\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.bias_p = self.add_weight(\n",
    "            shape=(self.cfg.pho_units, ),\n",
    "            name='bias_p',\n",
    "            initializer='zeros',\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.bias_c = self.add_weight(\n",
    "            shape=(self.cfg.cleanup_units, ),\n",
    "            name='bias_c',\n",
    "            initializer='zeros',\n",
    "            regularizer=self.weight_regularizer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # If input_p_dignostic = True, it will output input_p instead of act_p (for troubleshooting)\n",
    "        # Hack for complying keras.layers.concatenate() format\n",
    "        # Dimension note: (batch, timestep, input_dim)\n",
    "        # Spliting input_dim below (index = 2)\n",
    "        if self.cfg.use_semantic == True:\n",
    "            o_input, s_input = tf.split(\n",
    "                inputs, [self.cfg.o_input_dim, self.cfg.pho_units], 2\n",
    "            )\n",
    "        else:\n",
    "            o_input = inputs\n",
    "\n",
    "        ### Trial level init ###\n",
    "        self.input_h_list = []\n",
    "        self.input_p_list = []\n",
    "        self.input_c_list = []\n",
    "\n",
    "        self.act_h_list = []\n",
    "        self.act_p_list = []\n",
    "        self.act_c_list = []\n",
    "\n",
    "        # Set inputs to 0\n",
    "        self.input_h_list.append(\n",
    "            tf.zeros((1, self.cfg.hidden_units), dtype=tf.float32)\n",
    "        )\n",
    "        self.input_p_list.append(\n",
    "            tf.zeros((1, self.cfg.pho_units), dtype=tf.float32)\n",
    "        )\n",
    "        self.input_c_list.append(\n",
    "            tf.zeros((1, self.cfg.cleanup_units), dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        # Set activations to 0.5\n",
    "        self.act_h_list.append(self.input_h_list[0] + 0.5)\n",
    "        self.act_p_list.append(self.input_p_list[0] + 0.5)\n",
    "        self.act_c_list.append(self.input_c_list[0] + 0.5)\n",
    "\n",
    "        for t in range(1, self.cfg.n_timesteps + 1):\n",
    "            # Inject noise to weights in each time step\n",
    "            # Method 1: Inject noise at each time step with reset\n",
    "            if self.cfg.w_oh_noise != 0:\n",
    "                w_oh = self.inject_noise(self.w_oh, self.cfg.w_oh_noise)\n",
    "            else:\n",
    "                w_oh = self.w_oh\n",
    "\n",
    "            if self.cfg.w_hp_noise != 0:\n",
    "                w_hp = self.inject_noise(self.w_hp, self.cfg.w_hp_noise)\n",
    "            else:\n",
    "                w_hp = self.w_hp\n",
    "\n",
    "            if self.cfg.w_pp_noise != 0:\n",
    "                w_pp = self.inject_noise(self.w_pp, self.cfg.w_pp_noise)\n",
    "            else:\n",
    "                w_pp = self.w_pp\n",
    "\n",
    "            if self.cfg.w_pc_noise != 0:\n",
    "                w_pc = self.inject_noise(self.w_pc, self.cfg.w_pc_noise)\n",
    "            else:\n",
    "                w_pc = self.w_pc\n",
    "\n",
    "            if self.cfg.w_cp_noise != 0:\n",
    "                w_cp = self.inject_noise(self.w_cp, self.cfg.w_cp_noise)\n",
    "            else:\n",
    "                w_cp = self.w_cp\n",
    "\n",
    "            ##### Hidden layer #####\n",
    "            oh = tf.matmul(o_input[:, t - 1, :], w_oh)\n",
    "            mem_h = self.input_h_list[t - 1]\n",
    "            h = self.cfg.tau * (oh + self.bias_h) + (1 - self.cfg.tau) * mem_h\n",
    "\n",
    "            self.input_h_list.append(h)\n",
    "            self.act_h_list.append(self.rnn_activation(h))\n",
    "\n",
    "            ##### Phonology layer #####\n",
    "            hp = tf.matmul(self.act_h_list[t - 1], w_hp)\n",
    "            pp = tf.matmul(\n",
    "                self.act_p_list[t - 1],\n",
    "                tf.linalg.set_diag(w_pp, tf.zeros(self.cfg.pho_units))\n",
    "            )  # Zero diagonal lock\n",
    "            cp = tf.matmul(self.act_c_list[t - 1], w_cp)\n",
    "\n",
    "            mem_p = self.input_p_list[t - 1]\n",
    "\n",
    "            if self.cfg.use_semantic == True:  # Inject semantic input\n",
    "                sp = s_input[:, t - 1, :]\n",
    "            else:\n",
    "                sp = 0\n",
    "\n",
    "            p = self.cfg.tau * (hp + pp + cp + sp +\n",
    "                                self.bias_p) + (1 - self.cfg.tau) * mem_p\n",
    "\n",
    "            self.input_p_list.append(p)\n",
    "            self.act_p_list.append(self.rnn_activation(p))\n",
    "\n",
    "            ##### Cleanup layer #####\n",
    "            pc = tf.matmul(self.act_p_list[t - 1], w_pc)\n",
    "            mem_c = self.input_c_list[t - 1]\n",
    "            c = self.cfg.tau * (pc + self.bias_c) + (1 - self.cfg.tau) * mem_c\n",
    "\n",
    "            self.input_c_list.append(c)\n",
    "            self.act_c_list.append(self.rnn_activation(c))\n",
    "\n",
    "        if self.input_p_dignostic == True:\n",
    "            return self.input_p_list[1:]\n",
    "        else:\n",
    "            return self.act_p_list[1:]\n",
    "\n",
    "    def inject_noise(self, x, noise_sd):\n",
    "        noise = K.random_normal(shape=K.shape(x), mean=0., stddev=noise_sd)\n",
    "        return x + noise\n",
    "\n",
    "    def compute_output_shape(self):\n",
    "        return tensor_shape.as_shape(\n",
    "            [1, self.cfg.pho_units] + self.cfg.n_timesteps\n",
    "        )\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'custom_cfg': self.cfg}\n",
    "        base_config = super(rnn, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class attractor_rnn(Layer):\n",
    "    def __init__(self, cfg, clamp_steps=14, **kwargs):\n",
    "        super(attractor_rnn, self).__init__(**kwargs)\n",
    "        self._name = 'rnn'\n",
    "        self.cfg = cfg\n",
    "        self.clamp_steps = clamp_steps\n",
    "        self.rnn_activation = activations.get(self.cfg.rnn_activation)\n",
    "\n",
    "    def build(self, input_shape, **kwargs):\n",
    "\n",
    "        self.w_pp = self.add_weight(\n",
    "            name='w_pp',\n",
    "            shape=(self.cfg.pho_units, self.cfg.pho_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_pc = self.add_weight(\n",
    "            name='w_pc',\n",
    "            shape=(self.cfg.pho_units, self.cfg.cleanup_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.w_cp = self.add_weight(\n",
    "            name='w_cp',\n",
    "            shape=(self.cfg.cleanup_units, self.cfg.pho_units),\n",
    "            initializer=self.cfg.w_initializer,\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.bias_p = self.add_weight(\n",
    "            shape=(self.cfg.pho_units, ),\n",
    "            name='bias_p',\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "        self.bias_c = self.add_weight(\n",
    "            shape=(self.cfg.cleanup_units, ),\n",
    "            name='bias_c',\n",
    "            initializer='zeros',\n",
    "            trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        import tensorflow as tf\n",
    "\n",
    "        ### Trial level init ###\n",
    "        self.input_p_list = []\n",
    "        self.input_c_list = []\n",
    "\n",
    "        self.act_p_list = []\n",
    "        self.act_c_list = []\n",
    "\n",
    "        # Initialize input at step 0 to 3*input\n",
    "        self.input_p_list.append(inputs * 3)\n",
    "        self.input_c_list.append(\n",
    "            tf.zeros((1, self.cfg.cleanup_units), dtype=tf.float32)\n",
    "        )\n",
    "\n",
    "        # Initialize activations\n",
    "        self.act_p_list.append(self.rnn_activation(self.input_p_list[0]))\n",
    "        self.act_c_list.append(self.input_c_list[0] + 0.5)\n",
    "\n",
    "        for t in range(1, self.cfg.n_timesteps + 1):\n",
    "\n",
    "            # ##### Phonology layer #####\n",
    "            pp = tf.matmul(\n",
    "                self.act_p_list[t - 1],\n",
    "                tf.linalg.set_diag(self.w_pp, tf.zeros(self.cfg.pho_units))\n",
    "            )  # Zero diagonal lock\n",
    "            cp = tf.matmul(self.act_c_list[t - 1], self.w_cp)\n",
    "\n",
    "            mem_p = self.input_p_list[t - 1]\n",
    "            p = self.cfg.tau * (pp + cp +\n",
    "                                self.bias_p) + (1 - self.cfg.tau) * mem_p\n",
    "\n",
    "            self.input_p_list.append(p)\n",
    "\n",
    "            if self.cfg.n_timesteps <= self.clamp_steps:\n",
    "                act_p = inputs\n",
    "            else:\n",
    "                act_p = self.rnn_activation(p)\n",
    "\n",
    "            self.act_p_list.append(act_p)\n",
    "\n",
    "            ##### Cleanup layer #####\n",
    "            pc = tf.matmul(self.act_p_list[t - 1], self.w_pc)\n",
    "\n",
    "            mem_c = self.input_c_list[t - 1]\n",
    "            c = self.cfg.tau * (pc + self.bias_c) + (1 - self.cfg.tau) * mem_c\n",
    "\n",
    "            self.input_c_list.append(c)\n",
    "            self.act_c_list.append(self.rnn_activation(c))\n",
    "\n",
    "        return self.act_p_list[self.clamp_steps + 1:\n",
    "                              ]  # Can get forgetting curve?\n",
    "\n",
    "    def compute_output_shape(self):\n",
    "        n = self.cfg.n_timesteps - self.clamp_steps\n",
    "        return tensor_shape.as_shape([1, self.cfg.pho_units] + n)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'custom_cfg': self.cfg}\n",
    "        base_config = super(attractor_rnn, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class attractor():\n",
    "    # Attractor class is a model level object\n",
    "    # Since we had used a non-serializable custom rnn layer... we need to rebuild the model using build_model()\n",
    "    # If the attractor structure change please update build_model()\n",
    "\n",
    "    def __init__(self, attractor_cfg, h5_name):\n",
    "\n",
    "        self.cfg = attractor_cfg\n",
    "        self.build_model()\n",
    "        self.model.summary()\n",
    "\n",
    "        self.model.load_weights(self.cfg.path_weight_folder + h5_name)\n",
    "        rnn_layer = self.model.get_layer('rnn')\n",
    "        names = [weight.name for weight in rnn_layer.weights]\n",
    "        weights = self.model.get_weights()\n",
    "\n",
    "        for name, weight in zip(names, weights):\n",
    "            if name.endswith('w_pp:0'):\n",
    "                self.pretrained_w_pp = weight\n",
    "            if name.endswith('w_pc:0'):\n",
    "                self.pretrained_w_pc = weight\n",
    "            if name.endswith('w_cp:0'):\n",
    "                self.pretrained_w_cp = weight\n",
    "            if name.endswith('bias_p:0'):\n",
    "                self.pretrained_bias_p = weight\n",
    "            if name.endswith('bias_c:0'):\n",
    "                self.pretrained_bias_c = weight\n",
    "\n",
    "    def build_model(self):\n",
    "        clamp_steps = 14\n",
    "        input_o = Input(shape=(self.cfg.pho_units, ))\n",
    "        rnn_model = attractor_rnn(self.cfg, clamp_steps)(input_o)\n",
    "        self.model = Model(input_o, rnn_model)\n",
    "\n",
    "\n",
    "def arm_attractor(model, attractor):\n",
    "    # This function will load attractor weights (w_pp, w_pc, w_cp, bias_p, and bias_c) to model\n",
    "\n",
    "    n_matrices = len(model.get_layer('rnn').weights)\n",
    "    new_weights = []\n",
    "\n",
    "    for i in range(n_matrices):\n",
    "        # Align model and attractor weight matrices by creating new_weights list\n",
    "\n",
    "        # Get attractor value if weight matrix name match attractor\n",
    "        if model.get_layer('rnn').weights[i].name.endswith('w_pp:0'):\n",
    "            new_weights.append(attractor.pretrained_w_pp)\n",
    "\n",
    "        if model.get_layer('rnn').weights[i].name.endswith('w_pc:0'):\n",
    "            new_weights.append(attractor.pretrained_w_pc)\n",
    "\n",
    "        if model.get_layer('rnn').weights[i].name.endswith('w_cp:0'):\n",
    "            new_weights.append(attractor.pretrained_w_cp)\n",
    "\n",
    "        if model.get_layer('rnn').weights[i].name.endswith('bias_p:0'):\n",
    "            new_weights.append(attractor.pretrained_bias_p)\n",
    "\n",
    "        if model.get_layer('rnn').weights[i].name.endswith('bias_c:0'):\n",
    "            new_weights.append(attractor.pretrained_bias_c)\n",
    "\n",
    "        # Fill original value if this slot have not been filled\n",
    "        if len(new_weights) < i + 1:\n",
    "            new_weights.append(model.get_weights()[i])\n",
    "\n",
    "    model.set_weights(new_weights)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_layer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
