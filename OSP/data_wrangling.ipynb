{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data_wrangling.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_wrangling.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def gen_pkey(p_file=\"../common/patterns/mappingv2.txt\"):\n",
    "    \"\"\"\n",
    "    Read phonological patterns from the mapping file\n",
    "    See Harm & Seidenberg PDF file\n",
    "    \"\"\"\n",
    "\n",
    "    mapping = pd.read_table(p_file, header=None, delim_whitespace=True)\n",
    "    m_dict = mapping.set_index(0).T.to_dict('list')\n",
    "    return m_dict\n",
    "\n",
    "\n",
    "class wf_manager():\n",
    "    # Note: the probability must sum to 1 when passing it to np.random.choice()\n",
    "    def __init__(self, wf):\n",
    "\n",
    "        self.wf = np.array(wf)\n",
    "\n",
    "    def wf(self):\n",
    "        return self.wf\n",
    "\n",
    "    def to_p(self, x):\n",
    "        return x / np.sum(x)\n",
    "\n",
    "    def samp_termf(self):\n",
    "        return self.to_p(self.wf)\n",
    "\n",
    "    def samp_log(self):\n",
    "        log = np.log(1 + self.wf)\n",
    "        return self.to_p(log)\n",
    "\n",
    "    def samp_hs04(self):\n",
    "        root = np.sqrt(self.wf) / np.sqrt(30000)\n",
    "        clip = root.clip(0.05, 1.0)\n",
    "        return self.to_p(clip)\n",
    "\n",
    "    def samp_jay(self):\n",
    "        cap = self.wf.clip(0, 10000)\n",
    "        root = np.sqrt(cap)\n",
    "        return self.to_p(root)\n",
    "\n",
    "\n",
    "# Input for training set\n",
    "def sample_generator(cfg, data):\n",
    "    # Dimension guide: (batch_size, timesteps, nodes)\n",
    "    from modeling import input_s\n",
    "\n",
    "    np.random.seed(cfg.sample_rng_seed)\n",
    "    epoch = 0\n",
    "    batch = 0\n",
    "\n",
    "    while True:\n",
    "        batch += 1\n",
    "        idx = np.random.choice(\n",
    "            range(len(data.sample_p)), cfg.batch_size, p=data.sample_p\n",
    "        )\n",
    "\n",
    "        # Preallocate for easier indexing: (batch_size, time_step, input_dim)\n",
    "        batch_s = np.zeros((cfg.batch_size, cfg.n_timesteps, cfg.pho_units))\n",
    "        batch_y = []\n",
    "\n",
    "        for t in range(cfg.n_timesteps):\n",
    "\n",
    "            if cfg.use_semantic == True:\n",
    "                input_s_cell = input_s(\n",
    "                    e=epoch,\n",
    "                    t=t * cfg.tau,\n",
    "                    f=data.wf[idx],\n",
    "                    i=data.img[idx],\n",
    "                    gf=cfg.sem_param_gf,\n",
    "                    gi=cfg.sem_param_gi,\n",
    "                    kf=cfg.sem_param_kf,\n",
    "                    ki=cfg.sem_param_ki,\n",
    "                    hf=cfg.sem_param_hf,\n",
    "                    hi=cfg.sem_param_hi,\n",
    "                    tmax=cfg.max_unit_time - cfg.tau\n",
    "                )  # Because output use zero indexing... we have to -1 step\n",
    "\n",
    "                batch_s[:, t, :] = np.tile(\n",
    "                    np.expand_dims(input_s_cell, 1), [1, cfg.pho_units]\n",
    "                )\n",
    "\n",
    "            batch_y.append(data.y_train[idx])\n",
    "\n",
    "        if batch % cfg.steps_per_epoch == 0:\n",
    "            epoch += 1  # Counting epoch for ramping up input S\n",
    "\n",
    "        if cfg.use_semantic == True:\n",
    "            yield (\n",
    "                [data.x_train[idx], batch_s, 2 * data.y_train[idx] - 1], batch_y\n",
    "            )  # With negative input signal\n",
    "        else:\n",
    "            yield (data.x_train[idx], batch_y)\n",
    "\n",
    "\n",
    "def test_set_input(\n",
    "    x_test, x_test_wf, x_test_img, y_test, epoch, cfg, test_use_semantic\n",
    "):\n",
    "    # We need to separate whether the model use semantic by cfg.use_semantic\n",
    "    # And whether the test set has semantic input by test_use_semantic\n",
    "    # If model use semantic, we need to return a list of 3 inputs (x, s[time step varying], y), otherwise (x) is enough\n",
    "    from modeling import input_s\n",
    "\n",
    "    if cfg.use_semantic:\n",
    "        batch_s = np.zeros(\n",
    "            (len(x_test), cfg.n_timesteps, cfg.pho_units)\n",
    "        )  # Fill batch_s with Plaut S formula\n",
    "        batch_y = np.zeros_like(y_test)\n",
    "\n",
    "        if test_use_semantic:\n",
    "            for t in range(cfg.n_timesteps):\n",
    "                s_cell = input_s(\n",
    "                    e=epoch,\n",
    "                    t=t * cfg.tau,\n",
    "                    f=x_test_wf,\n",
    "                    i=x_test_img,\n",
    "                    gf=cfg.sem_param_gf,\n",
    "                    gi=cfg.sem_param_gi,\n",
    "                    kf=cfg.sem_param_kf,\n",
    "                    ki=cfg.sem_param_ki,\n",
    "                    hf=cfg.sem_param_hf,\n",
    "                    hi=cfg.sem_param_hi,\n",
    "                    tmax=cfg.max_unit_time - cfg.tau\n",
    "                )  # Because of zero indexing\n",
    "                batch_s[:, t, :] = np.tile(\n",
    "                    np.expand_dims(s_cell, 1), [1, cfg.pho_units]\n",
    "                )\n",
    "\n",
    "            batch_y = 2 * y_test - 1  # With negative teaching signal\n",
    "\n",
    "        return [x_test, batch_s, batch_y]  # With negative teaching signal\n",
    "\n",
    "    else:\n",
    "        return x_test\n",
    "\n",
    "\n",
    "class my_data():\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        self.sample_name = cfg.sample_name\n",
    "\n",
    "        input_path = '../common/input/'\n",
    "\n",
    "        self.df_train = pd.read_csv(input_path + 'df_train.csv', index_col=0)\n",
    "        self.x_train = np.load(input_path + 'x_train.npz')['data']\n",
    "        self.y_train = np.load(input_path + 'y_train.npz')['data']\n",
    "\n",
    "        self.df_strain = pd.read_csv(input_path + 'df_strain.csv', index_col=0)\n",
    "        self.df_grain = pd.read_csv(input_path + 'df_grain.csv', index_col=0)\n",
    "\n",
    "        self.x_strain = np.load(input_path + 'x_strain.npz')['data']\n",
    "        self.x_strain_wf = np.array(self.df_strain['wf'])\n",
    "        self.x_strain_img = np.array(self.df_strain['img'])\n",
    "\n",
    "        self.x_grain = np.load(input_path + 'x_grain.npz')['data']\n",
    "        self.x_grain_wf = np.array(self.df_grain['wf'])\n",
    "        self.x_grain_img = np.array(self.df_grain['img'])\n",
    "\n",
    "        self.y_strain = np.load(input_path + 'y_strain.npz')['data']\n",
    "        self.y_large_grain = np.load(input_path + 'y_large_grain.npz')['data']\n",
    "        self.y_small_grain = np.load(input_path + 'y_small_grain.npz')['data']\n",
    "\n",
    "        from data_wrangling import gen_pkey\n",
    "        self.phon_key = gen_pkey()\n",
    "\n",
    "        print('==========Orthographic representation==========')\n",
    "        print('x_train shape:', self.x_train.shape)\n",
    "        print('x_strain shape:', self.x_strain.shape)\n",
    "        print('x_grain shape:', self.x_grain.shape)\n",
    "\n",
    "        print('\\n==========Phonological representation==========')\n",
    "        print(len(self.phon_key), ' phonemes: ', self.phon_key.keys())\n",
    "        print('y_train shape:', self.y_train.shape)\n",
    "        print('y_strain shape:', self.y_strain.shape)\n",
    "        print('y_large_grain shape:', self.y_large_grain.shape)\n",
    "        print('y_small_grain shape:', self.y_small_grain.shape)\n",
    "\n",
    "        self.gen_sample_p()\n",
    "        self.wf = np.array(self.df_train['wf'], dtype='float32')\n",
    "        self.img = np.array(self.df_train['img'], dtype='float32')\n",
    "\n",
    "    def gen_sample_p(self):\n",
    "        from data_wrangling import wf_manager\n",
    "        wf = wf_manager(self.df_train['wf'])\n",
    "        if self.sample_name == 'hs04':\n",
    "            self.sample_p = wf.samp_hs04()\n",
    "        if self.sample_name == 'jay':\n",
    "            self.sample_p = wf.samp_jay()\n",
    "        if self.sample_name == 'log':\n",
    "            self.sample_p = wf.samp_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read training file\n",
    "train_file = '../common/patterns/6ktraining.dict'\n",
    "\n",
    "strain_file = '../common/patterns/strain.txt'\n",
    "strain_key_file = '../common/patterns/strain_key.txt'\n",
    "\n",
    "grain_file = '../common/patterns/grain_nws.dict'\n",
    "grain_key_file = '../common/patterns/grain_key.txt'\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv('../common/patterns/cortese2004norms.csv', skiprows=9)\n",
    "img_map = cortese[['item', 'rating']]\n",
    "img_map.columns = ['word', 'img']\n",
    "\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho', 'wf'],\n",
    "    na_filter=False    # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of         word         ort         pho         wf       img\n",
      "0          a  ___a______  ___^______  1100290.0  4.199444\n",
      "1        ace  ___a_ce___  ___es_____      117.0  4.500000\n",
      "2       ache  ___a_che__  ___ek_____       27.0  3.800000\n",
      "3      ached  ___a_ched_  ___ekt____        3.0  4.199444\n",
      "4      aches  ___a_ches_  ___eks____       23.0  4.199444\n",
      "...      ...         ...         ...        ...       ...\n",
      "5865     zoo  __zoo_____  __zu______      172.0  6.800000\n",
      "5866    zoom  __zoom____  __zum_____       38.0  3.500000\n",
      "5867  zoomed  __zoomed__  __zumd____       44.0  4.199444\n",
      "5868   zooms  __zooms___  __zumz____       18.0  4.199444\n",
      "5869    zoos  __zoos____  __zuz_____       23.0  4.199444\n",
      "\n",
      "[5870 rows x 5 columns]>\n",
      "<bound method NDFrame.head of       word         ort         pho     wf frequency pho_consistency  \\\n",
      "0     ball  __ba_ll___  __bal_____   1393        HF             INC   \n",
      "1     bank  __ba_nk___  __b@nk____  53170        HF             CON   \n",
      "2    beach  __beach___  __biC_____   1691        HF             CON   \n",
      "3     beak  __beak____  __bik_____     10        LF             INC   \n",
      "4    beard  __beard___  __bird____    210        LF             INC   \n",
      "..     ...         ...         ...    ...       ...             ...   \n",
      "155   wool  __wool____  __wUl_____    153        LF             INC   \n",
      "156   worm  __wo_rm___  __w^rm____    100        LF             INC   \n",
      "157  worth  __wo_rth__  __w^rT____   5251        HF             INC   \n",
      "158  wrong  _wro_ng___  __rang____   3195        HF             CON   \n",
      "159   zone  __zo_ne___  __zon_____    721        LF             INC   \n",
      "\n",
      "    imageability  img  \n",
      "0             HI  6.6  \n",
      "1             HI  6.1  \n",
      "2             HI  6.5  \n",
      "3             HI  5.2  \n",
      "4             HI  6.3  \n",
      "..           ...  ...  \n",
      "155           HI  5.8  \n",
      "156           HI  6.6  \n",
      "157           LI  2.3  \n",
      "158           LI  2.4  \n",
      "159           LI  4.1  \n",
      "\n",
      "[160 rows x 8 columns]>\n",
      "<bound method NDFrame.head of        word         ort   pho_large   pho_small    condition  img  wf\n",
      "0    blance  _bla_nce__  _blens____  _bl@ns____  unambiguous    0   0\n",
      "1    brance  _bra_nce__  _brens____  _br@ns____  unambiguous    0   0\n",
      "2    crance  _cra_nce__  _krens____  _kr@ns____  unambiguous    0   0\n",
      "3    drance  _dra_nce__  _drens____  _dr@ns____  unambiguous    0   0\n",
      "4    quance  _qua_nce__  _kwens____  _kw@ns____  unambiguous    0   0\n",
      "..      ...         ...         ...         ...          ...  ...  ..\n",
      "115    pook  __pook____  __pUk_____  __puk_____    ambiguous    0   0\n",
      "116   plook  _plook____  _plUk_____  _pluk_____    ambiguous    0   0\n",
      "117   prook  _prook____  _prUk_____  _pruk_____    ambiguous    0   0\n",
      "118   slook  _slook____  _slUk_____  _sluk_____    ambiguous    0   0\n",
      "119   trook  _trook____  _trUk_____  _truk_____    ambiguous    0   0\n",
      "\n",
      "[120 rows x 7 columns]>\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.merge(train, img_map, on='word', how='left')\n",
    "\n",
    "strain = pd.read_csv(\n",
    "    strain_file, sep='\\t', header=None, names=['word', 'ort', 'pho', 'wf']\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'frequency', 'pho_consistency', 'imageability']\n",
    ")\n",
    "\n",
    "df_strain = pd.merge(strain, strain_key)\n",
    "df_strain = pd.merge(df_strain, img_map, on='word', how='left')\n",
    "\n",
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "df_grain = pd.merge(grain, grain_key)\n",
    "\n",
    "df_grain['img'] = 0\n",
    "df_grain['wf'] = 0\n",
    "\n",
    "\n",
    "def prepDF(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = prepDF(df_train)\n",
    "df_strain = prepDF(df_strain)\n",
    "df_grain = prepDF(df_grain)\n",
    "\n",
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train = df_train.fillna(mean_img)\n",
    "df_strain = df_strain.fillna(mean_img)\n",
    "\n",
    "df_train.to_csv('../common/input/df_train.csv')\n",
    "df_strain.to_csv('../common/input/df_strain.csv')\n",
    "df_grain.to_csv('../common/input/df_grain.csv')\n",
    "\n",
    "print(df_train.head)\n",
    "print(df_strain.head)\n",
    "print(df_grain.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print('In slot ', slotId, '\\t')\n",
    "            print('token count:', t.word_counts)\n",
    "            print('word count:', t.document_count)\n",
    "            print('dictionary:', t.word_index)\n",
    "            print('token appear in how many words:', t.word_docs)\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "\n",
    "    return binData\n",
    "\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort], ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=False)\n",
    "splitId_strain = len(df_train)\n",
    "splitId_grain = len(df_train) + len(df_strain)\n",
    "\n",
    "x_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "x_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "x_grain = np.array(all_ort_bin[splitId_grain::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/x_train.npz', data=x_train)\n",
    "np.savez_compressed('../common/input/x_strain.npz', data=x_strain)\n",
    "np.savez_compressed('../common/input/x_grain.npz', data=x_grain)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_strain shape:', x_strain.shape)\n",
    "print('x_grain shape:', x_grain.shape)\n",
    "\n",
    "\n",
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    n = len(p_col)\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([n, binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        outSeries = slotSeries.map(p_key)\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = outSeries.to_list()\n",
    "    return p_output\n",
    "\n",
    "\n",
    "phon_key = gen_pkey()\n",
    "y_train = pho2bin_v2(train.pho, phon_key)\n",
    "y_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "y_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "y_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/y_train.npz', data=y_train)\n",
    "np.savez_compressed('../common/input/y_strain.npz', data=y_strain)\n",
    "np.savez_compressed('../common/input/y_large_grain.npz', data=y_large_grain)\n",
    "np.savez_compressed('../common/input/y_small_grain.npz', data=y_small_grain)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_strain shape:', y_strain.shape)\n",
    "print('y_large_grain shape:', y_large_grain.shape)\n",
    "print('y_small_grain shape:', y_small_grain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('../common/input/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "from data_wrangling import wf_manager\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "sortwf = wf_manager(plot_f['wf'])\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(sortwf.wf, sortwf.samp_log(), label='HS04')\n",
    "line2, = ax.plot(sortwf.wf, sortwf.samp_hs04(), label='JAY')\n",
    "line3, = ax.plot(sortwf.wf, sortwf.samp_jay(), label='LOG')\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
