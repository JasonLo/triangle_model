{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbgouQwZ1Y1f"
   },
   "source": [
    "# OSP model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "%load_ext lab_black\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from meta import gpu_mem_cap\n",
    "\n",
    "gpu_mem_cap(2048)  # Put memory cap to allow parallel run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters block for Papermill\n",
    "- Instead of using model_cfg directly, this extra step is needed for batch run using Papermill\n",
    "- Consider carefully the variable type in each cfg setting \n",
    "    - Do not use integer (e.g., 1, 2, 3, 0) in variables that can be float32 (e.g., w_oh_noise, tau...)\n",
    "    - Use integer with a dot instead (e.g., 1., 2., 3., 0.)\n",
    "- To use attractor, two params must be config, \n",
    "    - 1) embed_attractor_cfg --> json cfg file of the pretrain attractor \n",
    "    - 2) embed_attractor_h5 --> h5 file of the exact weight (e.g. ep0500.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = 'O2P_main_model_boo'\n",
    "\n",
    "sample_name = 'hs04'\n",
    "sample_rng_seed = 329\n",
    "tf_rng_seed = 123\n",
    "\n",
    "# Semantic formula\n",
    "use_semantic = False\n",
    "sem_param_gf = 0.\n",
    "sem_param_gi = 0.\n",
    "sem_param_kf = 0.\n",
    "sem_param_ki = 0.\n",
    "sem_param_hf = 0.\n",
    "sem_param_hi = 0.\n",
    "\n",
    "# Model architechture\n",
    "o_input_dim = 119\n",
    "hidden_units = 100\n",
    "pho_units = 250\n",
    "cleanup_units = 50\n",
    "rnn_activation = 'sigmoid'\n",
    "regularizer_const = 0.\n",
    "\n",
    "embed_attractor_cfg = None\n",
    "embed_attractor_h5 = None\n",
    "\n",
    "# embed_attractor_cfg = 'models/Attractor_50/model_config.json'\n",
    "# embed_attractor_h5 = 'ep0500.h5'\n",
    "\n",
    "p_noise = 0.  # i.e. w_pp, w_pc, and w_cp noise\n",
    "tau = 0.2\n",
    "max_unit_time = 4.\n",
    "\n",
    "# Training\n",
    "n_mil_sample = 0.04\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "save_freq = 1\n",
    "\n",
    "# Results push to BQ database\n",
    "bq_dataset = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing parameters into model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Orthographic representation==========\n",
      "x_train shape: (5832, 119)\n",
      "x_strain shape: (160, 119)\n",
      "x_grain shape: (120, 119)\n",
      "\n",
      "==========Phonological representation==========\n",
      "38  phonemes:  dict_keys(['d', 'y', 'h', 'm', 'r', 'A', 'n', 'w', 'z', 't', 'E', 'U', 'k', 'o', 'D', 'T', '_', 'b', 'l', 's', 'O', 'u', '@', 'S', 'g', 'I', 'a', 'W', 'Z', 'f', 'v', '^', 'C', 'p', 'Y', 'i', 'J', 'e'])\n",
      "y_train shape: (5832, 250)\n",
      "y_strain shape: (160, 250)\n",
      "y_large_grain shape: (120, 250)\n",
      "y_small_grain shape: (120, 250)\n"
     ]
    }
   ],
   "source": [
    "from meta import model_cfg\n",
    "\n",
    "cfg = model_cfg(\n",
    "    code_name=code_name,\n",
    "    sample_name=sample_name,\n",
    "    sample_rng_seed=sample_rng_seed,\n",
    "    tf_rng_seed=tf_rng_seed,\n",
    "    use_semantic=use_semantic,\n",
    "    sem_param_gf=sem_param_gf,\n",
    "    sem_param_gi=sem_param_gi,\n",
    "    sem_param_kf=sem_param_kf,\n",
    "    sem_param_ki=sem_param_ki,\n",
    "    sem_param_hf=sem_param_hf,\n",
    "    sem_param_hi=sem_param_hi,\n",
    "    o_input_dim=o_input_dim,\n",
    "    hidden_units=hidden_units,\n",
    "    pho_units=pho_units,\n",
    "    cleanup_units=cleanup_units,\n",
    "    embed_attractor_cfg=embed_attractor_cfg,\n",
    "    embed_attractor_h5=embed_attractor_h5,\n",
    "    w_oh_noise=0.,\n",
    "    w_hp_noise=0.,\n",
    "    w_pp_noise=p_noise,\n",
    "    w_pc_noise=p_noise,\n",
    "    w_cp_noise=p_noise,\n",
    "    tau=tau,\n",
    "    max_unit_time=max_unit_time,\n",
    "    n_mil_sample=n_mil_sample,\n",
    "    batch_size=batch_size,\n",
    "    rnn_activation=rnn_activation,\n",
    "    regularizer_const=regularizer_const,\n",
    "    learning_rate=learning_rate,\n",
    "    save_freq=save_freq,\n",
    "    bq_dataset=bq_dataset\n",
    ")\n",
    "\n",
    "# TF random seed (Sampling is out of TF scope... change sample_rng_seed instead)\n",
    "tf.random.set_seed(cfg.tf_rng_seed)\n",
    "\n",
    "# Preload data\n",
    "from data_wrangling import sample_generator, my_data\n",
    "data = my_data(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporary merge saved epoch list instead of retaining..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempo\n",
    "saved_epo = list(range(1, 5)) + list(range(5, 101, 5))\n",
    "cfg.path_weights_list = []\n",
    "cfg.saved_epoch_list = []\n",
    "for epoch in saved_epo:\n",
    "    cfg.path_weights_list += [\n",
    "        cfg.path_weight_folder + 'ep' + str(epoch).zfill(4) + '.h5'\n",
    "    ]\n",
    "\n",
    "    cfg.saved_epoch_list.append(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IwxbE29_0yx"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFTsfceXUGIh",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(training=True):\n",
    "    # Organization principal:\n",
    "    # Structure things, such as repeat vector should build within the model\n",
    "    # Static calculation of input --> Easier to modify --> build within sample generator\n",
    "\n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.layers import Layer, Input, concatenate, multiply, RepeatVector\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from modeling import rnn\n",
    "    #     from modeling_without_cleanup import rnn_no_cleanup_no_pp\n",
    "\n",
    "    # Train/test mode checking\n",
    "    cfg.noise_on() if training is True else cfg.noise_off()\n",
    "\n",
    "    input_o = Input(shape=(cfg.o_input_dim, ), name=\"Input_O\")\n",
    "    input_o_t = RepeatVector(cfg.n_timesteps, name='Input_Ot')(input_o)\n",
    "\n",
    "    if cfg.use_semantic == True:\n",
    "        raw_s_t = Input(shape=(cfg.n_timesteps, cfg.pho_units), name='Plaut_St')\n",
    "\n",
    "        input_p = Input(shape=(cfg.pho_units, ), name='input_P')\n",
    "        input_p_t = RepeatVector(cfg.n_timesteps, name='Teaching_Pt')(input_p)\n",
    "\n",
    "        input_s_t = multiply([raw_s_t, input_p_t], name='Input_St')\n",
    "\n",
    "        combined = concatenate([input_o_t, input_s_t], name='Combined_input')\n",
    "        rnn_model = rnn(cfg)(combined)\n",
    "        model = Model([input_o, raw_s_t, input_p], rnn_model)\n",
    "\n",
    "    else:\n",
    "        rnn_model = rnn(cfg)(input_o_t)\n",
    "        model = Model(input_o, rnn_model)\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(\n",
    "            learning_rate=cfg.learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            amsgrad=False\n",
    "        ),\n",
    "        metrics=['BinaryAccuracy', 'mse']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "# model = build_model(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arming attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if cfg.embed_attractor_cfg is not None:\n",
    "#     print('Found attractor info in config (cfg), arming attractor...')\n",
    "#     from modeling import attractor, arm_attractor\n",
    "#     from evaluate import plot_variables\n",
    "\n",
    "#     attractor_cfg = model_cfg(None)\n",
    "#     attractor_cfg.load_cfg_json(cfg.embed_attractor_cfg)\n",
    "#     attractor_obj = attractor(attractor_cfg, cfg.embed_attractor_h5)\n",
    "\n",
    "#     model = arm_attractor(model, attractor_obj)\n",
    "#     plot_variables(model)\n",
    "# else:\n",
    "#     print('Config indicates no attractor, I have do nothing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKUOoZkP8QiA"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import h5py, pickle, os\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# from data_wrangling import sample_generator\n",
    "# from IPython.display import clear_output\n",
    "\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     cfg.path_weights_checkpoint,\n",
    "#     verbose=1,\n",
    "#     save_freq=cfg.save_freq_sample,\n",
    "#     save_weights_only=True\n",
    "# )\n",
    "\n",
    "# history = model.fit(\n",
    "#     sample_generator(cfg, data),\n",
    "#     steps_per_epoch=cfg.steps_per_epoch,\n",
    "#     epochs=cfg.nEpo,\n",
    "#     verbose=0,\n",
    "#     callbacks=[checkpoint]\n",
    "# )\n",
    "\n",
    "# # Saving history and model\n",
    "# pickle_out = open(cfg.path_history_pickle, \"wb\")\n",
    "# pickle.dump(history.history, pickle_out)\n",
    "# pickle_out.close()\n",
    "\n",
    "# clear_output()\n",
    "# print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMmNcbJdcPMh"
   },
   "source": [
    "# Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BqpDOQStugK"
   },
   "source": [
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import training_history\n",
    "\n",
    "# hist = training_history(cfg.path_history_pickle)\n",
    "# hist.plot_all(cfg.path_plot_folder + 'history.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test set: 4.0%\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'models/O2P_main_model_boo/weights/ep0001.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-983f2e49d33e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m strain.start_evaluate(\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_use_semantic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_model_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'result_strain_item.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m~/tf/OSP/evaluate.py\u001b[0m in \u001b[0;36mstart_evaluate\u001b[0;34m(self, test_use_semantic, output)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_epoch_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_h5_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             test_input = test_set_input(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1213\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   1214\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'models/O2P_main_model_boo/weights/ep0001.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Must turn training mode off before evaluation\n",
    "model = build_model(training=False)\n",
    "from evaluate import strain_eval, grain_eval\n",
    "\n",
    "# Strain full model\n",
    "strain = strain_eval(cfg, data, model)\n",
    "strain.start_evaluate(\n",
    "    test_use_semantic=True,\n",
    "    output=cfg.path_model_folder + 'result_strain_item.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic lesion in Strain\n",
    "# strain_ns = strain_eval(cfg, data, model)\n",
    "# strain_ns.start_evaluate(\n",
    "#     test_use_semantic=False,\n",
    "#     output=cfg.path_model_folder + 'result_strain_ns_item.csv'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain\n",
    "from evaluate import strain_eval, grain_eval\n",
    "grain = grain_eval(cfg, data, model)\n",
    "grain.start_evaluate(\n",
    "    test_use_semantic=False,\n",
    "    output=cfg.path_model_folder + 'result_grain_item.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KnxCNzMyWah"
   },
   "source": [
    "### Strain plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import vis\n",
    "\n",
    "# vis_ns = vis(\n",
    "#     cfg.path_model_folder, 'result_strain_ns_item.csv', 'result_grain_item.csv'\n",
    "# )\n",
    "# vis_ns.parse_cond_df()\n",
    "# lesion = vis_ns.plot_dev('acc').properties(title='Semantic lesion')\n",
    "# strain_plot = full | lesion\n",
    "# strain_plot\n",
    "\n",
    "vis = vis(\n",
    "    cfg.path_model_folder, 'result_strain_item.csv', 'result_grain_item.csv'\n",
    ")\n",
    "\n",
    "vis.parse_cond_df()\n",
    "\n",
    "full = vis.plot_dev_interactive('acc').properties(title='Full input')\n",
    "full.save(cfg.path_plot_folder + 'strain.html')\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesion development deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_inter = vis.plot_dev_interactive('acc')\n",
    "# dev_inter.save(cfg.path_plot_folder + 'interactive_strain_dev_full.html')\n",
    "# dev_inter\n",
    "\n",
    "# dev_inter = vis_ns.plot_dev_interactive('acc')\n",
    "# dev_inter.save(cfg.path_plot_folder + 'interactive_strain_dev_lesion.html')\n",
    "# dev_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesion time plot deep dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_inter = vis.plot_time_interactive('acc')\n",
    "# time_inter.save(cfg.path_plot_folder + 'interactive_strain_time.html')\n",
    "# time_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = vis.plot_dev_interactive('acc_small_grain', exp='grain')\n",
    "large = vis.plot_dev_interactive('acc_large_grain', exp='grain')\n",
    "grain_plot = small | large\n",
    "grain_plot.save(cfg.path_plot_folder + 'grain.html')\n",
    "grain_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluate import plot_variables\n",
    "# plot_variables(model, cfg.path_plot_folder + 'variables.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "basicO2P_master.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
