{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read training file\n",
    "train_file = '../common/patterns/6ktraining_v2.dict'\n",
    "\n",
    "strain_file = '../common/patterns/strain.txt'\n",
    "strain_key_file = '../common/patterns/strain_key.txt'\n",
    "\n",
    "grain_file = '../common/patterns/grain_nws.dict'\n",
    "grain_key_file = '../common/patterns/grain_key.txt'\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv('../common/patterns/cortese2004norms.csv', skiprows=9)\n",
    "img_map = cortese[['item', 'rating']]\n",
    "img_map.columns = ['word', 'img']\n",
    "\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho', 'wf'],\n",
    "    na_filter=\n",
    "    False  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(train, img_map, on='word', how='left')\n",
    "\n",
    "strain = pd.read_csv(\n",
    "    strain_file, sep='\\t', header=None, names=['word', 'ort', 'pho', 'wf']\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'frequency', 'pho_consistency', 'imageability']\n",
    ")\n",
    "\n",
    "df_strain = pd.merge(strain, strain_key)\n",
    "df_strain = pd.merge(df_strain, img_map, on='word', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "df_grain = pd.merge(grain, grain_key)\n",
    "\n",
    "df_grain['img'] = 0\n",
    "df_grain['wf'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban = pd.read_csv('../common/input/taraban.csv')\n",
    "taraban = pd.merge(taraban, img_map, on='word', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glushko = pd.read_csv('../common/input/glushko_nonword.csv')\n",
    "glushko.columns = ['id', 'cond', 'word', 'pho', 'ort']\n",
    "\n",
    "glushko['img'] = 0\n",
    "glushko['wf'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def trim_ort(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = trim_ort(df_train)\n",
    "df_strain = trim_ort(df_strain)\n",
    "df_grain = trim_ort(df_grain)\n",
    "df_taraban = trim_ort(taraban)\n",
    "df_glushko = trim_ort(glushko)\n",
    "\n",
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train = df_train.fillna(mean_img)\n",
    "\n",
    "# Fill missing value to condition mean img rating\n",
    "mean_strain_hi_img = df_strain.img.loc[df_strain.imageability == \"HI\", ].mean()\n",
    "mean_strain_lo_img = df_strain.img.loc[df_strain.imageability == \"LI\", ].mean()\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"HI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"HI\",\n",
    "                                     \"img\"].fillna(mean_strain_hi_img)\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"LI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"LI\",\n",
    "                                     \"img\"].fillna(mean_strain_lo_img)\n",
    "\n",
    "# Since taraban do not maniputate img, just replace by training set mean\n",
    "df_taraban = df_taraban.fillna(mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('../common/input/df_train.csv')\n",
    "df_strain.to_csv('../common/input/df_strain.csv')\n",
    "df_grain.to_csv('../common/input/df_grain.csv')\n",
    "df_taraban.to_csv('../common/input/df_taraban.scv')\n",
    "df_glushko.to_csv('../common/input/df_glushko.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print('In slot ', slotId, '\\t')\n",
    "            print('token count:', t.word_counts)\n",
    "            print('word count:', t.document_count)\n",
    "            print('dictionary:', t.word_index)\n",
    "            print('token appear in how many words:', t.word_docs)\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "\n",
    "    return binData\n",
    "\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort, df_taraban.ort, df_glushko.ort],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=False)\n",
    "splitId_strain = len(df_train)\n",
    "splitId_grain = splitId_strain + len(df_strain)\n",
    "splitId_taraban = splitId_grain + len(df_grain)\n",
    "splitId_glushko = splitId_taraban + len(df_taraban)\n",
    "\n",
    "x_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "x_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "x_grain = np.array(all_ort_bin[splitId_grain:splitId_taraban])\n",
    "x_taraban = np.array(all_ort_bin[splitId_taraban:splitId_glushko])\n",
    "x_glushko = np.array(all_ort_bin[splitId_glushko::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/x_train.npz', data=x_train)\n",
    "np.savez_compressed('../common/input/x_strain.npz', data=x_strain)\n",
    "np.savez_compressed('../common/input/x_grain.npz', data=x_grain)\n",
    "np.savez_compressed('../common/input/x_taraban.npz', data=x_taraban)\n",
    "np.savez_compressed('../common/input/x_glushko.npz', data=x_glushko)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_strain shape:', x_strain.shape)\n",
    "print('x_grain shape:', x_grain.shape)\n",
    "print('x_taraban shape:', x_taraban.shape)\n",
    "print('x_glushko shape:', x_glushko.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([len(p_col), binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        outSeries = slotSeries.map(p_key)\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = outSeries.to_list()\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "y_train = pho2bin_v2(train.pho, phon_key)\n",
    "y_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "y_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "y_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "y_taraban = pho2bin_v2(taraban.pho, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/y_train.npz', data=y_train)\n",
    "np.savez_compressed('../common/input/y_strain.npz', data=y_strain)\n",
    "np.savez_compressed('../common/input/y_large_grain.npz', data=y_large_grain)\n",
    "np.savez_compressed('../common/input/y_small_grain.npz', data=y_small_grain)\n",
    "np.savez_compressed('../common/input/y_taraban.npz', data=y_taraban)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_strain shape:', y_strain.shape)\n",
    "print('y_large_grain shape:', y_large_grain.shape)\n",
    "print('y_small_grain shape:', y_small_grain.shape)\n",
    "print('y_taraban shape:', y_taraban.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, pickle\n",
    "\n",
    "# Glushko pho dictionary\n",
    "gdict = {\n",
    "    x: ast.literal_eval(df_glushko.loc[i, 'pho'])\n",
    "    for i, x in enumerate(df_glushko.word)\n",
    "}\n",
    "\n",
    "p_key = phon_key\n",
    "\n",
    "# Glushko one-hot encoded output dictionary\n",
    "d = {}\n",
    "for k, v in gdict.items():\n",
    "    ys = []\n",
    "    for pho in v:\n",
    "        y = []\n",
    "        for char in pho:\n",
    "            y += p_key[char]\n",
    "        ys.append(y)\n",
    "    d[k] = ys\n",
    "    \n",
    "with open('../common/input/y_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('../common/input/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "from data_wrangling import wf_manager\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "sortwf = wf_manager(plot_f['wf'])\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(sortwf.wf, sortwf.samp_log(), label='Log')\n",
    "line2, = ax.plot(sortwf.wf, sortwf.samp_hs04(), label='HS04')\n",
    "line3, = ax.plot(sortwf.wf, sortwf.samp_jay(), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.xlim((0, 10000))\n",
    "plt.ylim((0, .0006))\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f = df_train.sort_values('wf')\n",
    "sortwf = wf_manager(plot_f['wf'])\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(sortwf.wf, sortwf.samp_log(), label='Log')\n",
    "line2, = ax.plot(sortwf.wf, sortwf.samp_hs04(), label='HS04')\n",
    "line3, = ax.plot(sortwf.wf, sortwf.samp_jay(), label='JAY')\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
