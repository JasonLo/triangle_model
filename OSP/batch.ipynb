{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook uses Papermill to batch run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import papermill as pm\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = \"O2P_main\"\n",
    "\n",
    "# Create batch cfgs\n",
    "batch_cfgs = []\n",
    "i = 0\n",
    "\n",
    "for noise in [0., 1., 2., 4., 8.]:\n",
    "    for h in [25, 50, 100, 250]:\n",
    "        for c_unit in [25, 50]:\n",
    "            if c_unit == 25:\n",
    "                for c_stage in [0, 195, 300]:\n",
    "                    # Clean up unit = 25\n",
    "                    i += 1\n",
    "                    code_name = batch_name + \"_model_{:04d}\".format(i)\n",
    "                    batch_cfg = dict(\n",
    "                        sn=i,\n",
    "                        in_notebook=\"basicOSP_master.ipynb\",\n",
    "                        code_name=code_name,\n",
    "                        model_folder=\"models/\" + code_name + \"/\",\n",
    "                        out_notebook=\"models/\" + code_name + \"/output.ipynb\",\n",
    "                        params=dict(\n",
    "                            code_name=code_name,\n",
    "                            sample_name='hs04',\n",
    "                            sample_rng_seed=329,\n",
    "                            tf_rng_seed=123,\n",
    "                            use_semantic=False,\n",
    "                            sem_param_gf=0.,\n",
    "                            sem_param_gi=0.,\n",
    "                            sem_param_kf=0.,\n",
    "                            sem_param_ki=0.,\n",
    "                            sem_param_hf=0.,\n",
    "                            sem_param_hi=0.,\n",
    "                            o_input_dim=119,\n",
    "                            hidden_units=h,\n",
    "                            pho_units=250,\n",
    "                            cleanup_units=c_unit,\n",
    "                            rnn_activation='sigmoid',\n",
    "                            regularizer_const=5e-6,\n",
    "                            embed_attractor_cfg=\n",
    "                            'models/Attractor_{}/model_config.json'.\n",
    "                            format(c_unit),\n",
    "                            embed_attractor_h5='ep{0:04d}.h5'.format(c_stage),\n",
    "                            p_noise=noise,  # i.e. w_pp, w_pc, and w_cp noise\n",
    "                            tau=0.2,\n",
    "                            max_unit_time=4.,\n",
    "                            n_mil_sample=1.,\n",
    "                            batch_size=128,\n",
    "                            learning_rate=0.005,\n",
    "                            save_freq=5,\n",
    "                            bq_dataset=batch_name\n",
    "                        )\n",
    "                    )\n",
    "                    batch_cfgs.append(batch_cfg)\n",
    "\n",
    "            if c_unit == 50:\n",
    "                for c_stage in [0, 70, 125]:\n",
    "                    i += 1\n",
    "                    code_name = batch_name + \"_model_{:04d}\".format(i)\n",
    "                    batch_cfg = dict(\n",
    "                        sn=i,\n",
    "                        in_notebook=\"basicOSP_master.ipynb\",\n",
    "                        code_name=code_name,\n",
    "                        model_folder=\"models/\" + code_name + \"/\",\n",
    "                        out_notebook=\"models/\" + code_name + \"/output.ipynb\",\n",
    "                        params=dict(\n",
    "                            code_name=code_name,\n",
    "                            sample_name='hs04',\n",
    "                            sample_rng_seed=329,\n",
    "                            tf_rng_seed=123,\n",
    "                            use_semantic=False,\n",
    "                            sem_param_gf=0.,\n",
    "                            sem_param_gi=0.,\n",
    "                            sem_param_kf=0.,\n",
    "                            sem_param_ki=0.,\n",
    "                            sem_param_hf=0.,\n",
    "                            sem_param_hi=0.,\n",
    "                            o_input_dim=119,\n",
    "                            hidden_units=h,\n",
    "                            pho_units=250,\n",
    "                            cleanup_units=c_unit,\n",
    "                            rnn_activation='sigmoid',\n",
    "                            regularizer_const=5e-6,\n",
    "                            embed_attractor_cfg=\n",
    "                            'models/Attractor_{}/model_config.json'.\n",
    "                            format(c_unit),\n",
    "                            embed_attractor_h5='ep{0:04d}.h5'.format(c_stage),\n",
    "                            p_noise=noise,  # i.e. w_pp, w_pc, and w_cp noise\n",
    "                            tau=0.2,\n",
    "                            max_unit_time=4.,\n",
    "                            n_mil_sample=1.,\n",
    "                            batch_size=128,\n",
    "                            learning_rate=0.005,\n",
    "                            save_freq=5,\n",
    "                            bq_dataset=batch_name\n",
    "                        )\n",
    "                    )\n",
    "                    batch_cfgs.append(batch_cfg)\n",
    "\n",
    "\n",
    "# Run\n",
    "def run_batch(cfg):\n",
    "    try:\n",
    "        print(\"Running model {}\".format(cfg['sn']))\n",
    "\n",
    "        if not os.path.exists(cfg['model_folder']):\n",
    "            os.mkdir(cfg['model_folder'])\n",
    "\n",
    "        pm.execute_notebook(\n",
    "            cfg['in_notebook'],\n",
    "            cfg['out_notebook'],\n",
    "            parameters=cfg['params'],\n",
    "        )\n",
    "\n",
    "    except:\n",
    "        print(\"Error occur in {}\".format(cfg['code_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.72s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.22s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.13s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.53s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.58s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.09s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.15s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.78s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.33s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:18, 18.38s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.45s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:20, 20.56s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.53s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.13s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.29s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.85s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.40s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.96s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.24s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.56s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.54s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:14, 14.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.79s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.08s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.06s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.78s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.77s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.50s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.40s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.62s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:19, 19.76s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.97s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.60s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.77s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.02s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.41s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.88s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:06,  6.79s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.16s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.15s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.74s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:01,  1.99s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.54s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.77s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.36s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.23s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.05s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.06s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.40s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.02s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.73s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.79s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.31s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.01s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.44s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.45s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.93s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.38s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:17, 17.13s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.92s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.49s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.54s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.83s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.81s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:15, 15.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.56s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.62s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.43s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.37s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.87s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.03s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.59s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.04s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.25s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.53s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.05s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.23s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  9.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.25s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.20s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.50s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.57s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:05,  5.58s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.28s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.45s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.67s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.93s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.46s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.23s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.33s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.41s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.02s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.43s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.79s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.50s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.68s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.57s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.33s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.58s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.95s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.41s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.44s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.10s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.05s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.52s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.95s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.68s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.38s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.38s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.70s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.34s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.65s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.92s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.72s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.34s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.03s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.23s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.28s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.28s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.13s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:16, 16.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:05,  5.88s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.27s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:14, 14.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.89s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:06,  6.65s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:16, 16.27s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.63s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.61s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.87s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.50s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.45s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.13s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [01:17, 77.16s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:40, 40.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.26s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.50s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.56s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.09s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.00s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.23s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.69s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.06s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:05,  5.71s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.18s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.84s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.29s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.76s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.58s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.59s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.75s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:19, 19.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.09s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.07s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.49s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.53s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.27s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.34s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.42s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.06s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:14, 14.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.38s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.53s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.66s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.85s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.89s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.41s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.42s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:19, 19.21s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [06:33, 393.30s/it]A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:17, 17.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.54s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.94s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:14, 14.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.48s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.74s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.40s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:06,  6.29s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.47s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.67s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.56s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.83s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.41s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.70s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.38s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.98s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.20s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.90s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.92s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.29s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.81s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.23s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.02s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.78s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:17, 17.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.30s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.01s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.83s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.15s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.30s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.49s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.15s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.36s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.99s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.47s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.88s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.14s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:14, 14.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.86s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.76s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.38s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.85s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.25s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.56s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:15, 15.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.18s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.77s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.16s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.60s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.18s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:05,  5.03s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.30s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.18s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.49s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.50s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.67s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.41s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.50s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:04,  4.54s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:10, 10.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:11, 11.95s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.80s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.35s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.75s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.45s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.28s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:08,  8.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.08s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.29s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.48s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.13s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.42s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.06s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:18, 18.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.78s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.17s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:09,  9.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:12, 12.93s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:03,  3.30s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:13, 13.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:06,  6.21s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.94s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:07,  7.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:17, 17.26s/it]\u001b[A\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n",
      "Writing data to Bigquery\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1it [00:02,  2.16s/it][A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:18, 18.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "1it [00:01,  1.17s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Run in parallel pool\n",
    "# with Pool(4) as pool:\n",
    "#     pool.map(run_batch, batch_cfgs)\n",
    "\n",
    "# Push results to BQ\n",
    "from meta import model_cfg, connect_gbq\n",
    "import pandas as pd\n",
    "\n",
    "# Make connection to bq\n",
    "bq = connect_gbq()\n",
    "\n",
    "for sn in range(1, len(batch_cfgs) + 1):\n",
    "\n",
    "    model_folder = 'models/{0:s}_model_{1:04d}'.format(batch_name, sn)\n",
    "    \n",
    "    # Load model config\n",
    "    cfg = model_cfg(None)\n",
    "    cfg.load_cfg_json(model_folder + '/model_config.json')\n",
    "    cfg.bq_dataset = batch_name\n",
    "    \n",
    "    # Load Strain and Grain\n",
    "    strain_i_hist = pd.read_csv(model_folder + '/result_strain_item.csv')\n",
    "    grain_i_hist = pd.read_csv(model_folder + '/result_grain_item.csv')\n",
    "    bq.push_all(cfg, strain_i_hist, grain_i_hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown compute engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo poweroff  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from meta import read_bq_cfg\n",
    "from evaluate import vis\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# batch_name = 'O2P_weightdecay'\n",
    "save_fig = None\n",
    "cfgs = read_bq_cfg(batch_name)\n",
    "\n",
    "# Read cfg files from BQ\n",
    "print('===== Batch level hyperparams (columns that have >1 unique value) =====')\n",
    "for i, x in enumerate(cfgs.columns):\n",
    "    if not x == 'code_name':\n",
    "        if not x == 'uuid':\n",
    "            if len(cfgs[x].unique()) > 1:\n",
    "                print(\n",
    "                    'Column <{}> has these unique values: {}'.format(\n",
    "                        x, cfgs[x].unique()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "# Parse each run by batch_eval, which aggregate item level data to condition level and merge Grain and Strain into one single file (Using local files instead of BQ, may use BQ for way way more data... >5Gbs I guess)\n",
    "\n",
    "models_path = [\n",
    "    'models/' + batch_name + '_model_{0:04d}'.format(i)\n",
    "    for i in range(1,\n",
    "                   len(cfgs) + 1)\n",
    "]\n",
    "batch_acc = pd.DataFrame()\n",
    "\n",
    "for i, name in enumerate(models_path):\n",
    "    this_eval = vis(\n",
    "        name, 'result_strain_item.csv', 'result_grain_item.csv'\n",
    "    )  # Eval lesion and grain\n",
    "    this_eval.parse_cond_df()\n",
    "    batch_acc = pd.concat([batch_acc, this_eval.cdf], ignore_index=True)\n",
    "\n",
    "df = pd.merge(batch_acc, cfgs, 'left', 'code_name')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_attractor_acc(cleanup_units, embed_attractor_h5):\n",
    "    if cleanup_units == 25:\n",
    "        if embed_attractor_h5 == 'ep0000.h5':\n",
    "            acc = 0.\n",
    "        if embed_attractor_h5 == 'ep0195.h5':\n",
    "            acc = 0.6\n",
    "        if embed_attractor_h5 == 'ep0300.h5':\n",
    "            acc = 0.9\n",
    "\n",
    "    if cleanup_units == 50:\n",
    "        if embed_attractor_h5 == 'ep0000.h5':\n",
    "            acc = 0.\n",
    "        if embed_attractor_h5 == 'ep0070.h5':\n",
    "            acc = 0.6\n",
    "        if embed_attractor_h5 == 'ep0125.h5':\n",
    "            acc = 0.9\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_acc = []\n",
    "\n",
    "for i in df.index:\n",
    "    tmp_acc.append(\n",
    "        cal_attractor_acc(df.cleanup_units[i], df.embed_attractor_h5[i])\n",
    "    )\n",
    "\n",
    "df['attactor_acc'] = tmp_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"default\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Selectors for interactions\n",
    "sel_run = alt.selection(type=\"multi\", on=\"click\", fields=[\"code_name\"])\n",
    "sel_cond = alt.selection(\n",
    "    type=\"multi\", on=\"click\", fields=[\"cond\"], bind=\"legend\"\n",
    ")\n",
    "\n",
    "# Heatmap for final epoch & timestep (Overview)\n",
    "plot_timestep = df.timestep.max()\n",
    "# plot_timestep = 19\n",
    "\n",
    "# Plot strain\n",
    "\n",
    "df_epo_time = df[(df.epoch == df.epoch.max()) & (df.timestep == plot_timestep) &\n",
    "                 (df.exp == 'strain')]\n",
    "\n",
    "overview = (\n",
    "    alt.Chart(df_epo_time).mark_rect().encode(\n",
    "        x=\"hidden_units:O\",\n",
    "        y=\"w_pp_noise:O\",\n",
    "        column='attactor_acc',\n",
    "        row='cleanup_units',\n",
    "        color=alt.Color(\"acc\", scale=alt.Scale(scheme=\"redyellowgreen\")),\n",
    "        opacity=alt.condition(sel_run, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_run)\n",
    ")\n",
    "\n",
    "# Accuracy over epoch at last time step for selected model\n",
    "last_time_point = df[df.timestep == df.timestep.max()]\n",
    "\n",
    "acc_plot = (\n",
    "    alt.Chart(last_time_point).mark_line().encode(\n",
    "        y=alt.Y(\"acc:Q\", scale=alt.Scale(domain=(0, 1))),\n",
    "        x=\"epoch\",\n",
    "        color=\"cond\",\n",
    "        opacity=alt.condition(sel_cond, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_cond).transform_filter(sel_run).properties(\n",
    "        title=\"Full model at final time step\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plot = overview | acc_plot\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.save('batch_eval/O2P_l2reg.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta import model_cfg\n",
    "from evaluate import vis\n",
    "\n",
    "code_name = '{0:}_model_{1:04d}'.format(batch_name, 47)\n",
    "\n",
    "# Load cfg from json\n",
    "cfg = model_cfg(None)\n",
    "cfg.load_cfg_json('models/' + code_name + '/model_config.json')\n",
    "\n",
    "vis = vis(\n",
    "    cfg.path_model_folder, 'result_strain_item.csv', 'result_grain_item.csv'\n",
    ")\n",
    "\n",
    "vis.parse_cond_df()\n",
    "\n",
    "full = vis.plot_dev_interactive('acc').properties(title='Full input')\n",
    "\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible development plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_dev('acc', exp=None, condition='cond', timestep=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flexible time plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_time('acc', exp='strain', condition='cond', epoch=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive for other batches (for record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## semantic_lr_batch\n",
    "- Purpose: Examine learning rate effect interaction with g and k\n",
    "- Hyperparams: lr, g, k\n",
    "- Modeling branch: OSP (Feedforward)\n",
    "- Total models: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from meta import read_bq_cfg\n",
    "from evaluate import vis\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# batch_name = 'over_ride_batch_name_here'\n",
    "save_fig = None\n",
    "cfgs = read_bq_cfg(batch_name)\n",
    "\n",
    "# Read cfg files from BQ\n",
    "print('===== Batch level hyperparams (columns that have >1 unique value) =====')\n",
    "for i, x in enumerate(cfgs.columns):\n",
    "    if not x == 'code_name':\n",
    "        if not x == 'uuid':\n",
    "            if len(cfgs[x].unique()) > 1:\n",
    "                print(\n",
    "                    'Column <{}> has these unique values: {}'.format(\n",
    "                        x, cfgs[x].unique()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "# Parse each run by batch_eval, which aggregate item level data to condition level and merge Grain and Strain into one single file (Using local files instead of BQ, may use BQ for way way more data... >5Gbs I guess)\n",
    "\n",
    "models_path = ['models/' + batch_name + '_model_{0:04d}'.format(i) for i in range(1, 13)]\n",
    "batch_acc = pd.DataFrame()\n",
    "\n",
    "for i, name in enumerate(models_path):\n",
    "    # for i, name in enumerate(os.listdir(\"models/\")):\n",
    "    this_eval = vis(name, 'result_strain_ns_item.csv', 'result_grain_item.csv') # Eval lesion and grain\n",
    "    this_eval.parse_cond_df()\n",
    "    batch_acc = pd.concat(\n",
    "        [batch_acc, this_eval.cdf], ignore_index=True\n",
    "    )\n",
    "\n",
    "df = pd.merge(batch_acc, cfgs, 'left', 'code_name')\n",
    "print('Done')\n",
    "\n",
    "# Plotting results\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"default\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Selectors for interactions\n",
    "sel_run = alt.selection(type=\"multi\", on=\"click\", fields=[\"code_name\"])\n",
    "sel_cond = alt.selection(\n",
    "    type=\"multi\", on=\"click\", fields=[\"cond\"], bind=\"legend\"\n",
    ")\n",
    "\n",
    "# Heatmap for final epoch & timestep (Overview)\n",
    "final_epo_time = df[(df.epoch == df.epoch.max()) &\n",
    "                    (df.timestep == df.timestep.max())]\n",
    "\n",
    "overview = (\n",
    "    alt.Chart(final_epo_time).mark_rect().encode(\n",
    "        y=\"sem_param_kf:O\",\n",
    "        x=\"sem_param_gf:O\",\n",
    "        column=\"learning_rate:Q\",\n",
    "        row = 'exp',\n",
    "        color=alt.Color(\"acc\", scale=alt.Scale(scheme=\"redyellowgreen\")),\n",
    "        opacity=alt.condition(sel_run, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_run)\n",
    ")\n",
    "\n",
    "# Accuracy over epoch at last time step for selected model\n",
    "last_time_point = df[df.timestep == df.timestep.max()]\n",
    "\n",
    "acc_plot = (\n",
    "    alt.Chart(last_time_point).mark_line().encode(\n",
    "        y=\"acc:Q\",\n",
    "        x=\"epoch\",\n",
    "        color=\"cond\",\n",
    "        opacity=alt.condition(sel_cond, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_cond).transform_filter(sel_run).properties(\n",
    "        title=\"At final time step\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plot = overview | acc_plot\n",
    "plot.save('batch_eval/batch_lr_g_k_12.html')\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 3\n",
    "- Purpose: Extra plotting for Pretrain attractor\n",
    "- Hyperparams: hidden, pretrain or not\n",
    "- Modeling branch: OSP\n",
    "- Total models: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "from meta import batch_eval, read_bq_cfg\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# batch_name = 'over_ride_batch_name_here'\n",
    "save_fig = None\n",
    "cfgs = read_bq_cfg(batch_name)\n",
    "\n",
    "# Read cfg files from BQ\n",
    "print('===== Batch level hyperparams (columns that have >1 unique value) =====')\n",
    "for i, x in enumerate(cfgs.columns):\n",
    "    if not x == 'code_name':\n",
    "        if not x == 'uuid':\n",
    "            if len(cfgs[x].unique()) > 1:\n",
    "                print(\n",
    "                    'Column <{}> has these unique values: {}'.format(\n",
    "                        x, cfgs[x].unique()\n",
    "                    )\n",
    "                )\n",
    "\n",
    "# Parse each run by batch_eval, which aggregate item level data to condition level and merge Grain and Strain into one single file (Using local files instead of BQ, may use BQ for way way more data... >5Gbs I guess)\n",
    "\n",
    "models_path = [batch_name + '_model_{0:04d}'.format(i) for i in range(1, 9)]\n",
    "batch_acc = pd.DataFrame()\n",
    "\n",
    "for i, name in enumerate(models_path):\n",
    "    # for i, name in enumerate(os.listdir(\"models/\")):\n",
    "    if name.startswith(batch_name):\n",
    "        this_eval = batch_eval(name)\n",
    "        batch_acc = pd.concat(\n",
    "            [batch_acc, this_eval.export_result()], ignore_index=True\n",
    "        )\n",
    "\n",
    "df = pd.merge(batch_acc, cfgs, 'left', 'code_name')\n",
    "print('Done')\n",
    "\n",
    "# Plotting results\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"default\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Selectors for interactions\n",
    "sel_run = alt.selection(type=\"multi\", on=\"click\", fields=[\"code_name\"])\n",
    "sel_cond = alt.selection(\n",
    "    type=\"multi\", on=\"click\", fields=[\"cond\"], bind=\"legend\"\n",
    ")\n",
    "\n",
    "# Heatmap for final epoch & timestep (Overview)\n",
    "final_epo_time = df[(df.epoch == df.epoch.max()) &\n",
    "                    (df.timestep == df.timestep.max())]\n",
    "\n",
    "overview = (\n",
    "    alt.Chart(final_epo_time).mark_rect().encode(\n",
    "        y=\"cond:N\",\n",
    "        x=\"regularizer_const:O\",\n",
    "        color=alt.Color(\"acc\", scale=alt.Scale(scheme=\"redyellowgreen\")),\n",
    "        opacity=alt.condition(sel_run, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_run)\n",
    ")\n",
    "\n",
    "# Accuracy over epoch at last time step for selected model\n",
    "last_time_point = df[df.timestep == df.timestep.max()]\n",
    "\n",
    "acc_plot = (\n",
    "    alt.Chart(last_time_point).mark_line().encode(\n",
    "        y=\"acc:Q\",\n",
    "        x=\"epoch\",\n",
    "        color=\"cond\",\n",
    "        opacity=alt.condition(sel_cond, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_cond).transform_filter(sel_run).properties(\n",
    "        title=\"At final time step\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plot = overview | acc_plot\n",
    "if save_fig is not None:\n",
    "    plot.save(save_fig)\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2\n",
    "- Purpose: Replicate OP models result\n",
    "- Hyperparams: p-noise, hidden, and clean up\n",
    "- Modeling branch: OSP\n",
    "- Total models: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"default\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "sel_run = alt.selection(type=\"multi\", on=\"click\", fields=[\"code_name\"])\n",
    "sel_cond = alt.selection(\n",
    "    type=\"multi\", on=\"click\", fields=[\"cond\"], bind=\"legend\"\n",
    ")\n",
    "\n",
    "final_epo_time = df[(df.epoch == df.epoch.max()) &\n",
    "                    (df.timestep == df.timestep.max())]\n",
    "\n",
    "overview = (\n",
    "    alt.Chart(final_epo_time).mark_rect().encode(\n",
    "        y=\"hidden_units:O\",\n",
    "        x=\"cleanup_units:O\",\n",
    "        column=\"w_pp_noise\",\n",
    "        row=\"exp\",\n",
    "        color=alt.Color(\"acc\", scale=alt.Scale(scheme=\"redyellowgreen\")),\n",
    "        opacity=alt.condition(sel_run, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_run)\n",
    ")\n",
    "\n",
    "last_time_point = df[df.timestep == df.timestep.max()]\n",
    "\n",
    "acc_plot = (\n",
    "    alt.Chart(last_time_point).mark_line().encode(\n",
    "        y=\"acc:Q\",\n",
    "        x=\"epoch\",\n",
    "        color=\"cond\",\n",
    "        opacity=alt.condition(sel_cond, alt.value(1), alt.value(0.1)),\n",
    "        tooltip=[\"code_name\", \"acc\"],\n",
    "    ).add_selection(sel_cond).transform_filter(sel_run).properties(\n",
    "        title=\"At final time step\"\n",
    "    )\n",
    ")\n",
    "\n",
    "plot = overview | acc_plot\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 1\n",
    "- Purpose: Initial testing\n",
    "- Hyperparams: p-noise, hidden, and clean up\n",
    "- Modeling branch: OP\n",
    "- Total models: 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a eval script snapshot for batch1 eval\n",
    "# Put in OSP folder to run\n",
    "# Use meta eval block for development\n",
    "\n",
    "batch_name = \"batch_v1\"\n",
    "\n",
    "from meta import read_bq_cfg\n",
    "cfgs = read_bq_cfg('batch_test')\n",
    "\n",
    "# Read cfg files from BQ\n",
    "print('===== Check columns that has multiple unique values =====')\n",
    "for i, x in enumerate(cfgs.columns):\n",
    "    if not x == 'code_name':\n",
    "        if not x == 'uuid':\n",
    "            if len(cfgs[x].unique()) > 1:\n",
    "                print('Column: {} has these unique values: {}'.format(\n",
    "                    x, cfgs[x].unique()))\n",
    "\n",
    "import os, json\n",
    "import pandas as pd\n",
    "from meta import batch_eval\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Read all local results\n",
    "batch_acc = pd.DataFrame()\n",
    "\n",
    "from meta import model_cfg\n",
    "\n",
    "class batch_eval():\n",
    "    def __init__(self, model_folder):\n",
    "        from evaluate import training_history, strain_eval, grain_eval, plot_development\n",
    "        from data_wrangling import my_data\n",
    "        import altair as plt\n",
    "\n",
    "        self.model_folder = 'models/' + model_folder\n",
    "        self.load_config()\n",
    "        self.read_eval_from_file()\n",
    "        self.max_epoch = self.strain_i_hist['epoch'].max()\n",
    "\n",
    "    def load_config(self):\n",
    "        self.cfg = model_cfg(None)\n",
    "        self.cfg.load_cfg_json(self.model_folder + '/model_config.json')\n",
    "\n",
    "    def training_hist(self):\n",
    "        self.t_hist = training_history(self.cfg.path_history_pickle)\n",
    "        return self.t_hist.plot_all()\n",
    "\n",
    "    def read_eval_from_file(self):\n",
    "        self.strain_i_hist = pd.read_csv(self.model_folder +\n",
    "                                         '/result_strain_item.csv')\n",
    "        self.grain_i_hist = pd.read_csv(self.model_folder +\n",
    "                                        '/result_grain_item.csv')\n",
    "\n",
    "    def parse_strain_cond_df(self, cond):\n",
    "        self.strain_i_hist['code_name'] = self.strain_i_hist.model_version\n",
    "        self.scdf = self.strain_i_hist[[\n",
    "            'code_name', 'epoch', 'sample_mil', 'timestep', 'unit_time', cond,\n",
    "            'input_s', 'acc', 'sse'\n",
    "        ]]\n",
    "        self.scdf = self.scdf.groupby(['code_name', 'epoch', 'timestep', cond],\n",
    "                                      as_index=False).mean()\n",
    "        self.scdf['cond'] = self.scdf[cond]\n",
    "        self.scdf['exp'] = 'strain'\n",
    "\n",
    "    def parse_grain_cond_df(self, cond):\n",
    "        self.grain_i_hist['code_name'] = self.grain_i_hist.model_version\n",
    "        self.gcdf = self.grain_i_hist[[\n",
    "            'code_name', 'epoch', 'sample_mil', 'timestep', 'unit_time', cond,\n",
    "            'input_s', 'acc_acceptable', 'sse_acceptable'\n",
    "        ]]\n",
    "        self.gcdf = self.gcdf.rename(columns={\n",
    "            'acc_acceptable': 'acc',\n",
    "            'sse_acceptable': 'sse'\n",
    "        })\n",
    "        self.gcdf = self.gcdf.groupby(['code_name', 'epoch', 'timestep', cond],\n",
    "                                      as_index=False).mean()\n",
    "        self.gcdf['cond'] = self.gcdf[cond]\n",
    "        self.gcdf['exp'] = 'grain'\n",
    "\n",
    "    def parse_cond_df(self,\n",
    "                      cond_strain='condition_pf',\n",
    "                      cond_grain='condition'):\n",
    "        self.parse_strain_cond_df(cond_strain)\n",
    "        self.parse_grain_cond_df(cond_grain)\n",
    "        self.cdf = pd.concat([self.scdf, self.gcdf], sort=False)\n",
    "\n",
    "    def plot_dev(self, y, time_step=None):\n",
    "\n",
    "        if time_step == None: time_step = self.cfg.n_timesteps - 1\n",
    "\n",
    "        title = '{} at timestep {} / unit time {}'.format(\n",
    "            y, plot_time_step + 1, self.cfg.unit_time)\n",
    "        sel = alt.selection(type='single',\n",
    "                            on='click',\n",
    "                            fields=['cond'],\n",
    "                            empty='all')\n",
    "        plot = alt.Chart(\n",
    "            self.cdf[lambda x: x['timestep'] == time_step]).mark_line(\n",
    "                point=True).encode(\n",
    "                    y=y,\n",
    "                    x='epoch:Q',\n",
    "                    color='cond',\n",
    "                    opacity=alt.condition(sel, alt.value(1), alt.value(0)),\n",
    "                    tooltip=[\n",
    "                        'epoch', 'timestep', 'sample_mil', 'acc', 'sse'\n",
    "                    ]).add_selection(sel).interactive().properties(title=title)\n",
    "\n",
    "        return plot\n",
    "\n",
    "    def plot_time(self, y, epoch=None):\n",
    "        if epoch == None: epoch = self.max_epoch\n",
    "        title = '{} at epoch {} '.format(y, epoch)\n",
    "        sel = alt.selection(type='single',\n",
    "                            on='click',\n",
    "                            fields=['cond'],\n",
    "                            empty='all')\n",
    "\n",
    "        plot = alt.Chart(self.cdf[lambda x: x['epoch'] == epoch]).mark_line(\n",
    "            point=True).encode(\n",
    "                y=y,\n",
    "                x='unit_time:Q',\n",
    "                color='cond',\n",
    "                opacity=alt.condition(sel, alt.value(1), alt.value(0)),\n",
    "                tooltip=[\n",
    "                    'epoch', 'timestep', 'sample_mil', 'acc', 'sse'\n",
    "                ]).add_selection(sel).interactive().properties(title=title)\n",
    "\n",
    "        return plot\n",
    "\n",
    "    def plots(self,\n",
    "              mode,\n",
    "              ys,\n",
    "              cond_strain='condition_pf',\n",
    "              cond_grain='condition'):\n",
    "        # Mode = dev(d) / time(t)\n",
    "        self.parse_cond_df(cond_strain, cond_grain)\n",
    "\n",
    "        plots = alt.hconcat()\n",
    "        for y in ys:\n",
    "            if mode == 'd':\n",
    "                plots |= self.plot_dev(y, cond_strain, cond_grain)\n",
    "            elif mode == 't':\n",
    "                plots |= self.plot_time(y, self.max_epoch, cond_strain,\n",
    "                                        cond_grain)\n",
    "            else:\n",
    "                print('Use d for development plot, use t for time plot')\n",
    "\n",
    "        return plots\n",
    "\n",
    "    def export_result(self):\n",
    "        self.parse_cond_df()\n",
    "        return self.cdf.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Parse each run by batch_eval, which aggregate item level data to condition level and merge Grain and Strain into one single file\n",
    "\n",
    "for i, name in enumerate(\n",
    "    ['batch_v1_model_{0:04d}'.format(i) for i in range(1, 36 + 1)]):\n",
    "    # for i, name in enumerate(os.listdir(\"models/\")):\n",
    "    if name.startswith(batch_name):\n",
    "        this_eval = batch_eval(name)\n",
    "        batch_acc = pd.concat([batch_acc, this_eval.export_result()],\n",
    "                              ignore_index=True)\n",
    "\n",
    "df = pd.merge(batch_acc, cfgs, 'left', 'code_name')\n",
    "clear_output()\n",
    "print('Done')\n",
    "\n",
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.enable(\"default\")\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "sel_run = alt.selection(type=\"multi\", on=\"click\", fields=[\"code_name\"])\n",
    "sel_cond = alt.selection(type=\"multi\",\n",
    "                         on=\"click\",\n",
    "                         fields=[\"cond\"],\n",
    "                         bind=\"legend\")\n",
    "\n",
    "final_epo_time = df[(df.epoch == df.epoch.max())\n",
    "                    & (df.timestep == df.timestep.max())]\n",
    "\n",
    "overview = (alt.Chart(final_epo_time).mark_rect().encode(\n",
    "    y=\"hidden_units:O\",\n",
    "    x=\"cleanup_units:O\",\n",
    "    column=\"w_pp_noise\",\n",
    "    row=\"exp\",\n",
    "    color=alt.Color(\"acc\", scale=alt.Scale(scheme=\"redyellowgreen\")),\n",
    "    opacity=alt.condition(sel_run, alt.value(1), alt.value(0.1)),\n",
    "    tooltip=[\"code_name\", \"acc\"],\n",
    ").add_selection(sel_run))\n",
    "\n",
    "last_time_point = df[df.timestep == df.timestep.max()]\n",
    "\n",
    "acc_plot = (alt.Chart(last_time_point).mark_line().encode(\n",
    "    y=\"acc:Q\",\n",
    "    x=\"epoch\",\n",
    "    color=\"cond\",\n",
    "    opacity=alt.condition(sel_cond, alt.value(1), alt.value(0.1)),\n",
    "    tooltip=[\"code_name\", \"acc\"],\n",
    ").add_selection(sel_cond).transform_filter(sel_run).properties(\n",
    "    title=\"At final time step\"))\n",
    "\n",
    "plot = overview | acc_plot\n",
    "plot.save('batch_eval.html')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
