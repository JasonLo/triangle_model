{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move all the weight and biases from MikeNet to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import troubleshooting, meta, modeling\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "reload(troubleshooting)\n",
    "mn_weight = troubleshooting.MikeNetWeight(\"mikenet/Reading_Weight_v1\")  # They have v1-v10, looks similar to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapes of weights in MikeNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_weight.create_weights_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_name = \"surgery\"\n",
    "batch_name = None\n",
    "tf_root = \"/home/jupyter/triangle_model\"\n",
    "\n",
    "# Model configs\n",
    "ort_units = 364\n",
    "pho_units = 200\n",
    "sem_units = 2446\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 500\n",
    "hidden_ps_units = 300\n",
    "hidden_sp_units = 300\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "pho_noise_level = 0.\n",
    "sem_noise_level = 0.\n",
    "activation = \"sigmoid\"\n",
    "\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 13\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training configs\n",
    "learning_rate = 0.005\n",
    "zero_error_radius = 0.1\n",
    "save_freq = 20\n",
    "\n",
    "# Environment configs\n",
    "tasks = (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"ort_pho\", \"ort_sem\", \"triangle\")\n",
    "wf_compression = \"log\"\n",
    "wf_clip_low = 0\n",
    "wf_clip_high = 999_999_999\n",
    "oral_start_pct = 1.0\n",
    "oral_end_pct = 1.0\n",
    "\n",
    "oral_sample = 1_800_000\n",
    "# oral_tasks_ps = (0.4, 0.4, 0.1, 0.1, 0.)\n",
    "oral_tasks_ps = (0.4, 0.4, 0.05, 0.15, 0., 0., 0.)\n",
    "transition_sample = 800_000\n",
    "reading_sample = 15_000_000\n",
    "# reading_tasks_ps = (0.2, 0.2, 0.05, 0.05, 0.5)\n",
    "reading_tasks_ps = (0.2, 0.2, 0.05, 0.05, .1, .1, .3)\n",
    "\n",
    "batch_size = 100\n",
    "rng_seed = 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = meta.ModelConfig.from_global(globals_dict=globals())\n",
    "model = modeling.MyModel(cfg)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grafting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in model.weights:\n",
    "    try:\n",
    "        name = weight.name[:-2]\n",
    "        weight.assign(mn_weight.weights_tf[name])\n",
    "        print(f\"Grafted mikenet weight {name} to tf.weights\")\n",
    "\n",
    "        # Post-load weight sanity check\n",
    "        tf.debugging.assert_equal(mn_weight.weights_tf[name], weight)\n",
    "\n",
    "    except KeyError:\n",
    "        print(f\"Missing weight {name} in mikenet\")\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse MN pattern into TF format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_wrangling\n",
    "x = data_wrangling.load_testset('train_r100')\n",
    "r100_items = x['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern(lines, line_start, line_end):\n",
    "    pattern = []\n",
    "    for i in range(line_start, line_end):\n",
    "        line = lines[i].strip().split(' ')\n",
    "        [pattern.append(unit) for unit in line]\n",
    "    return pattern\n",
    "\n",
    "\n",
    "def parse_pattern(file: str=None) -> dict:\n",
    "    items = list()\n",
    "    ort_pattern = {}\n",
    "    pho_pattern = {}\n",
    "    sparse_sem_pattern = {}\n",
    "\n",
    "    if file is None:\n",
    "        file = \"mikenet/englishdict_randcon.pat.txt\"\n",
    "\n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"TAG Word: \"):\n",
    "                content = line.split(\" \")\n",
    "                word = content[2]\n",
    "                ort = content[6] # human readable ort\n",
    "                pho = content[4] # human readable pho\n",
    "                print(f\"word:{word}, ort:{ort}, pho:{pho}\")\n",
    "\n",
    "                # Record item\n",
    "                items.append(word)\n",
    "\n",
    "            if line.startswith(\"CLAMP Ortho\"):\n",
    "                # Take the next 14 lines value into ort representation dictionary\n",
    "                ort_pattern[word] = get_pattern(lines, i+1, i+15)\n",
    "\n",
    "            if line.startswith(\"TARGET Phono\"):\n",
    "                # Take the next 8 lines value into pho representation dictionary\n",
    "                pho_pattern[word] = get_pattern(lines, i+1, i+9)\n",
    "\n",
    "            if line.startswith(\"TARGET Semantics\"):\n",
    "                sparse_sem_pattern[word] = get_pattern(lines, i+1, i+1)\n",
    "\n",
    "    return items, ort_pattern, pho_pattern, sparse_sem_pattern\n",
    "\n",
    "def sparse_to_dense(representation, units=2446) -> np.array:\n",
    "    \"\"\"Convert sparse representation to dense representation\"\"\"\n",
    "    dense = np.zeros(units)\n",
    "    for unit in representation:\n",
    "        dense[int(unit)] = 1\n",
    "    return dense\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items, ort_pattern, pho_pattern, sparse_sem_pattern = parse_pattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack into my TF data format\n",
    "# Important: the order of the items must be the same as the order of the inputs\n",
    "# I did not account for multi meaning items\n",
    "\n",
    "mn_r100 = {}\n",
    "mn_r100['item'] = r100_items\n",
    "\n",
    "np_ort = np.zeros(shape=(100, ort_units))\n",
    "np_pho = np.zeros(shape=(100, pho_units))\n",
    "np_sem = np.zeros(shape=(100, sem_units))\n",
    "\n",
    "for idx, item in enumerate(r100_items):\n",
    "    np_ort[idx,:] = np.array(ort_pattern[item])\n",
    "    np_pho[idx,:] = np.array(pho_pattern[item])\n",
    "    np_sem[idx,:] = sparse_to_dense(sparse_sem_pattern[item], sem_units)\n",
    "\n",
    "mn_r100['ort'] = np_ort\n",
    "mn_r100['pho'] = np_pho\n",
    "mn_r100['sem'] = np_sem\n",
    "mn_r100['cond'] = None\n",
    "\n",
    "data_wrangling.save_testset(mn_r100, 'dataset/testsets/mn_r100.pkl.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_active_task('triangle')\n",
    "pred = model([mn_r100['ort']]* 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = troubleshooting.Diagnosis(code_name)\n",
    "d.model = model\n",
    "ts = TestSet_MN(cfg)\n",
    "ts.eval(testset_name='mn_r100', task='triangle')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = troubleshooting.Diagnosis(code_name)\n",
    "d.eval(testset_name, task='triangle', epoch=200)\n",
    "\n",
    "@interact(\n",
    "    sel_word=d.testset_package['item'], \n",
    "    layer=['pho', 'sem'], \n",
    "    task=['triangle', 'ort_pho', 'exp_osp', 'ort_sem', 'exp_ops'], \n",
    "    epoch=(10, d.cfg.total_number_of_epoch + 1, d.cfg.save_freq)\n",
    "    )\n",
    "def interactive_plot(sel_word, layer, task, epoch):\n",
    "    d = troubleshooting.Diagnosis(code_name)\n",
    "    d.eval(testset_name, task=task, epoch=epoch)\n",
    "    d.set_target_word(sel_word)\n",
    "    print(f\"Output phoneme over timeticks: {d.list_output_phoneme}\")\n",
    "    return d.plot_one_layer(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
