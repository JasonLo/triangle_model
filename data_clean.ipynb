{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "%reload_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Read training file\n",
    "train_file = \"issues/preprocessing/6ktraining_v2.dict\"\n",
    "\n",
    "strain_file = \"issues/preprocessing/strain.txt\"\n",
    "strain_key_file = \"issues/preprocessing/strain_key.txt\"\n",
    "\n",
    "grain_file = \"issues/preprocessing/grain_nws.dict\"\n",
    "grain_key_file = \"issues/preprocessing/grain_key.txt\"\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "\n",
    "# Zeno norm\n",
    "zeno = pd.read_csv(\"issues/preprocessing/EWFG.csv\", na_values=\"\", keep_default_na=False)\n",
    "zeno[\"gr14\"] = pd.to_numeric(zeno.f, errors=\"coerce\")  # Stage 14 is adult frequency\n",
    "[zeno.pop(v) for v in [\"sfi\", \"d\", \"u\", \"f\"]]\n",
    "clear_output()\n",
    "\n",
    "# Chang training set (from Chang 2019 github)\n",
    "y_wordnet = np.genfromtxt(\"issues/preprocessing/wordNet_6229.csv\", delimiter=\",\")\n",
    "\n",
    "wordnet_dict = pd.read_csv(\n",
    "    \"issues/preprocessing/6kdict\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "## Copy index for key\n",
    "wordnet_dict[\"wn_idx\"] = wordnet_dict.index\n",
    "\n",
    "## Drop wordnet duplicates (Do not drop)\n",
    "## HS04: There were 39 words in which a single spelling was associated with two or more meanings (mainly words such as SHEEP, FISH, or HIT, whose plural or past tense morphological inflection involves no change from the stem).\n",
    "# wordnet_dict.drop_duplicates(subset=['word'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Zeno and IMG into train\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "train = pd.merge(train, zeno, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Assume Zeno missing = 0\n",
    "for x in range(14):\n",
    "    variable_name = 'gr' + str(x+1)\n",
    "    train[variable_name] = train[variable_name].map(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "train = pd.merge(train, img_map, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "\n",
    "# Merge Chang\n",
    "wnid = wordnet_dict.loc[:,['word', 'wn_idx']]\n",
    "train = train.merge(wnid, how='inner', on='word')\n",
    "print(f'Words in training set: {len(train)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "strain = pd.read_csv(\n",
    "    strain_file, sep=\"\\t\", header=None, names=[\"word\", \"ort\", \"pho\", \"wf\"]\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=[\"word\", \"frequency\", \"pho_consistency\", \"imageability\"],\n",
    ")\n",
    "\n",
    "strain = pd.merge(strain, strain_key)\n",
    "strain = pd.merge(strain, img_map, on=\"word\", how=\"left\")\n",
    "strain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain.groupby(\"frequency\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "grain = pd.merge(grain, grain_key)\n",
    "\n",
    "grain['img'] = 0\n",
    "grain['wf'] = 0\n",
    "grain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban = pd.read_csv(\"issues/preprocessing/taraban.csv\")\n",
    "taraban.columns = [\"id\", \"cond\", \"word\", \"ort\", \"pho\", \"wf\"]\n",
    "taraban = pd.merge(taraban, img_map, on=\"word\", how=\"left\")\n",
    "taraban.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glushko = pd.read_csv(\"issues/preprocessing/glushko_nonword.csv\")\n",
    "glushko.columns = [\"id\", \"cond\", \"word\", \"pho\", \"ort\"]\n",
    "\n",
    "glushko[\"img\"] = 0\n",
    "glushko[\"wf\"] = 0\n",
    "glushko.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check raw data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all represtation follow 14 ort, 10 pho format\n",
    "assert all([len(x) == 14 for x in train.ort])\n",
    "assert all([len(x) == 14 for x in strain.ort])\n",
    "assert all([len(x) == 14 for x in grain.ort])\n",
    "assert all([len(x) == 14 for x in taraban.ort])\n",
    "assert all([len(x) == 14 for x in glushko.ort])\n",
    "\n",
    "assert all([len(x) == 10 for x in train.pho])\n",
    "assert all([len(x) == 10 for x in strain.pho])\n",
    "assert all([len(x) == 10 for x in grain.pho_small])\n",
    "assert all([len(x) == 10 for x in grain.pho_large])\n",
    "assert all([len(x) == 10 for x in taraban.pho])\n",
    "\n",
    "from ast import literal_eval\n",
    "for pho in glushko.pho:\n",
    "    ps = literal_eval(pho)\n",
    "    for p in ps:\n",
    "        assert len(p) == 10\n",
    "\n",
    "# Check all fufill trim_ort criteria\n",
    "locs = [0, 11, 12, 13]\n",
    "\n",
    "for l in locs:\n",
    "    assert all([x == '_' for x in train.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in strain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in grain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in taraban.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in glushko.ort.str.get(l)])\n",
    "\n",
    "# No missing data in critical variables\n",
    "assert sum(train.ort.isna()) == 0\n",
    "assert sum(train.pho.isna()) == 0\n",
    "assert sum(train.wf.isna()) == 0\n",
    "\n",
    "assert sum(strain.ort.isna()) == 0\n",
    "assert sum(strain.pho.isna()) == 0\n",
    "assert sum(strain.wf.isna()) == 0\n",
    "\n",
    "assert sum(grain.ort.isna()) == 0\n",
    "assert sum(grain.pho_small.isna()) == 0\n",
    "assert sum(grain.pho_large.isna()) == 0\n",
    "\n",
    "assert sum(taraban.ort.isna()) == 0\n",
    "assert sum(taraban.pho.isna()) == 0\n",
    "\n",
    "assert sum(glushko.ort.isna()) == 0\n",
    "assert sum(glushko.pho.isna()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def trim_ort(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = trim_ort(train)\n",
    "df_strain = trim_ort(strain)\n",
    "df_grain = trim_ort(grain)\n",
    "df_taraban = trim_ort(taraban)\n",
    "df_glushko = trim_ort(glushko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imageability missing data replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_missing(df, var):\n",
    "    print(\n",
    "        '{} missing in {}: {}/{}'.format(\n",
    "            var, df, sum(globals()[df][var].isna()), len((globals()[df]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "chk_missing('df_train', 'img')\n",
    "chk_missing('df_strain', 'img')\n",
    "chk_missing('df_grain', 'img')\n",
    "chk_missing('df_taraban', 'img')\n",
    "chk_missing('df_glushko', 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train['img'] = df_train.img.fillna(mean_img)\n",
    "\n",
    "# Fill missing value to condition mean img rating\n",
    "mean_strain_hi_img = df_strain.loc[df_strain.imageability == \"HI\", 'img'].mean()\n",
    "mean_strain_lo_img = df_strain.loc[df_strain.imageability == \"LI\", 'img'].mean()\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"HI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"HI\",\n",
    "                                     \"img\"].fillna(mean_strain_hi_img)\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"LI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"LI\",\n",
    "                                     \"img\"].fillna(mean_strain_lo_img)\n",
    "\n",
    "# Since taraban do not maniputate img, just replace by training set mean\n",
    "df_taraban['img'] = df_taraban.img.fillna(mean_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle homograph\n",
    "\n",
    "From Jay (201217)\n",
    "I'm guessing they just split the real frequency in two.  If it's possible to check that, even approximately, that would be good.  (If we don't have the WJ frequencies independently of these training sets, we could just ball park it--do the frequencies in the file look comparable, or half of, the frequencies of words that are more-or-less the same frequency in some other norms.  (I'm not sure if that's clear.  If not, we can talk about it.)\n",
    "\n",
    "- Split all frequency into n_dup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary for looking up word:n_dup\n",
    "\n",
    "tmp_count = train.groupby('word').agg('count').reset_index().loc[:,['word','wn_idx']]\n",
    "tmp_dups = tmp_count.loc[tmp_count.wn_idx>1,]\n",
    "dups_dict = dict(zip(tmp_dups.word, tmp_dups.wn_idx))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dups in testset\n",
    "strain.loc[strain.word.isin(dups_dict.keys()),]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban.loc[taraban.word.isin(dups_dict.keys()),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wf(row):\n",
    "    if row.word in dups_dict.keys():\n",
    "        return(row.wf / dups_dict[row.word])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['wf'] = train.apply(adjust_wf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('dataset/df_train.csv')\n",
    "df_strain.to_csv('dataset/df_strain.csv')\n",
    "df_grain.to_csv('dataset/df_grain.csv')\n",
    "df_taraban.to_csv('dataset/df_taraban.csv')\n",
    "df_glushko.to_csv('dataset/df_glushko.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export wordnet semantic representation (n=5821) for training set\n",
    "sem_train = y_wordnet[train.wn_idx,]\n",
    "print(f'Shape of selected semantic representation: {sem_train.shape}')\n",
    "np.savez_compressed('dataset/sem_train.npz', data=sem_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Strain semantic\n",
    "strain_word_idx = [df_train.loc[df_train.word==w,].index[0] for w in strain.word]\n",
    "sem_strain = sem_train[strain_word_idx,]\n",
    "np.savez_compressed('dataset/sem_strain.npz', data=sem_strain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print(\n",
    "                'Slot {} (n = {}, unique token = {}) {} \\n'.format(\n",
    "                    slotId, t.document_count, len(t.word_index.items()),\n",
    "                    t.word_docs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "        \n",
    "    return binData\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_word = pd.concat(\n",
    "    [\n",
    "        df_train.word, df_strain.word, df_grain.word, df_taraban.word,\n",
    "        df_glushko.word\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort, df_taraban.ort, df_glushko.ort],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "splitId_strain = len(df_train)\n",
    "splitId_grain = splitId_strain + len(df_strain)\n",
    "splitId_taraban = splitId_grain + len(df_grain)\n",
    "splitId_glushko = splitId_taraban + len(df_taraban)\n",
    "\n",
    "ort_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "ort_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "ort_grain = np.array(all_ort_bin[splitId_grain:splitId_taraban])\n",
    "ort_taraban = np.array(all_ort_bin[splitId_taraban:splitId_glushko])\n",
    "ort_glushko = np.array(all_ort_bin[splitId_glushko::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/ort_train.npz', data=ort_train)\n",
    "np.savez_compressed('dataset/ort_strain.npz', data=ort_strain)\n",
    "np.savez_compressed('dataset/ort_grain.npz', data=ort_grain)\n",
    "np.savez_compressed('dataset/ort_taraban.npz', data=ort_taraban)\n",
    "np.savez_compressed('dataset/ort_glushko.npz', data=ort_glushko)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('ort_train shape:', ort_train.shape)\n",
    "print('ort_strain shape:', ort_strain.shape)\n",
    "print('ort_grain shape:', ort_grain.shape)\n",
    "print('ort_taraban shape:', ort_taraban.shape)\n",
    "print('ort_glushko shape:', ort_glushko.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([len(p_col), binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        out = slotSeries.map(p_key).to_list()\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = out\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from src.data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "pho_train = pho2bin_v2(train.pho, phon_key)\n",
    "pho_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "pho_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "pho_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "pho_taraban = pho2bin_v2(taraban.pho, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/pho_train.npz', data=pho_train)\n",
    "np.savez_compressed('dataset/pho_strain.npz', data=pho_strain)\n",
    "np.savez_compressed('dataset/pho_large_grain.npz', data=pho_large_grain)\n",
    "np.savez_compressed('dataset/pho_small_grain.npz', data=pho_small_grain)\n",
    "np.savez_compressed('dataset/pho_taraban.npz', data=pho_taraban)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('pho_train shape:', pho_train.shape)\n",
    "print('pho_strain shape:', pho_strain.shape)\n",
    "print('pho_large_grain shape:', pho_large_grain.shape)\n",
    "print('pho_small_grain shape:', pho_small_grain.shape)\n",
    "print('pho_taraban shape:', pho_taraban.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import get_all_pronunciations_fast as gapf\n",
    "assert all(gapf(pho_train, phon_key) == df_train.pho)\n",
    "assert all(gapf(pho_strain, phon_key) == df_strain.pho)\n",
    "assert all(gapf(pho_large_grain, phon_key) == df_grain.pho_large)\n",
    "assert all(gapf(pho_small_grain, phon_key) == df_grain.pho_small)\n",
    "assert all(gapf(pho_taraban, phon_key) == df_taraban.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special format for Glushko PHO (due to multiple correct answer with different length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, pickle\n",
    "\n",
    "# Glushko pho dictionary\n",
    "phonology_glushko = {\n",
    "    x: ast.literal_eval(df_glushko.loc[i, 'pho'])\n",
    "    for i, x in enumerate(df_glushko.word)\n",
    "}\n",
    "\n",
    "# Glushko one-hot encoded output dictionary\n",
    "pho_glushko = {}\n",
    "for k, v in phonology_glushko.items():\n",
    "    ys = []\n",
    "    for pho in v:\n",
    "        y = []\n",
    "        for char in pho:\n",
    "            y += phon_key[char]\n",
    "        ys.append(y)\n",
    "    pho_glushko[k] = ys\n",
    "\n",
    "with open('dataset/pho_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(pho_glushko, f)\n",
    "\n",
    "print('y_glushko dimension: {}'.format(len(pho_glushko['beed'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('dataset/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "import data_wrangling\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "# plt.xlim((0, 200))\n",
    "# plt.ylim((0, .0006))\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "plt.xlim((0, 100))\n",
    "plt.ylim((0, .0002))\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new dictionary style representation (super fast hash table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_representation_mapping = dict(zip(train.word, train.index))\n",
    "\n",
    "ort, pho, sem = {}, {}, {}\n",
    "\n",
    "for word in df_train.word:\n",
    "    word_idx = word_representation_mapping[word]\n",
    "    ort[word] = ort_train[word_idx]\n",
    "    pho[word] = pho_train[word_idx]\n",
    "    sem[word] = sem_train[word_idx]\n",
    "\n",
    "representation = {'ort':ort, 'pho':pho, 'sem':sem}\n",
    "print(f'Total no. of training items: {len(ort.keys())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: representation[\"ort\" or \"pho\" or \"sem\"][\"word\"]\n",
    "print(f'Representations of \"cat\" are:\\n')\n",
    "print(f'ort: {representation[\"ort\"][\"cat\"]} \\n with shape = {representation[\"ort\"][\"cat\"].shape}\\n')\n",
    "print(f'pho: {representation[\"pho\"][\"cat\"]} \\n with shape = {representation[\"pho\"][\"cat\"].shape}\\n')\n",
    "print(f'sem: {representation[\"sem\"][\"cat\"]} \\n with shape = {representation[\"sem\"][\"cat\"].shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to pkl.gz\n",
    "import pickle, gzip\n",
    "\n",
    "with gzip.open('dataset/representation_dictionary.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(representation, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New test set pickle format\n",
    "a dictionary with 4 keys\n",
    "- item: maybe word or nonword string, easy to human eye\n",
    "- ort: orthgraphic representation\n",
    "- pho: phonological representation\n",
    "- sem: semantic representation\n",
    "\n",
    "If all item within training set, use data_wrangling.MyData.create_testset_from_train_idx() to create dictionary. \n",
    "\n",
    "Otherwise, create it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_wrangling\n",
    "import gzip, pickle\n",
    "from importlib import reload\n",
    "\n",
    "reload(data_wrangling)\n",
    "data = data_wrangling.MyData()\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain (all)\n",
    "strain_items = data.df_strain.word.unique()\n",
    "strain_items_idx = list(data.df_train.loc[data.df_train.word.isin(strain_items)].index)\n",
    "strain_dict = data.create_testset_from_train_idx(strain_items_idx)\n",
    "\n",
    "# with gzip.open(\"dataset/testsets/strain.pkl.gz\", \"wb\") as f:\n",
    "#     pickle.dump(strain_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain by each condition\n",
    "\n",
    "def make_strain_sub_testsets(df, f, c, i, save_file):\n",
    "\n",
    "    words = df.loc[\n",
    "        (df.frequency == f) & (df.pho_consistency == c) & (df.imageability == i),\n",
    "        \"word\",\n",
    "    ].unique()\n",
    "\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(words)].index)\n",
    "    testset_dict = data.create_testset_from_train_idx(idx)\n",
    "    with gzip.open(f\"dataset/testsets/{save_file}.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset_dict, f)\n",
    "\n",
    "\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"HI\", save_file=\"strain_hf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"LI\", save_file=\"strain_hf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"HI\", save_file=\"strain_hf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"LI\", save_file=\"strain_hf_inc_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"HI\", save_file=\"strain_lf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"LI\", save_file=\"strain_lf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"HI\", save_file=\"strain_lf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"LI\", save_file=\"strain_lf_inc_li\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (Removed all one ort, multi sem words)\n",
    "train_count = data.df_train.groupby(\"word\").count().reset_index()\n",
    "word_with_dup = list(train_count.loc[train_count.pho > 1, \"word\"])\n",
    "train_no_dup_idx = list(data.df_train.loc[~data.df_train.word.isin(word_with_dup)].index)\n",
    "train_dict = data.create_testset_from_train_idx(train_no_dup_idx)\n",
    "with gzip.open(\"dataset/testsets/train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.unique(train_dict['item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain ambiguous\n",
    "import numpy as np\n",
    "\n",
    "nw_amb_idx = list(data.df_grain.loc[data.df_grain.condition==\"ambiguous\"].index)\n",
    "\n",
    "grain_ambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_amb_idx]),\n",
    "    \"ort\": data.ort_grain[nw_amb_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_amb_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_amb_idx],\n",
    "    \"sem\": np.zeros((len(nw_amb_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_ambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_ambiguous_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain unambiguous\n",
    "nw_un_idx = list(data.df_grain.loc[data.df_grain.condition==\"unambiguous\"].index)\n",
    "\n",
    "grain_unambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_un_idx]),\n",
    "    \"ort\": data.ort_grain[nw_un_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_un_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_un_idx],\n",
    "    \"sem\": np.zeros((len(nw_un_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_unambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_unambiguous_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train img (3 groups)\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "img_map[\"3gp\"] = pd.qcut(img_map.img, 3, [\"low\", \"med\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [\"low\", \"med\", \"high\"]:\n",
    "    x = img_map.loc[img_map[\"3gp\"]==g, \"word\"]\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(x)].index)\n",
    "    testset = data.create_testset_from_train_idx(idx)\n",
    "    \n",
    "    with gzip.open(f\"dataset/testsets/cortese_3gp_{g}_img.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low imageability (median split by Cortese rating)\n",
    "low_img_cortese_word = list(img_map.loc[img_map.img < 4, \"word\"])\n",
    "low_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(low_img_cortese_word)].index\n",
    ")\n",
    "testset_low_img_cortest = data.create_testset_from_train_idx(low_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_low_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_low_img_cortest, f)\n",
    "\n",
    "# Hi imageability (median split by Cortese rating)\n",
    "hi_img_cortese_word = list(img_map.loc[img_map.img >= 4, \"word\"])\n",
    "hi_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(hi_img_cortese_word)].index\n",
    ")\n",
    "testset_hi_img_cortest = data.create_testset_from_train_idx(hi_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_hi_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_hi_img_cortest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taraban\n",
    "taraban_name_map = {\n",
    "    \"High-frequency exception\": \"HF-EXC\",\n",
    "    \"High-frequency regular-inconsistent\": \"HF-REG-INC\",\n",
    "    \"Low-frequency exception\": \"LF-EXC\",\n",
    "    \"Low-frequency regular-inconsistent\": \"LF-REG-INC\",\n",
    "    \"Regular control for High-frequency exception\": \"CTRL-HF-EXC\",\n",
    "    \"Regular control for High-frequency regular-inconsistent\": \"CTRL-HF-REG-INC\",\n",
    "    \"Regular control for Low-frequency exception\": \"CTRL-LF-EXC\",\n",
    "    \"Regular control for Low-frequency regular-inconsistent\": \"CTRL-LF-REG-INC\",\n",
    "}\n",
    "\n",
    "\n",
    "for c in taraban.cond.unique():\n",
    "    idx = list(\n",
    "        data.df_train.loc[\n",
    "            data.df_train.word.isin(taraban.loc[taraban.cond == c, \"word\"]),\n",
    "        ].index\n",
    "    )\n",
    "    print(idx)\n",
    "\n",
    "    with gzip.open(\n",
    "        f\"dataset/testsets/taraban_{taraban_name_map[c].lower()}.pkl.gz\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(data.create_testset_from_train_idx(idx), f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python379jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
