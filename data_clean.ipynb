{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tidy source files to keys csv"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%reload_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Read training file\n",
    "train_file = \"issues/preprocessing/6ktraining_v2.dict\"\n",
    "\n",
    "strain_file = \"issues/preprocessing/strain.txt\"\n",
    "strain_key_file = \"issues/preprocessing/strain_key.txt\"\n",
    "\n",
    "grain_file = \"issues/preprocessing/grain_nws.dict\"\n",
    "grain_key_file = \"issues/preprocessing/grain_key.txt\"\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "\n",
    "# Zeno norm\n",
    "zeno = pd.read_csv(\"issues/preprocessing/EWFG.csv\", na_values=\"\", keep_default_na=False)\n",
    "zeno[\"gr14\"] = pd.to_numeric(zeno.f, errors=\"coerce\")  # Stage 14 is adult frequency\n",
    "[zeno.pop(v) for v in [\"sfi\", \"d\", \"u\", \"f\"]]\n",
    "clear_output()\n",
    "\n",
    "# Chang training set (from Chang 2019 github)\n",
    "y_wordnet = np.genfromtxt(\"issues/preprocessing/wordNet_6229.csv\", delimiter=\",\")\n",
    "\n",
    "wordnet_dict = pd.read_csv(\n",
    "    \"issues/preprocessing/6kdict\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "## Copy index for key\n",
    "wordnet_dict[\"wn_idx\"] = wordnet_dict.index\n",
    "\n",
    "## Drop wordnet duplicates (Do not drop)\n",
    "## HS04: There were 39 words in which a single spelling was associated with two or more meanings (mainly words such as SHEEP, FISH, or HIT, whose plural or past tense morphological inflection involves no change from the stem).\n",
    "# wordnet_dict.drop_duplicates(subset=['word'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Merge Zeno and IMG into train\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "train = pd.merge(train, zeno, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Assume Zeno missing = 0\n",
    "for x in range(14):\n",
    "    variable_name = 'gr' + str(x+1)\n",
    "    train[variable_name] = train[variable_name].map(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "train = pd.merge(train, img_map, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "\n",
    "# Merge Chang\n",
    "wnid = wordnet_dict.loc[:,['word', 'wn_idx']]\n",
    "train = train.merge(wnid, how='inner', on='word')\n",
    "print(f'Words in training set: {len(train)}')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strain = pd.read_csv(\n",
    "    strain_file, sep=\"\\t\", header=None, names=[\"word\", \"ort\", \"pho\", \"wf\"]\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=[\"word\", \"frequency\", \"pho_consistency\", \"imageability\"],\n",
    ")\n",
    "\n",
    "strain = pd.merge(strain, strain_key)\n",
    "strain = pd.merge(strain, img_map, on=\"word\", how=\"left\")\n",
    "strain.sample(5)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strain.groupby(\"frequency\").mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "grain = pd.merge(grain, grain_key)\n",
    "\n",
    "grain['img'] = 0\n",
    "grain['wf'] = 0\n",
    "grain.sample(5)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taraban = pd.read_csv(\"issues/preprocessing/taraban.csv\")\n",
    "taraban.columns = [\"id\", \"cond\", \"word\", \"ort\", \"pho\", \"wf\"]\n",
    "taraban = pd.merge(taraban, img_map, on=\"word\", how=\"left\")\n",
    "taraban.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "glushko = pd.read_csv(\"issues/preprocessing/glushko_nonword.csv\")\n",
    "glushko.columns = [\"id\", \"cond\", \"word\", \"pho\", \"ort\"]\n",
    "\n",
    "glushko[\"img\"] = 0\n",
    "glushko[\"wf\"] = 0\n",
    "glushko.sample(5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check raw data integrity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check all represtation follow 14 ort, 10 pho format\n",
    "assert all([len(x) == 14 for x in train.ort])\n",
    "assert all([len(x) == 14 for x in strain.ort])\n",
    "assert all([len(x) == 14 for x in grain.ort])\n",
    "assert all([len(x) == 14 for x in taraban.ort])\n",
    "assert all([len(x) == 14 for x in glushko.ort])\n",
    "\n",
    "assert all([len(x) == 10 for x in train.pho])\n",
    "assert all([len(x) == 10 for x in strain.pho])\n",
    "assert all([len(x) == 10 for x in grain.pho_small])\n",
    "assert all([len(x) == 10 for x in grain.pho_large])\n",
    "assert all([len(x) == 10 for x in taraban.pho])\n",
    "\n",
    "from ast import literal_eval\n",
    "for pho in glushko.pho:\n",
    "    ps = literal_eval(pho)\n",
    "    for p in ps:\n",
    "        assert len(p) == 10\n",
    "\n",
    "# Check all fufill trim_ort criteria\n",
    "locs = [0, 11, 12, 13]\n",
    "\n",
    "for l in locs:\n",
    "    assert all([x == '_' for x in train.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in strain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in grain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in taraban.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in glushko.ort.str.get(l)])\n",
    "\n",
    "# No missing data in critical variables\n",
    "assert sum(train.ort.isna()) == 0\n",
    "assert sum(train.pho.isna()) == 0\n",
    "assert sum(train.wf.isna()) == 0\n",
    "\n",
    "assert sum(strain.ort.isna()) == 0\n",
    "assert sum(strain.pho.isna()) == 0\n",
    "assert sum(strain.wf.isna()) == 0\n",
    "\n",
    "assert sum(grain.ort.isna()) == 0\n",
    "assert sum(grain.pho_small.isna()) == 0\n",
    "assert sum(grain.pho_large.isna()) == 0\n",
    "\n",
    "assert sum(taraban.ort.isna()) == 0\n",
    "assert sum(taraban.pho.isna()) == 0\n",
    "\n",
    "assert sum(glushko.ort.isna()) == 0\n",
    "assert sum(glushko.pho.isna()) == 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def trim_ort(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = trim_ort(train)\n",
    "df_strain = trim_ort(strain)\n",
    "df_grain = trim_ort(grain)\n",
    "df_taraban = trim_ort(taraban)\n",
    "df_glushko = trim_ort(glushko)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imageability missing data replacement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def chk_missing(df, var):\n",
    "    print(\n",
    "        '{} missing in {}: {}/{}'.format(\n",
    "            var, df, sum(globals()[df][var].isna()), len((globals()[df]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "chk_missing('df_train', 'img')\n",
    "chk_missing('df_strain', 'img')\n",
    "chk_missing('df_grain', 'img')\n",
    "chk_missing('df_taraban', 'img')\n",
    "chk_missing('df_glushko', 'img')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train['img'] = df_train.img.fillna(mean_img)\n",
    "\n",
    "# Fill missing value to condition mean img rating\n",
    "mean_strain_hi_img = df_strain.loc[df_strain.imageability == \"HI\", 'img'].mean()\n",
    "mean_strain_lo_img = df_strain.loc[df_strain.imageability == \"LI\", 'img'].mean()\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"HI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"HI\",\n",
    "                                     \"img\"].fillna(mean_strain_hi_img)\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"LI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"LI\",\n",
    "                                     \"img\"].fillna(mean_strain_lo_img)\n",
    "\n",
    "# Since taraban do not maniputate img, just replace by training set mean\n",
    "df_taraban['img'] = df_taraban.img.fillna(mean_img)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Handle homograph\n",
    "\n",
    "From Jay (201217)\n",
    "I'm guessing they just split the real frequency in two.  If it's possible to check that, even approximately, that would be good.  (If we don't have the WJ frequencies independently of these training sets, we could just ball park it--do the frequencies in the file look comparable, or half of, the frequencies of words that are more-or-less the same frequency in some other norms.  (I'm not sure if that's clear.  If not, we can talk about it.)\n",
    "\n",
    "- Split all frequency into n_dup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Build a dictionary for looking up word:n_dup\n",
    "\n",
    "tmp_count = train.groupby('word').agg('count').reset_index().loc[:,['word','wn_idx']]\n",
    "tmp_dups = tmp_count.loc[tmp_count.wn_idx>1,]\n",
    "dups_dict = dict(zip(tmp_dups.word, tmp_dups.wn_idx))\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check dups in testset\n",
    "strain.loc[strain.word.isin(dups_dict.keys()),]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "taraban.loc[taraban.word.isin(dups_dict.keys()),]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def adjust_wf(row):\n",
    "    if row.word in dups_dict.keys():\n",
    "        return(row.wf / dups_dict[row.word])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['wf'] = train.apply(adjust_wf, axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_train.to_csv('dataset/df_train.csv')\n",
    "df_strain.to_csv('dataset/df_strain.csv')\n",
    "df_grain.to_csv('dataset/df_grain.csv')\n",
    "df_taraban.to_csv('dataset/df_taraban.csv')\n",
    "df_glushko.to_csv('dataset/df_glushko.csv')"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save semantics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export wordnet semantic representation (n=5821) for training set\n",
    "sem_train = y_wordnet[train.wn_idx,]\n",
    "print(f'Shape of selected semantic representation: {sem_train.shape}')\n",
    "np.savez_compressed('dataset/sem_train.npz', data=sem_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export Strain semantic\n",
    "strain_word_idx = [df_train.loc[df_train.word==w,].index[0] for w in strain.word]\n",
    "sem_strain = sem_train[strain_word_idx,]\n",
    "np.savez_compressed('dataset/sem_strain.npz', data=sem_strain)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encode input and output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print(\n",
    "                'Slot {} (n = {}, unique token = {}) {} \\n'.format(\n",
    "                    slotId, t.document_count, len(t.word_index.items()),\n",
    "                    t.word_docs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "        \n",
    "    return binData\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_word = pd.concat(\n",
    "    [\n",
    "        df_train.word, df_strain.word, df_grain.word, df_taraban.word,\n",
    "        df_glushko.word\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort, df_taraban.ort, df_glushko.ort],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=True)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "splitId_strain = len(df_train)\n",
    "splitId_grain = splitId_strain + len(df_strain)\n",
    "splitId_taraban = splitId_grain + len(df_grain)\n",
    "splitId_glushko = splitId_taraban + len(df_taraban)\n",
    "\n",
    "ort_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "ort_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "ort_grain = np.array(all_ort_bin[splitId_grain:splitId_taraban])\n",
    "ort_taraban = np.array(all_ort_bin[splitId_taraban:splitId_glushko])\n",
    "ort_glushko = np.array(all_ort_bin[splitId_glushko::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/ort_train.npz', data=ort_train)\n",
    "np.savez_compressed('dataset/ort_strain.npz', data=ort_strain)\n",
    "np.savez_compressed('dataset/ort_grain.npz', data=ort_grain)\n",
    "np.savez_compressed('dataset/ort_taraban.npz', data=ort_taraban)\n",
    "np.savez_compressed('dataset/ort_glushko.npz', data=ort_glushko)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('ort_train shape:', ort_train.shape)\n",
    "print('ort_strain shape:', ort_strain.shape)\n",
    "print('ort_grain shape:', ort_grain.shape)\n",
    "print('ort_taraban shape:', ort_taraban.shape)\n",
    "print('ort_glushko shape:', ort_glushko.shape)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([len(p_col), binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        out = slotSeries.map(p_key).to_list()\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = out\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from src.data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "pho_train = pho2bin_v2(train.pho, phon_key)\n",
    "pho_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "pho_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "pho_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "pho_taraban = pho2bin_v2(taraban.pho, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/pho_train.npz', data=pho_train)\n",
    "np.savez_compressed('dataset/pho_strain.npz', data=pho_strain)\n",
    "np.savez_compressed('dataset/pho_large_grain.npz', data=pho_large_grain)\n",
    "np.savez_compressed('dataset/pho_small_grain.npz', data=pho_small_grain)\n",
    "np.savez_compressed('dataset/pho_taraban.npz', data=pho_taraban)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('pho_train shape:', pho_train.shape)\n",
    "print('pho_strain shape:', pho_strain.shape)\n",
    "print('pho_large_grain shape:', pho_large_grain.shape)\n",
    "print('pho_small_grain shape:', pho_small_grain.shape)\n",
    "print('pho_taraban shape:', pho_taraban.shape)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decoding check"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from evaluate import get_all_pronunciations_fast as get_p\n",
    "assert all(get_p(pho_train, phon_key) == df_train.pho)\n",
    "assert all(get_p(pho_strain, phon_key) == df_strain.pho)\n",
    "assert all(get_p(pho_large_grain, phon_key) == df_grain.pho_large)\n",
    "assert all(get_p(pho_small_grain, phon_key) == df_grain.pho_small)\n",
    "assert all(get_p(pho_taraban, phon_key) == df_taraban.pho)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Special format for Glushko PHO (due to multiple correct answer with different length)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ast, pickle\n",
    "\n",
    "# Glushko pho dictionary\n",
    "phonology_glushko = {\n",
    "    x: ast.literal_eval(df_glushko.loc[i, 'pho'])\n",
    "    for i, x in enumerate(df_glushko.word)\n",
    "}\n",
    "\n",
    "# Glushko one-hot encoded output dictionary\n",
    "pho_glushko = {}\n",
    "for k, v in phonology_glushko.items():\n",
    "    ys = []\n",
    "    for pho in v:\n",
    "        y = []\n",
    "        for char in pho:\n",
    "            y += phon_key[char]\n",
    "        ys.append(y)\n",
    "    pho_glushko[k] = ys\n",
    "\n",
    "with open('dataset/pho_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(pho_glushko, f)\n",
    "\n",
    "print('y_glushko dimension: {}'.format(len(pho_glushko['beed'][0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing and evaluating new sampling probability"
   ],
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('dataset/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "import data_wrangling\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "# plt.xlim((0, 200))\n",
    "# plt.ylim((0, .0006))\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.Sampling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "plt.xlim((0, 100))\n",
    "plt.ylim((0, .0002))\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create new dictionary style representation (hash table)\n",
    "each word contains a maximum of 3 representations\n",
    "- orthography(ort): trimed slot-based one-hot letters encoding \n",
    "- phonology(pho): phonological features from Jason Zevin based on HS04 and Harm, 1998\n",
    "- semantic(sem): based on wordnet, clone from [Chang, 19 JML](https://github.com/JasonLo/Chang_Monaghan_Welbourne_AoA_Paper_for_JML)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_representation_mapping = dict(zip(train.word, train.index))\n",
    "\n",
    "ort, pho, sem = {}, {}, {}\n",
    "\n",
    "for word in df_train.word:\n",
    "    word_idx = word_representation_mapping[word]\n",
    "    ort[word] = ort_train[word_idx]\n",
    "    pho[word] = pho_train[word_idx]\n",
    "    sem[word] = sem_train[word_idx]\n",
    "\n",
    "representation = {'ort':ort, 'pho':pho, 'sem':sem}\n",
    "print(f'Total no. of training items: {len(ort.keys())}')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Usage: representation[\"ort\" or \"pho\" or \"sem\"][\"word\"]\n",
    "print(f'Representations of \"cat\" are:\\n')\n",
    "print(f'ort: {representation[\"ort\"][\"cat\"]} \\n with shape = {representation[\"ort\"][\"cat\"].shape}\\n')\n",
    "print(f'pho: {representation[\"pho\"][\"cat\"]} \\n with shape = {representation[\"pho\"][\"cat\"].shape}\\n')\n",
    "print(f'sem: {representation[\"sem\"][\"cat\"]} \\n with shape = {representation[\"sem\"][\"cat\"].shape}\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Export to pkl.gz\n",
    "import pickle, gzip\n",
    "\n",
    "with gzip.open('dataset/representation_dictionary.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(representation, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# New test set pickle format\n",
    "a dictionary with 4 keys\n",
    "- item: maybe word or nonword string, easy to human eye\n",
    "- ort: orthgraphic representation\n",
    "- pho: phonological representation\n",
    "- sem: semantic representation\n",
    "\n",
    "If all item within training set, use data_wrangling.MyData.create_testset_from_train_idx() to create dictionary. \n",
    "\n",
    "Otherwise, create it manually"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import data_wrangling\n",
    "import gzip, pickle\n",
    "from importlib import reload\n",
    "\n",
    "reload(data_wrangling)\n",
    "data = data_wrangling.MyData()\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\n",
    "    \"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False\n",
    ")\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Strain (all)\n",
    "strain_items = data.df_strain.word.unique()\n",
    "strain_items_idx = list(data.df_train.loc[data.df_train.word.isin(strain_items)].index)\n",
    "strain_dict = data.create_testset_from_train_idx(strain_items_idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(strain_dict['item']) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strain_dict['cond'] = list(data.df_strain.frequency + '_' + data.df_strain.pho_consistency + '_' + data.df_strain.imageability)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "strain_dict['freq'] = list(data.df_strain.frequency)\n",
    "strain_dict['cons'] = list(data.df_strain.pho_consistency)\n",
    "strain_dict['img'] = list(data.df_strain.imageability)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with gzip.open(\"dataset/testsets/strain.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(strain_dict, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Strain by each condition\n",
    "\n",
    "def make_strain_sub_testsets(df, f, c, i, save_file):\n",
    "\n",
    "    words = df.loc[\n",
    "        (df.frequency == f) & (df.pho_consistency == c) & (df.imageability == i),\n",
    "        \"word\",\n",
    "    ].unique()\n",
    "\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(words)].index)\n",
    "    testset_dict = data.create_testset_from_train_idx(idx)\n",
    "    with gzip.open(f\"dataset/testsets/{save_file}.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset_dict, f)\n",
    "\n",
    "\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"HI\", save_file=\"strain_hf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"CON\", i=\"LI\", save_file=\"strain_hf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"HI\", save_file=\"strain_hf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"HF\", c=\"INC\", i=\"LI\", save_file=\"strain_hf_inc_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"HI\", save_file=\"strain_lf_con_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"CON\", i=\"LI\", save_file=\"strain_lf_con_li\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"HI\", save_file=\"strain_lf_inc_hi\"\n",
    ")\n",
    "make_strain_sub_testsets(\n",
    "    data.df_strain, f=\"LF\", c=\"INC\", i=\"LI\", save_file=\"strain_lf_inc_li\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train (Removed all one ort, multi sem words)\n",
    "train_count = data.df_train.groupby(\"word\").count().reset_index()\n",
    "word_with_dup = list(train_count.loc[train_count.pho > 1, \"word\"])\n",
    "train_no_dup_idx = list(data.df_train.loc[~data.df_train.word.isin(word_with_dup)].index)\n",
    "train_dict = data.create_testset_from_train_idx(train_no_dup_idx)\n",
    "with gzip.open(\"dataset/testsets/train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_dict, f)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "np.unique(train_dict['item'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "input_path = '/home/jupyter/tf/dataset'\n",
    "o = \n",
    "p_large = \n",
    "p_small = "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grain all\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "d = {\n",
    "    \"item\": list(data.df_grain.word),\n",
    "    \"cond\": list(data.df_grain.condition),\n",
    "    \"ort\": np.load(os.path.join(input_path, \"ort_grain.npz\"))[\"data\"],\n",
    "    \"pho_large_grain\": np.load(os.path.join(input_path, \"pho_large_grain.npz\"))[\"data\"],\n",
    "    \"pho_small_grain\": np.load(os.path.join(input_path, \"pho_small_grain.npz\"))[\"data\"],\n",
    "    \"sem\": np.zeros((len(data.df_grain.word), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(d, f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grain ambiguous\n",
    "import numpy as np\n",
    "\n",
    "nw_amb_idx = list(data.df_grain.loc[data.df_grain.condition==\"ambiguous\"].index)\n",
    "\n",
    "grain_ambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_amb_idx]),\n",
    "    \"ort\": data.ort_grain[nw_amb_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_amb_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_amb_idx],\n",
    "    \"sem\": np.zeros((len(nw_amb_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_ambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_ambiguous_dict, f)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grain unambiguous\n",
    "nw_un_idx = list(data.df_grain.loc[data.df_grain.condition==\"unambiguous\"].index)\n",
    "\n",
    "grain_unambiguous_dict = {\n",
    "    \"item\": list(data.df_grain.word[nw_un_idx]),\n",
    "    \"ort\": data.ort_grain[nw_un_idx],\n",
    "    \"pho_large_grain\": data.pho_large_grain[nw_un_idx],\n",
    "    \"pho_small_grain\": data.pho_small_grain[nw_un_idx],\n",
    "    \"sem\": np.zeros((len(nw_un_idx), 2446))\n",
    "}\n",
    "\n",
    "with gzip.open(\"dataset/testsets/grain_unambiguous.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(grain_unambiguous_dict, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train img (3 groups)\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "img_map[\"3gp\"] = pd.qcut(img_map.img, 3, [\"low\", \"med\", \"high\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for g in [\"low\", \"med\", \"high\"]:\n",
    "    x = img_map.loc[img_map[\"3gp\"]==g, \"word\"]\n",
    "    idx = list(data.df_train.loc[data.df_train.word.isin(x)].index)\n",
    "    testset = data.create_testset_from_train_idx(idx)\n",
    "    \n",
    "    with gzip.open(f\"dataset/testsets/cortese_3gp_{g}_img.pkl.gz\", \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Low imageability (median split by Cortese rating)\n",
    "low_img_cortese_word = list(img_map.loc[img_map.img < 4, \"word\"])\n",
    "low_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(low_img_cortese_word)].index\n",
    ")\n",
    "testset_low_img_cortest = data.create_testset_from_train_idx(low_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_low_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_low_img_cortest, f)\n",
    "\n",
    "# Hi imageability (median split by Cortese rating)\n",
    "hi_img_cortese_word = list(img_map.loc[img_map.img >= 4, \"word\"])\n",
    "hi_img_cortese_idx = list(\n",
    "    data.df_train.loc[data.df_train.word.isin(hi_img_cortese_word)].index\n",
    ")\n",
    "testset_hi_img_cortest = data.create_testset_from_train_idx(hi_img_cortese_idx)\n",
    "with gzip.open(\"dataset/testsets/cortese_hi_img.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(testset_hi_img_cortest, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Taraban\n",
    "taraban_name_map = {\n",
    "    \"High-frequency exception\": \"HF-EXC\",\n",
    "    \"High-frequency regular-inconsistent\": \"HF-REG-INC\",\n",
    "    \"Low-frequency exception\": \"LF-EXC\",\n",
    "    \"Low-frequency regular-inconsistent\": \"LF-REG-INC\",\n",
    "    \"Regular control for High-frequency exception\": \"CTRL-HF-EXC\",\n",
    "    \"Regular control for High-frequency regular-inconsistent\": \"CTRL-HF-REG-INC\",\n",
    "    \"Regular control for Low-frequency exception\": \"CTRL-LF-EXC\",\n",
    "    \"Regular control for Low-frequency regular-inconsistent\": \"CTRL-LF-REG-INC\",\n",
    "}\n",
    "\n",
    "\n",
    "for c in taraban.cond.unique():\n",
    "    idx = list(\n",
    "        data.df_train.loc[\n",
    "            data.df_train.word.isin(taraban.loc[taraban.cond == c, \"word\"]),\n",
    "        ].index\n",
    "    )\n",
    "    print(idx)\n",
    "\n",
    "    with gzip.open(\n",
    "        f\"dataset/testsets/taraban_{taraban_name_map[c].lower()}.pkl.gz\", \"wb\"\n",
    "    ) as f:\n",
    "        pickle.dump(data.create_testset_from_train_idx(idx), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Grain multi pho (add multiple answer to axis 0)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "import pickle, gzip\n",
    "\n",
    "def convert_grain(file):\n",
    "    testset = data_wrangling.load_testset(file)\n",
    "    s = testset['pho_small_grain']\n",
    "    l = testset['pho_large_grain']\n",
    "    es = np.expand_dims(s, axis = 1)\n",
    "    el = np.expand_dims(l, axis = 1)\n",
    "    testset['pho'] = np.concatenate((es, el), axis=1)\n",
    "\n",
    "    print(f\"Merged large and small grain pho into multi-answer format, with dim (items, ans, pho units)={testset['pho'].shape}\")\n",
    "    with gzip.open(file, \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "\n",
    "names = ('grain', 'grain_ambiguous', 'grain_unambiguous')\n",
    "files = [os.path.join(\"/home/jupyter/tf/dataset/testsets\", f\"{x}.pkl.gz\") for x in names]\n",
    "[convert_grain(f) for f in files]\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading tests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle, gzip\n",
    "import pandas as pd \n",
    "\n",
    "testset_file = \"dataset/testsets/grain_unambiguous.pkl.gz\"\n",
    "\n",
    " \n",
    "with gzip.open(testset_file, \"rb\") as f:\n",
    "    testset = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testset.keys()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "testset['pho_large_grain']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Get output [epoch, tick, y, item]\n",
    "2. Get ans key [y, item]\n",
    "3. Parallel eval is possible, cast to [epoch, tick]\n",
    "4. Eval routine: acc, sse\n",
    "5. Eval extra: out1, out0, other diagnostic\n",
    "6. Combine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create 300 words test set for 3 levels of difficulties"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def sample_within_difficulty(low:float, high:float) -> [List[int], List[str]]:\n",
    "    \"\"\"Sample 100 words and returns its index within a range of difficulty levels\n",
    "    args:\n",
    "        low: lower bound of difficulty in percetile rank\n",
    "        high: upper bound of difficult in percetile rank\n",
    "    \"\"\"\n",
    "\n",
    "    data = data_wrangling.MyData()\n",
    "    pct = data.df_train.wf.rank(pct=True, ascending=False)\n",
    "    sel = pct.loc[(pct >= low) & (pct <= high)].sample(100)\n",
    "    words = data.df_train.loc[sel.index, \"word\"]\n",
    "    return sel.index.values, list(words)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import gzip, pickle\n",
    "\n",
    "def create_testset(idx: List[int], filename:str) -> dict:\n",
    "    \"\"\"Create a testset for a given list of indices\n",
    "    args:\n",
    "        idx: list of indices\n",
    "    \"\"\"\n",
    "    data = data_wrangling.MyData()\n",
    "    testset = data.create_testset_from_train_idx(idx)\n",
    "    \n",
    "    # Add condition labels\n",
    "    testset['cond'] = ['low'] * 100 + ['mid'] * 100 + ['hi'] * 100\n",
    "    \n",
    "    full_pickle_file_path = f\"dataset/testsets/{filename}.pkl.gz\"\n",
    "\n",
    "    with gzip.open(full_pickle_file_path, \"wb\") as f:\n",
    "        pickle.dump(testset, f)\n",
    "\n",
    "    return testset\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "low_idx, low_words = sample_within_difficulty(low=0.0, high=0.2)\n",
    "mid_idx, mid_words = sample_within_difficulty(low=0.4, high=0.6)\n",
    "hi_idx, hi_words = sample_within_difficulty(low=0.8, high=1.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "difficulty_testset_idx = np.concatenate([low_idx, mid_idx, hi_idx])\n",
    "create_testset(idx=difficulty_testset_idx, filename=\"train_r300_difficulty\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import data_wrangling\n",
    "import numpy as np\n",
    "from typing import List"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add difficulty label to random 100"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "r100 = data_wrangling.load_testset('train_r100')\n",
    "data = data_wrangling.MyData()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "median_wf = data.df_train.wf.median()\n",
    "df = data.df_train[['word', 'wf']].copy()\n",
    "df['group'] = df.wf.apply(lambda x: 'lf' if x < median_wf else 'hf')\n",
    "mapper = {word: group for word, group in zip(df.word, df.group)}\n",
    "r100['cond'] = [mapper[word] for word in r100['item']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "import gzip, pickle\n",
    "full_pickle_file_path = f\"dataset/testsets/{'train_r100'}.pkl.gz\"\n",
    "\n",
    "with gzip.open(full_pickle_file_path, \"wb\") as f:\n",
    "        pickle.dump(r100, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}