{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Read training file\n",
    "train_file = \"issues/preprocessing/6ktraining_v2.dict\"\n",
    "\n",
    "strain_file = \"issues/preprocessing/strain.txt\"\n",
    "strain_key_file = \"issues/preprocessing/strain_key.txt\"\n",
    "\n",
    "grain_file = \"issues/preprocessing/grain_nws.dict\"\n",
    "grain_key_file = \"issues/preprocessing/grain_key.txt\"\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv(\"issues/preprocessing/cortese2004norms.csv\", skiprows=9, na_filter=False)\n",
    "img_map = cortese[[\"item\", \"rating\"]]\n",
    "img_map.columns = [\"word\", \"img\"]\n",
    "\n",
    "# Zeno norm\n",
    "zeno = pd.read_csv(\"issues/preprocessing/EWFG.csv\", na_values='', keep_default_na=False)\n",
    "zeno['gr14'] = pd.to_numeric(zeno.f, errors='coerce') # Stage 14 is adult frequency\n",
    "[zeno.pop(v) for v in ['sfi', 'd', 'u', 'f']]\n",
    "clear_output()\n",
    "\n",
    "# Chang training set (from Chang 2019 github)\n",
    "y_wordnet = np.genfromtxt('issues/preprocessing/wordNet_6229.csv', delimiter=',')\n",
    "\n",
    "wordnet_dict = pd.read_csv(\n",
    "    'issues/preprocessing/6kdict',\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "## Copy index for key\n",
    "wordnet_dict['wn_idx'] = wordnet_dict.index\n",
    "\n",
    "## Drop wordnet duplicates\n",
    "wordnet_dict.drop_duplicates(subset=['word'], inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export sampling probability\n",
    "\n",
    "1. prob_log.txt: log compressed frequency\n",
    "2. prob_hs04.txt: hs04 implementation\n",
    "3. prob_jay.txt: jay's implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Zeno and IMG into train\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")\n",
    "\n",
    "train = pd.merge(train, zeno, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Assume Zeno missing = 0\n",
    "for x in range(14):\n",
    "    variable_name = 'gr' + str(x+1)\n",
    "    train[variable_name] = train[variable_name].map(lambda x: 0 if np.isnan(x) else x)\n",
    "\n",
    "train = pd.merge(train, img_map, on=\"word\", how=\"left\", validate=\"1:1\")\n",
    "\n",
    "# Merge Chang\n",
    "train = train.merge(wordnet_dict.loc[:,['wn_idx', 'word']], how='inner', on='word', validate='1:1')\n",
    "print(f'Words in training set: {len(train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "strain = pd.read_csv(\n",
    "    strain_file, sep=\"\\t\", header=None, names=[\"word\", \"ort\", \"pho\", \"wf\"]\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=[\"word\", \"frequency\", \"pho_consistency\", \"imageability\"],\n",
    ")\n",
    "\n",
    "strain = pd.merge(strain, strain_key)\n",
    "strain = pd.merge(strain, img_map, on=\"word\", how=\"left\")\n",
    "strain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strain.groupby(\"frequency\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "grain = pd.merge(grain, grain_key)\n",
    "\n",
    "grain['img'] = 0\n",
    "grain['wf'] = 0\n",
    "grain.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taraban = pd.read_csv('issues/preprocessing/taraban.csv')\n",
    "taraban.columns = ['id', 'cond', 'word', 'ort', 'pho', 'wf']\n",
    "taraban = pd.merge(taraban, img_map, on='word', how='left')\n",
    "taraban.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glushko = pd.read_csv('issues/preprocessing/glushko_nonword.csv')\n",
    "glushko.columns = ['id', 'cond', 'word', 'pho', 'ort']\n",
    "\n",
    "glushko['img'] = 0\n",
    "glushko['wf'] = 0\n",
    "glushko.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check raw data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all represtation follow 14 ort, 10 pho format\n",
    "assert all([len(x) == 14 for x in train.ort])\n",
    "assert all([len(x) == 14 for x in strain.ort])\n",
    "assert all([len(x) == 14 for x in grain.ort])\n",
    "assert all([len(x) == 14 for x in taraban.ort])\n",
    "assert all([len(x) == 14 for x in glushko.ort])\n",
    "\n",
    "assert all([len(x) == 10 for x in train.pho])\n",
    "assert all([len(x) == 10 for x in strain.pho])\n",
    "assert all([len(x) == 10 for x in grain.pho_small])\n",
    "assert all([len(x) == 10 for x in grain.pho_large])\n",
    "assert all([len(x) == 10 for x in taraban.pho])\n",
    "\n",
    "from ast import literal_eval\n",
    "for pho in glushko.pho:\n",
    "    ps = literal_eval(pho)\n",
    "    for p in ps:\n",
    "        assert len(p) == 10\n",
    "\n",
    "# Check all fufill trim_ort criteria\n",
    "locs = [0, 11, 12, 13]\n",
    "\n",
    "for l in locs:\n",
    "    assert all([x == '_' for x in train.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in strain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in grain.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in taraban.ort.str.get(l)])\n",
    "    assert all([x == '_' for x in glushko.ort.str.get(l)])\n",
    "\n",
    "# No missing data in critical variables\n",
    "assert sum(train.ort.isna()) == 0\n",
    "assert sum(train.pho.isna()) == 0\n",
    "assert sum(train.wf.isna()) == 0\n",
    "\n",
    "assert sum(strain.ort.isna()) == 0\n",
    "assert sum(strain.pho.isna()) == 0\n",
    "assert sum(strain.wf.isna()) == 0\n",
    "\n",
    "assert sum(grain.ort.isna()) == 0\n",
    "assert sum(grain.pho_small.isna()) == 0\n",
    "assert sum(grain.pho_large.isna()) == 0\n",
    "\n",
    "assert sum(taraban.ort.isna()) == 0\n",
    "assert sum(taraban.pho.isna()) == 0\n",
    "\n",
    "assert sum(glushko.ort.isna()) == 0\n",
    "assert sum(glushko.pho.isna()) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def trim_ort(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = trim_ort(train)\n",
    "df_strain = trim_ort(strain)\n",
    "df_grain = trim_ort(grain)\n",
    "df_taraban = trim_ort(taraban)\n",
    "df_glushko = trim_ort(glushko)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imageability missing data replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chk_missing(df, var):\n",
    "    print(\n",
    "        '{} missing in {}: {}/{}'.format(\n",
    "            var, df, sum(globals()[df][var].isna()), len((globals()[df]))\n",
    "        )\n",
    "    )\n",
    "\n",
    "chk_missing('df_train', 'img')\n",
    "chk_missing('df_strain', 'img')\n",
    "chk_missing('df_grain', 'img')\n",
    "chk_missing('df_taraban', 'img')\n",
    "chk_missing('df_glushko', 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train['img'] = df_train.img.fillna(mean_img)\n",
    "\n",
    "# Fill missing value to condition mean img rating\n",
    "mean_strain_hi_img = df_strain.loc[df_strain.imageability == \"HI\", 'img'].mean()\n",
    "mean_strain_lo_img = df_strain.loc[df_strain.imageability == \"LI\", 'img'].mean()\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"HI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"HI\",\n",
    "                                     \"img\"].fillna(mean_strain_hi_img)\n",
    "\n",
    "df_strain.loc[df_strain.imageability == \"LI\",\n",
    "              \"img\"] = df_strain.loc[df_strain.imageability == \"LI\",\n",
    "                                     \"img\"].fillna(mean_strain_lo_img)\n",
    "\n",
    "# Since taraban do not maniputate img, just replace by training set mean\n",
    "df_taraban['img'] = df_taraban.img.fillna(mean_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('dataset/df_train.csv')\n",
    "df_strain.to_csv('dataset/df_strain.csv')\n",
    "df_grain.to_csv('dataset/df_grain.csv')\n",
    "df_taraban.to_csv('dataset/df_taraban.csv')\n",
    "df_glushko.to_csv('dataset/df_glushko.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export wordnet semantic representation (n=5821)\n",
    "y_wordnet_5821 = y_wordnet[train.wn_idx,]\n",
    "print(f'Shape of selected semantic representation: {y_wordnet_5821.shape}')\n",
    "np.savez_compressed('dataset/sem_train.npz', data=y_wordnet_5821)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print(\n",
    "                'Slot {} (n = {}, unique token = {}) {} \\n'.format(\n",
    "                    slotId, t.document_count, len(t.word_index.items()),\n",
    "                    t.word_docs\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "        \n",
    "    return binData\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)\n",
    "\n",
    "\n",
    "# Merge all 3 ortho representation\n",
    "all_word = pd.concat(\n",
    "    [\n",
    "        df_train.word, df_strain.word, df_grain.word, df_taraban.word,\n",
    "        df_glushko.word\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort, df_taraban.ort, df_glushko.ort],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "splitId_strain = len(df_train)\n",
    "splitId_grain = splitId_strain + len(df_strain)\n",
    "splitId_taraban = splitId_grain + len(df_grain)\n",
    "splitId_glushko = splitId_taraban + len(df_taraban)\n",
    "\n",
    "x_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "x_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "x_grain = np.array(all_ort_bin[splitId_grain:splitId_taraban])\n",
    "x_taraban = np.array(all_ort_bin[splitId_taraban:splitId_glushko])\n",
    "x_glushko = np.array(all_ort_bin[splitId_glushko::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/x_train.npz', data=x_train)\n",
    "np.savez_compressed('dataset/x_strain.npz', data=x_strain)\n",
    "np.savez_compressed('dataset/x_grain.npz', data=x_grain)\n",
    "np.savez_compressed('dataset/x_taraban.npz', data=x_taraban)\n",
    "np.savez_compressed('dataset/x_glushko.npz', data=x_glushko)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_strain shape:', x_strain.shape)\n",
    "print('x_grain shape:', x_grain.shape)\n",
    "print('x_taraban shape:', x_taraban.shape)\n",
    "print('x_glushko shape:', x_glushko.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([len(p_col), binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        out = slotSeries.map(p_key).to_list()\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = out\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from src.data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "y_train = pho2bin_v2(train.pho, phon_key)\n",
    "y_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "y_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "y_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "y_taraban = pho2bin_v2(taraban.pho, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('dataset/y_train.npz', data=y_train)\n",
    "np.savez_compressed('dataset/y_strain.npz', data=y_strain)\n",
    "np.savez_compressed('dataset/y_large_grain.npz', data=y_large_grain)\n",
    "np.savez_compressed('dataset/y_small_grain.npz', data=y_small_grain)\n",
    "np.savez_compressed('dataset/y_taraban.npz', data=y_taraban)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_strain shape:', y_strain.shape)\n",
    "print('y_large_grain shape:', y_large_grain.shape)\n",
    "print('y_small_grain shape:', y_small_grain.shape)\n",
    "print('y_taraban shape:', y_taraban.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import get_all_pronunciations_fast as gapf\n",
    "assert all(gapf(y_train, phon_key) == df_train.pho)\n",
    "assert all(gapf(y_strain, phon_key) == df_strain.pho)\n",
    "assert all(gapf(y_large_grain, phon_key) == df_grain.pho_large)\n",
    "assert all(gapf(y_small_grain, phon_key) == df_grain.pho_small)\n",
    "assert all(gapf(y_taraban, phon_key) == df_taraban.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special format for Glushko PHO (due to multiple correct answer with different length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, pickle\n",
    "\n",
    "# Glushko pho dictionary\n",
    "pho_glushko = {\n",
    "    x: ast.literal_eval(df_glushko.loc[i, 'pho'])\n",
    "    for i, x in enumerate(df_glushko.word)\n",
    "}\n",
    "\n",
    "with open('issues/preprocessing/pho_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(pho_glushko, f)\n",
    "\n",
    "# Glushko one-hot encoded output dictionary\n",
    "y_glushko = {}\n",
    "for k, v in pho_glushko.items():\n",
    "    ys = []\n",
    "    for pho in v:\n",
    "        y = []\n",
    "        for char in pho:\n",
    "            y += phon_key[char]\n",
    "        ys.append(y)\n",
    "    y_glushko[k] = ys\n",
    "\n",
    "with open('dataset/y_glushko.pkl', 'wb') as f:\n",
    "    pickle.dump(y_glushko, f)\n",
    "\n",
    "print('y_glushko dimension: {}'.format(len(y_glushko['beed'][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('dataset/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "import data_wrangling\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(plot_f.wf, data_wrangling.get_sampling_probability(plot_f, \"log\"), label='Log')\n",
    "line2, = ax.plot(plot_f.wf, data_wrangling.get_sampling_probability(plot_f, \"hs04\"), label='HS04')\n",
    "line3, = ax.plot(plot_f.wf, data_wrangling.get_sampling_probability(plot_f, \"jay\"), label='JAY')\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "# plt.xlim((0, 200))\n",
    "# plt.ylim((0, .0006))\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
