{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pandas as pd\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create head and body sound mapping with 6kdict\n",
    "- Since column *ort* is vowel centered at position 5, we can use this column to split a word into its head and body\n",
    "- Also column *pho* is aligned with *ort*, so that  we can map between ort and pho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 6k dictionary from file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"../patterns/6ktraining_v2.dict\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"wf\"],\n",
    "    na_filter=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split head and body, create O to P mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ort_head\"] = df.ort.str.slice(0, 4).str.strip(\"_\")\n",
    "df[\"ort_b1\"] = df.ort.str.slice(4, 5).str.strip(\"_\")\n",
    "df[\"ort_b2\"] = df.ort.str.slice(5).str.strip(\"_\")\n",
    "df[\"ort_body\"] = df.ort_b1 + df.ort_b2\n",
    "df[\"pho_head\"] = df.pho.str.slice(0, 3).str.strip(\"_\")\n",
    "df[\"pho_body\"] = df.pho.str.slice(3).str.strip(\"_\")\n",
    "df[\"map_head\"] = df.ort_head + \"_\" + df.pho_head\n",
    "df[\"map_body\"] = df.ort_body + \"_\" + df.pho_body\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unique mapping pairs and count statistics for head mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_map = pd.DataFrame(df.map_head.value_counts())\n",
    "head_map.columns = [\"n\"]\n",
    "head_map[\"map\"] = head_map.index\n",
    "head_map[[\"o\", \"p\"]] = head_map.map.str.split(\"_\", expand=True)\n",
    "head_map.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create unique mapping pairs and count statistics for body mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body_map = pd.DataFrame(df.map_body.value_counts())\n",
    "body_map.columns = [\"n\"]\n",
    "body_map[\"map\"] = body_map.index\n",
    "body_map[[\"o\", \"p\"]] = body_map.map.str.split(\"_\", expand=True, n=1)\n",
    "body_map.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glushko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = pd.read_csv('../patterns/glushko_raw.csv', na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since Glushko only provides words, we need to spilt head and body by looking for first vowel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_vowel_loc(word):\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u', 'y']\n",
    "    pos = []\n",
    "    for v in vowels:\n",
    "        if word.find(v) > -1:\n",
    "            pos.append(word.find(v))\n",
    "\n",
    "    if len(pos) > 0:\n",
    "        return min(pos)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def get_head(word):\n",
    "    return (word[:first_vowel_loc(word)])\n",
    "\n",
    "\n",
    "def get_body(word):\n",
    "    return (word[first_vowel_loc(word):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk['word_head'] = list(map(get_head, gk.word))\n",
    "gk['word_body'] = list(map(get_body, gk.word))\n",
    "gk['pseudoword_head'] = list(map(get_head, gk.pseudoword))\n",
    "gk['pseudoword_body'] = list(map(get_body, gk.pseudoword))\n",
    "gk.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pronounciation using 6k dictionary mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p(unit, mapping, best=False):\n",
    "    \"\"\"\n",
    "    Get phonology from orthography\n",
    "    Since head is not vary by design, please use the most frequent mapping by setting best=True\n",
    "    unit: can be head or body\n",
    "    mapping: must have the follow columns in pandas dataframe format:\n",
    "        n: count of occurance\n",
    "        o: orthography\n",
    "        p: phonology\n",
    "    best: only return highest count p\n",
    "    \"\"\"\n",
    "    p = []\n",
    "    maxcount = 0\n",
    "    for i, h in enumerate(mapping.o):\n",
    "        if h == unit:\n",
    "            if mapping.p[i] != '':\n",
    "                if best:\n",
    "                    if mapping.n[i] > maxcount:\n",
    "                        maxcount = mapping.n[i]\n",
    "                        p.append(mapping.p[i])\n",
    "                else:\n",
    "                    p.append(mapping.p[i])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Glushko word pronounciation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpmap = df[['word', 'pho_head', 'pho_body']]\n",
    "gk_compiled = gk.merge(df_wpmap, 'left', on='word')\n",
    "gk_compiled.rename(\n",
    "    columns={\n",
    "        'pho_head': 'word_pho_head',\n",
    "        'pho_body': 'word_pho_body'\n",
    "    },\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the word \"been\" is missing in 6k dict, manually create pronounciation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_compiled.loc[gk_compiled.word == 'been']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_compiled.loc[gk_compiled.word == 'been', 'word_pho_head'] = 'b'\n",
    "gk_compiled.loc[gk_compiled.word == 'been', 'word_pho_body'] = 'in'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pseudoword (i.e., nonword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_compiled['nw_pho_head'] = [\n",
    "    get_p(gk.pseudoword_head[i], head_map, best=True) for i in gk.index\n",
    "]\n",
    "\n",
    "gk_compiled['nw_pho_body'] = [\n",
    "    get_p(gk.pseudoword_body[i], body_map) for i in gk.index\n",
    "]\n",
    "\n",
    "gk_compiled['word_pho_head_chk'] = [\n",
    "    get_p(gk.word_head[i], head_map, best=True) for i in gk.index\n",
    "]\n",
    "\n",
    "gk_compiled['word_pho_body_chk'] = [\n",
    "    get_p(gk.word_body[i], body_map) for i in gk.index\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word and nonword pronounciation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom pad_merge function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_merge(onset, body, mode):\n",
    "    \"\"\"\n",
    "    This function convert onset and body to 6kdictionary format with padding (some what complicated vowel centered...)\n",
    "    onset: head of a form\n",
    "    body: body of a form\n",
    "    mode: can be o (orthography) or p (phonology)\n",
    "    \n",
    "    This conversion logic is backward engineered from the 6kdict, with 100% correct conversion within 6kdict. \n",
    "    \n",
    "    in phonology mode, first 3 bits are reserved for onset, the other 7 bits are reserved to the body\n",
    "    \n",
    "    in orthography, first 4 bits are reserved for onset, if the 2nd character (coda) in a body is not a vowel a padding is added between 1st and 2nd bit\n",
    "    e.g., a word \"thank\", its onset (ort) is \"th\", its body is \"ank\". Since the n in ank is not vowel ('a', 'e', 'i', 'o', 'u', 'y', 'w')\n",
    "    therefore the orthograthy is \"__tha_nk______\"\n",
    "    Also, if the body start with 'ah', e.g. (bl-ah), despite \"h\" is not a vowel, no padding is inserted between 1st and 2nd bit... (apply to 3 cases in training set)\n",
    "    \"\"\"\n",
    "    if mode == 'o':\n",
    "        vowels = ['a', 'e', 'i', 'o', 'u', 'y', 'w']\n",
    "        head = onset.rjust(4, '_')\n",
    "        if len(body) > 1:\n",
    "            if (body[0:2] == 'ah') or (body[1] in vowels):\n",
    "                tail = body.ljust(10, '_')\n",
    "            else:\n",
    "                tail = body[0] + '_' + body[1::].ljust(8, '_')\n",
    "\n",
    "        else:\n",
    "            tail = body.ljust(10, '_')\n",
    "        out = head + tail\n",
    "\n",
    "    if mode == 'p':\n",
    "        out = onset.rjust(3, '_') + body.ljust(7, '_')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate pad_merge() by all training dict samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_pho = []\n",
    "for i in df.index:\n",
    "    chk_pho.append(pad_merge(df.pho_head[i], df.pho_body[i], mode='p'))\n",
    "\n",
    "print('All pho conversion pass? {}'.format(all(chk_pho == df.pho)))\n",
    "\n",
    "if not all(chk_pho == df.pho):\n",
    "    print('Failed orthographic conversions:')\n",
    "    df.loc[chk_pho != df.pho, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_ort = []\n",
    "for i in df.index:\n",
    "    chk_ort.append(pad_merge(df.ort_head[i], df.ort_body[i], mode='o'))\n",
    "\n",
    "print('All ortho conversion pass? {}'.format(all(chk_ort == df.ort)))\n",
    "\n",
    "if not all(chk_ort == df.ort):\n",
    "    print('Failed orthographic conversions:')\n",
    "    df.loc[chk_ort != df.ort, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad and Merge nonword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwp = []\n",
    "for j, pblist in enumerate(gk_compiled.nw_pho_body):\n",
    "    p = []\n",
    "    for b in pblist:\n",
    "        p.append(pad_merge(gk_compiled.nw_pho_head[j][0], b, mode='p'))\n",
    "\n",
    "    nwp.append(p)\n",
    "\n",
    "gk_compiled['nw_all_p'] = nwp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_compiled[['pseudoword', 'nw_pho_head', 'nw_pho_body', 'nw_all_p']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wo = []\n",
    "wp = []\n",
    "nwo = []\n",
    "for i in gk_compiled.index:\n",
    "    wo.append(\n",
    "        pad_merge(gk_compiled.word_head[i], gk_compiled.word_body[i], mode='o')\n",
    "    )\n",
    "\n",
    "    wp.append(\n",
    "        pad_merge(\n",
    "            gk_compiled.word_pho_head[i],\n",
    "            gk_compiled.word_pho_body[i],\n",
    "            mode='p'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    nwo.append(\n",
    "        pad_merge(\n",
    "            gk_compiled.pseudoword_head[i],\n",
    "            gk_compiled.pseudoword_body[i],\n",
    "            mode='o'\n",
    "        )\n",
    "    )\n",
    "\n",
    "gk_compiled['w_all_o'] = wo\n",
    "gk_compiled['nw_all_o'] = nwo\n",
    "gk_compiled['w_all_p'] = wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_compiled.to_csv('../patterns/glushko_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_nw = gk_compiled.loc[:, ['id', 'cond', 'pseudoword', 'nw_all_p', 'nw_all_o']]\n",
    "gk_nw.columns = ['id', 'cond', 'nonword', 'p', 'o']\n",
    "gk_nw.to_csv('../patterns/glushko_nonword.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
