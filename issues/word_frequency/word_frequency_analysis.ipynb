{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "Examine simulated data acc/sse related to word freqeuncy as a function of sigmoid. See Bry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import ipywidgets as widgets\n",
    "import wordfreq\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from BQ\n",
    "\n",
    "from google.cloud import bigquery\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/tf/secret/majestic-camp-303620-e8cb3a12037b.json\"\n",
    "client = bigquery.Client(location=\"US\", project=\"majestic-camp-303620\")\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Read data from BQ database\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        epoch,\n",
    "        sample,\n",
    "        word,\n",
    "        AVG(wf) AS wf ,  \n",
    "        AVG(acc) AS acc, \n",
    "        AVG(sse) AS sse, \n",
    "    FROM \n",
    "        slow_op_10.train\n",
    "    WHERE \n",
    "        unit_time=4.0\n",
    "    GROUP BY\n",
    "        epoch,\n",
    "        sample,\n",
    "        word;\n",
    "    \"\"\"\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "# df = load_raw_data()\n",
    "# df.to_csv(\"op10_ave_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"op10_ave_results.csv\")\n",
    "df.rename({'wf':'wf_dynamic'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get OP measure (unconditional surprisal)\n",
    "op = pd.read_csv('noam/supplementary_material.csv')\n",
    "op = op[['word', 'uncond.surprisal']]\n",
    "op.rename({'uncond.surprisal': 'op'}, axis=1, inplace=True)\n",
    "df = df.merge(op, how='left', on='word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsolete, calculate Zipf from WSJ frequency\n",
    "\n",
    "# df_train = pd.read_csv(\"../../dataset/df_train.csv\")\n",
    "# df_train = df_train[['word', 'wf', 'img']]\n",
    "# df_train.rename({'wf':'wf_wsj'}, axis=1, inplace=True)\n",
    "# df = df.merge(df_train, 'left', 'word')\n",
    "\n",
    "# df['zipf_wsj'] = np.log10((df.wf_wsj/1000) + 1)\n",
    "# df['log_wf_wsj'] = np.log10(df.wf_wsj+1)\n",
    "# df['log_wf_dynamic'] = np.log10(df.wf_dynamic+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When converting WSJ to Zipf, Zipf range is 0-3.4, which is a bit off the regular range of 0-7, perhaps WSJ is not a wpm scale in the raw data\n",
    "- To get Zipf scale, I used a [word_freq](https://github.com/LuminosoInsight/wordfreq/) library that based on [exquisite-corpus](https://github.com/LuminosoInsight/exquisite-corpus), which aggregated corpus from Wikipedia, SUBTLEX, News, Books, Web, Twitter, Reddit, and MISC content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zipf(x):\n",
    "    return wordfreq.zipf_frequency(str(x), lang='en', minimum=0)\n",
    "\n",
    "def get_wf(x):\n",
    "    return wordfreq.word_frequency(str(x), lang='en', minimum=0)\n",
    "\n",
    "df['zipf'] = df.word.apply(get_zipf)\n",
    "df['wf'] = df.word.apply(get_wf)\n",
    "\n",
    "# Peek at 1M sample\n",
    "df.loc[df.epoch==100 ,['wf_dynamic', 'wf', 'zipf', 'op']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"parsed_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot acc vs. frequency measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@widgets.interact(x_var=['wf_dynamic', \"wf\", \"zipf\"],\n",
    "                  epoch=(10,100,10), \n",
    "                  min_op=(0, 10, 0.1), \n",
    "                  max_op=(0, 10, 0.1),\n",
    "                  min_zipf=(0, 8, 0.01),\n",
    "                  max_zipf=(0, 7, 0.01),\n",
    "                  loess_bandwidth=(0,1,0.1))\n",
    "def plot_exploratory(x_var=\"zipf\", epoch=100, min_op=0, max_op=0, min_zipf=0, max_zipf=8, loess_bandwidth=0.3):\n",
    "    x = df.loc[(df.epoch==epoch) & \n",
    "               (df.op >= min_op) & \n",
    "               (df.op <= max_op) &\n",
    "               (df.zipf >= min_zipf) &\n",
    "               (df.zipf <= max_zipf)]\n",
    "\n",
    "    annotatation = f'Epoch: {epoch}; OP surprisal within: [{min_op}, {max_op}]; Zipf within: [{min_zipf}, {max_zipf}]'\n",
    "\n",
    "    if len(x) > 1000:\n",
    "        x = x.sample(1000)\n",
    "\n",
    "    p = alt.Chart(x).encode(x=x_var, y=alt.Y(\"acc\", scale=alt.Scale(domain=(0,1))),  tooltip=[\"word\", \"wf\", \"wf_dynamic\", \"zipf\", \"op\"]).mark_point()\n",
    "    l = p.transform_loess(x_var, 'acc', bandwidth=loess_bandwidth).mark_line(color='red')\n",
    "\n",
    "    return (p + l).properties(title=annotatation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fit\n",
    "\n",
    "### Fit accuracy to 2-PL IRT like equation\n",
    "\n",
    "$P(X=1|\\theta, a, b)= \\frac{e^{(a(\\theta -b))}}{1+e^{(a(\\theta -b))}}$\n",
    "\n",
    "where \n",
    "$\\theta$: frequency (zipf scale)\n",
    "\n",
    "$a$: max slope (IRT: discriminability)\n",
    "\n",
    "$b$: x-shift (IRT: difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2pl(theta, a, b):\n",
    "    \"\"\"2PL equation\"\"\"\n",
    "    x = a * (theta - b)\n",
    "    ex = np.exp(x)\n",
    "    return ex/(1+ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.loc[df.epoch==10]\n",
    "\n",
    "params, _ = curve_fit(f=f2pl, xdata=tmp.zipf, ydata=tmp.acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit sse to 4-PL IRT like equation\n",
    "\n",
    "$P(X=1|\\theta, a, b, c, d)= c + (d-c) \\frac{e^{(a(\\theta -b))}}{1+e^{(a(\\theta -b))}}$\n",
    "\n",
    "where \n",
    "$\\theta$: frequency (zipf scale)\n",
    "\n",
    "$a$: max slope (IRT: discriminability)\n",
    "\n",
    "$b$: x-shift (IRT: difficulty)\n",
    "\n",
    "$c$: lower asymptote\n",
    "\n",
    "$d$: upper asymptote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4pl(theta, a, b, c, d):\n",
    "    \"\"\"4PL equation\"\"\"\n",
    "    x = a * (theta - b)\n",
    "    ex = np.exp(x)\n",
    "    fr = ex / (1 + ex)\n",
    "    return c + (d-c) * fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('conda': virtualenv)",
   "name": "python379jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
