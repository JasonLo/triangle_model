{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "Tidy OP models results for further analysis.\n",
    "\n",
    "OP model details (10 runs): \n",
    "cleanup: 20, hidden: 100, learning rate: .001, pnoise: 0\n",
    "\n",
    "Results were pushed to GBQ slow_op_10.train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull data from BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/home/jupyter/tf/secret/majestic-camp-303620-e8cb3a12037b.json\"\n",
    "client = bigquery.Client(location=\"US\", project=\"majestic-camp-303620\")\n",
    "\n",
    "def load_raw_data():\n",
    "    \"\"\"Read data from BQ database\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        epoch,\n",
    "        sample,\n",
    "        word,\n",
    "        AVG(wf) AS wf ,  \n",
    "        AVG(acc) AS acc, \n",
    "        AVG(sse) AS sse, \n",
    "    FROM \n",
    "        slow_op_10.train\n",
    "    WHERE \n",
    "        unit_time=4.0\n",
    "    GROUP BY\n",
    "        epoch,\n",
    "        sample,\n",
    "        word;\n",
    "    \"\"\"\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    return query_job.to_dataframe()\n",
    "\n",
    "# df = load_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better name\n",
    "df = pd.read_csv(\"op10_ave_results.csv\")\n",
    "df.rename({'wf':'wf_dynamic'}, axis=1, inplace=True)\n",
    "\n",
    "# Calculate word length\n",
    "df['wlen'] = df.word.str.len()\n",
    "\n",
    "# Get OP measure (unconditional surprisal)\n",
    "op = pd.read_csv('noam/supplementary_material.csv')\n",
    "op = op[['word', 'uncond.surprisal']]\n",
    "op.rename({'uncond.surprisal': 'op'}, axis=1, inplace=True)\n",
    "df = df.merge(op, how='left', on='word')\n",
    "\n",
    "# Merge with training set to get WSJ frequency and IMG\n",
    "df_train = pd.read_csv(\"../../dataset/df_train.csv\")\n",
    "df_train = df_train[['word', 'wf', 'img']]\n",
    "df_train.rename({'wf':'wf_wsj'}, axis=1, inplace=True)\n",
    "df = df.merge(df_train, 'left', 'word')\n",
    "\n",
    "# df['zipf_wsj'] = np.log10((df.wf_wsj/1000) + 1)  # wrong scale\n",
    "df['log_wf_wsj'] = np.log10(df.wf_wsj+1)\n",
    "df['log_wf_dynamic'] = np.log10(df.wf_dynamic+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When converting WSJ to Zipf, Zipf range is 0-3.4, which is a bit off the regular range of 0-7, perhaps WSJ is not a wpm scale in the raw data\n",
    "- To get Zipf scale, I used a [word_freq](https://github.com/LuminosoInsight/wordfreq/) library that based on [exquisite-corpus](https://github.com/LuminosoInsight/exquisite-corpus), which aggregated corpus from Wikipedia, SUBTLEX, News, Books, Web, Twitter, Reddit, and MISC content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Zipf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordfreq\n",
    "\n",
    "def get_zipf(x):\n",
    "    return wordfreq.zipf_frequency(str(x), lang='en', minimum=0)\n",
    "\n",
    "def get_wf(x):\n",
    "    return wordfreq.word_frequency(str(x), lang='en', minimum=0)\n",
    "\n",
    "df['zipf_exq'] = df.word.apply(get_zipf)\n",
    "df['wf_exq'] = df.word.apply(get_wf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['epoch', 'sample', 'word', 'wlen', 'wf_wsj', 'wf_dynamic', 'log_wf_wsj', 'log_wf_dynamic', 'wf_exq', 'zipf_exq', 'op', 'img', 'acc', 'sse']]\n",
    "df.to_csv(\"parsed_df_210514.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python379jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
