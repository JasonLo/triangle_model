{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HS04 model\n",
    "\n",
    "This is a ort to sem model written under modeling.HS04 framework. The purpose of this model is to examine the \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext lab_black\n",
    "import pickle, os, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import meta, data_wrangling, modeling, metrics, evaluate\n",
    "\n",
    "# meta.limit_gpu_memory_use(7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"ort_pho\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model architechture\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "\n",
    "hidden_os_units = 500  # P2\n",
    "hidden_op_units = 100  # P2\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "\n",
    "pho_cleanup_units = 20\n",
    "sem_cleanup_units = 50\n",
    "\n",
    "pho_noise_level = 0.0  # P3\n",
    "sem_noise_level = 0.0  # P3\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "inject_error_ticks = 2\n",
    "output_ticks = 11\n",
    "\n",
    "# Training\n",
    "sample_name = \"jay\"\n",
    "\n",
    "rng_seed = 2021\n",
    "learning_rate = 0.01\n",
    "n_mil_sample = 1.0\n",
    "zero_error_radius = 0.1\n",
    "batch_size = 100\n",
    "save_freq = 10\n",
    "\n",
    "batch_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = meta.ModelConfig.from_json(\n",
    "#     os.path.join(tf_root, \"models\", code_name, \"model_config.json\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {}\n",
    "\n",
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "for v in meta.CORE_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and all supporting components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(modeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ort_pho\" \n",
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "\n",
    "sampler = data_wrangling.FastSampling(cfg, data)\n",
    "# sampler = data_wrangling.FastSampling_uniform(cfg, data)\n",
    "\n",
    "generators = {task: sampler.sample_generator(x=\"ort\", y=\"pho\")}\n",
    "optimizers = {task: tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)}\n",
    "loss_fns = {task: metrics.CustomBCE(radius=cfg.zero_error_radius)}\n",
    "\n",
    "# Train metrics\n",
    "train_losses = {task: tf.keras.metrics.Mean(\"train_loss\", dtype=tf.float32)}\n",
    "train_acc = {task: metrics.PhoAccuracy()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step for triangle model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_ort_pho(\n",
    "    x,\n",
    "    y,\n",
    "    model,\n",
    "    task,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    train_metric,\n",
    "    train_losses,\n",
    "):\n",
    "\n",
    "    train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "    train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x, training=True)\n",
    "        loss_value = loss_fn(y, y_pred)\n",
    "\n",
    "    grads = tape.gradient(loss_value, train_weights)\n",
    "    optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "    # Mean loss for Tensorboard\n",
    "    train_losses.update_state(loss_value)\n",
    "\n",
    "    # Metric for last time step (output first dimension is time ticks, from -cfg.output_ticks to end)\n",
    "    train_metric.update_state(tf.cast(y[-1], tf.float32), y_pred[-1])\n",
    "\n",
    "\n",
    "train_steps = {task: train_step_ort_pho}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()\n",
    "model.set_active_task(task)\n",
    "\n",
    "\n",
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(cfg.path[\"tensorboard_folder\"])\n",
    "\n",
    "for epoch in range(cfg.total_number_of_epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(cfg.steps_per_epoch):\n",
    "\n",
    "        x_batch_train, y_batch_train = next(generators[task])\n",
    "\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_acc[task],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # End of epoch operations\n",
    "\n",
    "    ## Log all scalar metrics (losses and metrics)and histogram (weights and biases) to tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        ### Loss\n",
    "        [\n",
    "            tf.summary.scalar(f\"loss_{x}\", train_losses[x].result(), step=epoch)\n",
    "            for x in train_losses.keys()\n",
    "        ]\n",
    "\n",
    "        ### Metrics\n",
    "        [\n",
    "            tf.summary.scalar(f\"acc_{x}\", train_acc[x].result(), step=epoch)\n",
    "            for x in train_acc.keys()\n",
    "        ]\n",
    "\n",
    "        ### Weights histogram\n",
    "        [tf.summary.histogram(f\"{x.name}\", x, step=epoch) for x in model.weights]\n",
    "\n",
    "    ## Print status\n",
    "    compute_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1} trained for {compute_time:.0f}s\")\n",
    "    print(f\"Losses: {train_losses[task].result().numpy()}\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## Save weights\n",
    "    if (epoch < 10) or ((epoch + 1) % cfg.save_freq == 0):\n",
    "        weight_path = cfg.path[\"weights_checkpoint_fstring\"].format(epoch=epoch + 1)\n",
    "        model.save_weights(weight_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "    ## Reset metric and loss\n",
    "    [train_losses[x].reset_states() for x in train_losses.keys()]\n",
    "    [train_acc[x].reset_states() for x in train_acc.keys()]\n",
    "\n",
    "# End of training ops\n",
    "# model.save(cfg.path[\"save_model_folder\"])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.testsets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(testset_name):\n",
    "\n",
    "    testset_object = evaluate.TestSet(\n",
    "        name=testset_name,\n",
    "        cfg=cfg,\n",
    "        model=model,\n",
    "        task=\"ort_pho\",\n",
    "        testitems=data.testsets[testset_name][\"item\"],\n",
    "        x_test=data.testsets[testset_name][\"ort\"],\n",
    "        y_test=data.testsets[testset_name][\"pho\"],\n",
    "    )\n",
    "\n",
    "    testset_object.eval_all()\n",
    "    return testset_object.result\n",
    "\n",
    "\n",
    "testsets = ['strain_hf_con_hi', 'strain_hf_inc_hi', 'strain_hf_con_li', 'strain_hf_inc_li', 'strain_lf_con_hi', 'strain_lf_inc_hi', 'strain_lf_con_li', 'strain_lf_inc_li']\n",
    "\n",
    "df = pd.concat(\n",
    "    [run_test(x) for x in testsets], ignore_index=True\n",
    ")\n",
    "\n",
    "df.to_csv(os.path.join(cfg.path[\"eval_folder\"], \"ort_pho.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"ort_sem_uniform_sampling.csv\")\n",
    "df = pd.read_csv(os.path.join(cfg.path[\"eval_folder\"], \"ort_pho.csv\"))\n",
    "df = df.groupby([\"epoch\", \"timetick\", \"y\", \"testset\", \"task\"]).mean().reset_index()\n",
    "\n",
    "\n",
    "def my_plot(y=\"acc\"):\n",
    "    return (\n",
    "        alt.Chart(df)\n",
    "        .mark_line(point=True)\n",
    "        .encode(x=\"timetick\", y=y, color=\"testset\", column=\"epoch:O\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plot(\"acc\").save(os.path.join(cfg.path[\"plot_folder\"], \"op_acc.html\"))\n",
    "my_plot(\"sse\").save(os.path.join(cfg.path[\"plot_folder\"], \"op_sse.html\"))\n",
    "my_plot(\"act0\").save(os.path.join(cfg.path[\"plot_folder\"], \"op_act0.html\"))\n",
    "my_plot(\"act1\").save(os.path.join(cfg.path[\"plot_folder\"], \"op_act1.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plots = my_plot(\"acc\") & my_plot(\"sse\") & my_plot(\"act0\") & my_plot(\"act1\")\n",
    "all_plots.save(os.path.join(cfg.path[\"plot_folder\"], \"op_all.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_wrangling.MyData()\n",
    "# model = modeling.HS04Model(cfg)\n",
    "# model.build()\n",
    "# model.set_active_task(\"triangle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = evaluate.EvalReading(cfg, model, data)\n",
    "# test.eval('train')\n",
    "# test.eval(\"cortese\")\n",
    "# test.eval(\"strain\")\n",
    "# test.eval(\"grain\")\n",
    "# test.eval(\"taraban\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp fix for pd float64 new data type error, read from disk as a work around\n",
    "# test.eval('train')\n",
    "# test.eval(\"cortese\")\n",
    "# test.eval(\"strain\")\n",
    "# test.eval(\"grain\")\n",
    "# test.eval(\"taraban\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic accuracy over epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ACC by OUTPUT\n",
    "# test.plot_reading_acc(test.train_mean_df).encode(y=\"mean(acc):Q\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"train_acc.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Strain ACC by OUTPUT\n",
    "# test.plot_reading_acc(test.strain_mean_df).encode(y=\"mean(acc)\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"strain_acc.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grain PHO ACC by COND\n",
    "# df = test.grain_mean_df.loc[test.grain_mean_df.y_test.isin([\"pho\"])]\n",
    "# test.plot_reading_acc(df).encode(color=\"testset\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"grain_acc.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grain ACC by RESP x COND\n",
    "# df = test.grain_mean_df.loc[\n",
    "#     test.grain_mean_df.y_test.isin([\"pho_large_grain\", \"pho_small_grain\"])\n",
    "# ]\n",
    "# test.plot_reading_acc(df).encode(color=\"testset\", strokeDash=\"y_test\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"grain_acc_by_resp.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freq x Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_selection = alt.selection_single(\n",
    "#     bind=alt.binding_range(min=10, max=150, step=10),\n",
    "#     fields=[\"epoch\"],\n",
    "#     init={\"epoch\": 150},\n",
    "#     name=\"epoch\",\n",
    "# )\n",
    "\n",
    "# timetick_selection = alt.selection_single(\n",
    "#     bind=alt.binding_range(min=0, max=cfg.n_timesteps, step=1),\n",
    "#     fields=[\"timetick\"],\n",
    "#     init={\"timetick\": cfg.n_timesteps},\n",
    "#     name=\"timetick\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Taraban\n",
    "# taraban_selected_conditions = [\n",
    "#     \"taraban_hf-exc\",\n",
    "#     \"taraban_hf-reg-inc\",\n",
    "#     \"taraban_lf-exc\",\n",
    "#     \"taraban_lf-reg-inc\",\n",
    "# ]\n",
    "\n",
    "# df = test.taraban_mean_df.copy()\n",
    "# df = df.loc[\n",
    "#     (df.testset.isin(taraban_selected_conditions))\n",
    "#     & (df.timetick >= 4)\n",
    "#     & (df.y == \"pho\")\n",
    "# ]\n",
    "\n",
    "# df[\"frequency\"] = df.testset.str.slice(8, 10)\n",
    "# df[\"regularity\"] = df.testset.str.slice(11, 14)\n",
    "\n",
    "\n",
    "# (\n",
    "#     alt.Chart(df)\n",
    "#     .mark_line()\n",
    "#     .encode(\n",
    "#         x=alt.X(\"frequency:N\", sort=\"descending\"),\n",
    "#         y=\"mean(conditional_sse):Q\",\n",
    "#         color=\"regularity:N\",\n",
    "#     )\n",
    "#     .add_selection(epoch_selection)\n",
    "#     .add_selection(timetick_selection)\n",
    "#     .transform_filter(epoch_selection)\n",
    "#     .transform_filter(timetick_selection)\n",
    "#     .properties(width=180, height=180)\n",
    "# )\n",
    "\n",
    "# # .save(os.path.join(cfg.path[\"plot_folder\"], \"replication_hs04_fig10_taraban.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Strain\n",
    "# df = test.strain_mean_df.loc[\n",
    "#     (test.strain_mean_df.timetick >= 4) & (test.strain_mean_df.y == \"pho\")\n",
    "# ]\n",
    "\n",
    "# alt.Chart(df).mark_line().encode(\n",
    "#     x=alt.X(\"frequency:N\", sort=\"descending\"),\n",
    "#     y=\"sum(sse):Q\",\n",
    "#     color=\"pho_consistency:N\",\n",
    "# ).add_selection(epoch_selection).transform_filter(epoch_selection).properties(\n",
    "#     width=180, height=180\n",
    "# ).save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"replication_hs04_fig10_strain.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate_old\n",
    "\n",
    "# glushko = evaluate_old.glushko_eval(cfg, data, model)\n",
    "# glushko.start_evaluate()\n",
    "\n",
    "# mdf = glushko.i_hist.groupby([\"epoch\", \"timestep\", \"cond\"]).mean().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# # ACC\n",
    "# alt.Chart(mdf).mark_line().encode(x=\"epoch\", y=\"acc\", color=\"cond\").add_selection(\n",
    "#     timetick_selection\n",
    "# ).transform_filter(timetick_selection).save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"glushko_acc.html\")\n",
    "# )\n",
    "\n",
    "# # SSE\n",
    "# alt.Chart(mdf).mark_line().encode(x=\"epoch\", y=\"sse\", color=\"cond\").add_selection(\n",
    "#     timetick_selection\n",
    "# ).transform_filter(timetick_selection).save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"glushko_sse.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imageability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Strain imageability\n",
    "# df = test.strain_mean_df.copy()\n",
    "# df[\"fc\"] = df.frequency + \"-\" + df.pho_consistency\n",
    "# df = df.loc[\n",
    "#     df.timetick >= 4,\n",
    "# ]\n",
    "\n",
    "# y_selection = alt.selection_single(\n",
    "#     bind=alt.binding_radio(options=[\"pho\", \"sem\"]), fields=[\"y\"], init={\"y\": \"pho\"}\n",
    "# )\n",
    "\n",
    "# epoch_selection = alt.selection_single(\n",
    "#     bind=alt.binding_range(min=10, max=150, step=10),\n",
    "#     fields=[\"epoch\"],\n",
    "#     init={\"epoch\": 150},\n",
    "#     name=\"epoch\",\n",
    "# )\n",
    "\n",
    "# # timetick_selection = alt.selection_single(\n",
    "# #     bind=alt.binding_range(min=2, max=12, step=1),\n",
    "# #     fields=[\"timetick\"],\n",
    "# #     init={\"timetick\": 12},\n",
    "# #     name=\"timetick\",\n",
    "# # )\n",
    "\n",
    "# fig11 = (\n",
    "#     alt.Chart(df)\n",
    "#     .mark_bar()\n",
    "#     .encode(\n",
    "#         column=alt.X(\"fc:N\", sort=[\"HF-CON\", \"LF-CON\", \"HF-INC\", \"LF-INC\"]),\n",
    "#         y=\"mean(conditional_sse):Q\",\n",
    "#         x=\"imageability:N\",\n",
    "#         color=\"imageability:N\",\n",
    "#     )\n",
    "#     .add_selection(epoch_selection)\n",
    "#     .add_selection(y_selection)\n",
    "#     .transform_filter(epoch_selection)\n",
    "#     .transform_filter(y_selection)\n",
    "# )\n",
    "\n",
    "# fig11.save(os.path.join(cfg.path[\"plot_folder\"], \"replication_hs04_fig11_csse.html\"))\n",
    "\n",
    "# fig11.encode(y=\"mean(sse):Q\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"replication_hs04_fig11_sse.html\")\n",
    "# )\n",
    "\n",
    "# fig11.encode(y=\"mean(acc):Q\").save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"replication_hs04_fig11_acc.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Imageability only within Strain\n",
    "# timetick_selection = alt.selection_single(\n",
    "#     bind=alt.binding_range(min=0, max=cfg.n_timesteps, step=1),\n",
    "#     fields=[\"timetick\"],\n",
    "#     init={\"timetick\": cfg.n_timesteps},\n",
    "#     name=\"timetick\",\n",
    "# )\n",
    "\n",
    "# alt.Chart(test.strain_mean_df).mark_line().encode(\n",
    "#     x=\"epoch\", y=\"mean(sse)\", color=\"imageability\", column=\"y\"\n",
    "# ).add_selection(timetick_selection).transform_filter(timetick_selection).save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"Strain_sse_img_by_output.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.plot_reading_acc(test.cortese_mean_df).encode(\n",
    "#     y=\"mean(conditional_sse)\", color=\"testset\", column=\"y\"\n",
    "# ).save(os.path.join(cfg.path[\"plot_folder\"], \"cortese_csse.html\"))\n",
    "\n",
    "# test.plot_reading_acc(test.cortese_mean_df).encode(\n",
    "#     y=\"mean(sse)\", color=\"testset\", column=\"y\"\n",
    "# ).save(os.path.join(cfg.path[\"plot_folder\"], \"cortese_sse.html\"))\n",
    "\n",
    "# test.plot_reading_acc(test.cortese_mean_df).encode(\n",
    "#     y=\"mean(acc)\", color=\"testset\", column=\"y\"\n",
    "# ).save(os.path.join(cfg.path[\"plot_folder\"], \"cortese_acc.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud compute ssh tensorflow-2-4-20210120-000018 --zone us-east4-b -- -L 6006:localhost:6006\n",
    "# !tensorboard --logdir tensorboard_log\n",
    "\n",
    "# !tensorboard dev upload --logdir tensorboard_log"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python379jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
