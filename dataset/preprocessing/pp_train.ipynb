{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing training set\n",
    "This notebook preprocesses the training set from the raw data files, including:\n",
    "\n",
    "- item, ort, pho: ../raw/Chang/6kdict\n",
    "- pho to encoded pho: ../raw/Chang/mapping\n",
    "- sem: ../raw/Chang/6ksem.mat\n",
    "- wf: ../raw/ELP/elp_wf.csv\n",
    "\n",
    "Outputs:\n",
    "- training set (in a custom training/testing set format): ../train.pkl.gz\n",
    "- human readable training set: ../train.csv\n",
    "- tokenizers used for encoding orthography: ../ort_tokenizers.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "from data_wrangling import (\n",
    "    trim_unused_slots, \n",
    "    get_duplicates, \n",
    "    ort_to_binary,\n",
    "    pho_to_binary,\n",
    "    gen_pkey,\n",
    "    save_tokenizers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training set raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jal21012/.local/lib/python3.6/site-packages/ipykernel_launcher.py:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>sampling_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>____a_________</td>\n",
       "      <td>___^______</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ace</td>\n",
       "      <td>____a_ce______</td>\n",
       "      <td>___es_____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ache</td>\n",
       "      <td>____a_che_____</td>\n",
       "      <td>___ek_____</td>\n",
       "      <td>0.161245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ached</td>\n",
       "      <td>____a_ched____</td>\n",
       "      <td>___ekt____</td>\n",
       "      <td>0.232379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aches</td>\n",
       "      <td>____a_ches____</td>\n",
       "      <td>___eks____</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6224</th>\n",
       "      <td>zoo</td>\n",
       "      <td>___zoo________</td>\n",
       "      <td>__zu______</td>\n",
       "      <td>0.527257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6225</th>\n",
       "      <td>zoom</td>\n",
       "      <td>___zoom_______</td>\n",
       "      <td>__zum_____</td>\n",
       "      <td>0.126491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>zoomed</td>\n",
       "      <td>___zoomed_____</td>\n",
       "      <td>__zumd____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>zooms</td>\n",
       "      <td>___zooms______</td>\n",
       "      <td>__zumz____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>zoos</td>\n",
       "      <td>___zoos_______</td>\n",
       "      <td>__zuz_____</td>\n",
       "      <td>0.252982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word             ort         pho  sampling_weight\n",
       "0          a  ____a_________  ___^______         1.000000\n",
       "1        ace  ____a_ce______  ___es_____         0.077460\n",
       "2       ache  ____a_che_____  ___ek_____         0.161245\n",
       "3      ached  ____a_ched____  ___ekt____         0.232379\n",
       "4      aches  ____a_ches____  ___eks____         0.141421\n",
       "...      ...             ...         ...              ...\n",
       "6224     zoo  ___zoo________  __zu______         0.527257\n",
       "6225    zoom  ___zoom_______  __zum_____         0.126491\n",
       "6226  zoomed  ___zoomed_____  __zumd____         0.077460\n",
       "6227   zooms  ___zooms______  __zumz____         0.077460\n",
       "6228    zoos  ___zoos_______  __zuz_____         0.252982\n",
       "\n",
       "[6229 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\n",
    "    \"../raw/Chang/6kdict\",\n",
    "    sep=r\"[\\t| ]\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"sampling_weight\"],\n",
    "    na_filter=False,\n",
    ")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unused slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Removing unused slots: [0, 11, 12, 13]\n",
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: [8, 9]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e64ec2f35cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ort'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_unused_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pho'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_unused_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/triangle_model/src/data_wrangling.py\u001b[0m in \u001b[0;36mtrim_unused_slots\u001b[0;34m(rep)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Removing unused slots: {list(unused_slots)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mremove_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_slots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/triangle_model/src/data_wrangling.py\u001b[0m in \u001b[0;36mremove_slots\u001b[0;34m(rep, remove_slots)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[1;32m     50\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Check that all the representation have the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mslots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train['ort'] = trim_unused_slots(train.ort)\n",
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed representation lenght safety check... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(len(x) for x in train['pho']))  # We failed length check in phoneme column, some words has 11 slots in pho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either 10 slots or 11 slots in pho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>sampling_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>clutched</td>\n",
       "      <td>_clu_tched</td>\n",
       "      <td>_kl^tCt____</td>\n",
       "      <td>0.232379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>dang</td>\n",
       "      <td>__da_ng___</td>\n",
       "      <td>__d@ng_____</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word         ort          pho  sampling_weight\n",
       "900   clutched  _clu_tched  _kl^tCt____         0.232379\n",
       "1187      dang  __da_ng___  __d@ng_____         0.050000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.pho.str.len() >= 11, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, two words has len of 11 in PHO, the extra slot should be the last one. Manually trim it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train.pho == \"_kl^tCt____\",'pho'] = \"_kl^tCt___\"\n",
    "train.loc[train.pho == \"__d@ng_____\",'pho'] = \"__d@ng____\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the slot trimming in pho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: [8, 9]\n"
     ]
    }
   ],
   "source": [
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check slot usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'_', 's'}, 1: {'T', 's', '_', 'b', 'd', 'S', 't', 'f', 'g', 'p', 'k'}, 2: {'s', 'd', 'h', 'S', 'p', 'T', 'l', 'w', 'm', 'n', '_', 'b', 'k', 'r', 'v', 'f', 'y', 'C', 'D', 't', 'z', 'g', 'J'}, 3: {'Y', 'i', 'a', 'u', 'U', 'e', 'O', 'W', 'o', '^', 'A', '@', 'I', 'E'}, 4: {'s', 'd', 'S', 'p', 'T', 'l', 'm', 'n', '_', 'b', 'Z', 'r', 'k', 'v', 'f', 'C', 'D', 't', 'z', 'g', 'J'}, 5: {'m', 'T', 's', 'n', '_', 'b', 'd', 'J', 'v', 'l', 'S', 't', 'f', 'z', 'g', 'p', 'C', 'k'}, 6: {'T', 's', '_', 'd', 't', 'z', 'J'}, 7: {'z', '_', 's', 't'}}\n"
     ]
    }
   ],
   "source": [
    "slots_usage = {slot: set([x[slot] for x in train.pho]) for slot in range(len(train.pho[0]))}\n",
    "print(slots_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the unique token can all be found in the mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 's', 'U', 'd', 'h', 'S', 'A', 'p', 'E', 'Y', 'i', 'T', 'e', 'O', 'W', 'l', '@', 'w', 'I', 'm', 'u', 'n', '_', 'b', 'Z', 'r', 'v', 'o', 'f', 'y', 'C', 'J', 'D', 't', '^', 'z', 'g', 'k'}\n",
      "38 unique token in the training set\n"
     ]
    }
   ],
   "source": [
    "unique_token = {x for slot in slots_usage.values() for x in slot}\n",
    "print(unique_token)\n",
    "print(f\"{len(unique_token)} unique token in the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_map = gen_pkey(key_file=\"../raw/Chang/mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a', 's', 'U', 'G', 'd', 'h', 'S', 'A', 'p', 'E', 'Y', 'i', 'T', 'e', 'O', '@', 'l', 'W', 'w', 'I', 'm', 'u', 'n', '_', 'b', 'Z', 'k', 'r', 'v', 'o', 'f', 'y', 'C', 'D', 't', '^', 'z', 'g', 'J'}\n",
      "39 unique tokens in the mapping file\n"
     ]
    }
   ],
   "source": [
    "token_with_mapping = set(pho_map.keys())\n",
    "print(token_with_mapping)\n",
    "print(f\"{len(token_with_mapping)} unique tokens in the mapping file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token is not used in the data, but exists in the mapping file:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'G'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"This token is not used in the data, but exists in the mapping file:\")\n",
    "set(pho_map.keys()).difference(unique_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G is not used in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token is used in the data, but not exists in the mapping file:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"This token is used in the data, but not exists in the mapping file:\")\n",
    "unique_token.difference(pho_map.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All token can be found in pho_map. It should be safe to use this mapping dict along with the current training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word frequency\n",
    "Since Chang did not provide the raw word frequency, we need to obtain it from elsewhere. Perhaps we can use SUBTLWF obtained from ELP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Freq_HAL</th>\n",
       "      <th>Log_Freq_HAL</th>\n",
       "      <th>SUBTLWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>1</td>\n",
       "      <td>gourd</td>\n",
       "      <td>5</td>\n",
       "      <td>185</td>\n",
       "      <td>5.220</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>1</td>\n",
       "      <td>fair</td>\n",
       "      <td>4</td>\n",
       "      <td>45321</td>\n",
       "      <td>10.722</td>\n",
       "      <td>94.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>1</td>\n",
       "      <td>chin</td>\n",
       "      <td>4</td>\n",
       "      <td>4747</td>\n",
       "      <td>8.465</td>\n",
       "      <td>12.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4543</th>\n",
       "      <td>1</td>\n",
       "      <td>snook</td>\n",
       "      <td>5</td>\n",
       "      <td>163</td>\n",
       "      <td>5.094</td>\n",
       "      <td>0.120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>1</td>\n",
       "      <td>gloats</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>4.043</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>1</td>\n",
       "      <td>deal</td>\n",
       "      <td>4</td>\n",
       "      <td>83766</td>\n",
       "      <td>11.336</td>\n",
       "      <td>261.370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>1</td>\n",
       "      <td>fruits</td>\n",
       "      <td>6</td>\n",
       "      <td>4378</td>\n",
       "      <td>8.384</td>\n",
       "      <td>2.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>1</td>\n",
       "      <td>botch</td>\n",
       "      <td>5</td>\n",
       "      <td>243</td>\n",
       "      <td>5.493</td>\n",
       "      <td>0.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4357</th>\n",
       "      <td>1</td>\n",
       "      <td>skimp</td>\n",
       "      <td>5</td>\n",
       "      <td>172</td>\n",
       "      <td>5.147</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1</td>\n",
       "      <td>boughs</td>\n",
       "      <td>6</td>\n",
       "      <td>35</td>\n",
       "      <td>3.555</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Occurrences    Word  Length  Freq_HAL  Log_Freq_HAL  SUBTLWF\n",
       "2005            1   gourd       5       185         5.220    0.470\n",
       "1476            1    fair       4     45321        10.722   94.750\n",
       "695             1    chin       4      4747         8.465   12.690\n",
       "4543            1   snook       5       163         5.094    0.120\n",
       "1949            1  gloats       6        57         4.043        #\n",
       "1173            1    deal       4     83766        11.336  261.370\n",
       "1798            1  fruits       6      4378         8.384    2.800\n",
       "379             1   botch       5       243         5.493    0.310\n",
       "4357            1   skimp       5       172         5.147    0.250\n",
       "382             1  boughs       6        35         3.555    0.350"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf = pd.read_csv('../raw/ELP/elp_wf.csv', index_col=None, na_filter='#', thousands=',')\n",
    "df_wf.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].str.replace(',', '')\n",
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].str.replace('#', '0')\n",
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.000000\n",
       "1       0.077460\n",
       "2       0.161245\n",
       "3       0.232379\n",
       "4       0.141421\n",
       "          ...   \n",
       "6224    0.527257\n",
       "6225    0.126491\n",
       "6226    0.077460\n",
       "6227    0.077460\n",
       "6228    0.252982\n",
       "Name: sampling_weight, Length: 6229, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_for_merge = df_wf[['Word', 'SUBTLWF']].rename(columns={'Word': 'word', 'SUBTLWF': 'wf'})\n",
    "train = train.merge(wf_for_merge, on='word', how='left')\n",
    "train.pop('sampling_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also replace missing with 0 after merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['wf'] = train.wf.apply(lambda x: x if x >= 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check semantics (Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'numFeature', 'semantic_vector', 'maxFeature', 'tarWord'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_repr = loadmat('../raw/Chang/6ksem.mat')\n",
    "wn_repr.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safety check: Is 6ksem.mat is sorted in the same order as 6kdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_word_seq = [str(wn_repr['tarWord'][0, x][0]) for x in range(6229)]\n",
    "assert wn_word_seq == train.word.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract semantic vector from wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6229, 2446)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem = wn_repr['semantic_vector'].astype(float)\n",
    "sem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because word frequency is counted by each word (ort form), but there are some homographs in the dataset, we need to adjust the word frequency to avoid over-sampling in the homograph case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 homographs in the training set\n"
     ]
    }
   ],
   "source": [
    "homographs = get_duplicates(train, 'ort')\n",
    "print(f\"There are {len(homographs)} homographs in the training set\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a homograph example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word         ort       pho      wf\n",
       "4496  shut  _shu_t____  __S^t___  263.82\n",
       "4497  shut  _shu_t____  __S^t___  263.82"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.ort=='_shu_t____']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, proceed to adjusting word frequency to avoid over sampling from the duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wf(row):\n",
    "    \"\"\"Adjust word frequencies by the number of occurance.\"\"\"\n",
    "    if row.ort in homographs.keys():\n",
    "        return(row.wf / homographs[row.ort])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['adjusted_wf'] = train.apply(adjust_wf, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some examples to make sure it is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "      <th>adjusted_wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>man</td>\n",
       "      <td>__ma_n____</td>\n",
       "      <td>__m@n___</td>\n",
       "      <td>1845.75</td>\n",
       "      <td>1845.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "      <td>131.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "      <td>131.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>swim</td>\n",
       "      <td>_swi_m____</td>\n",
       "      <td>_swIm___</td>\n",
       "      <td>31.80</td>\n",
       "      <td>31.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word         ort       pho       wf  adjusted_wf\n",
       "3014   man  __ma_n____  __m@n___  1845.75      1845.75\n",
       "4496  shut  _shu_t____  __S^t___   263.82       131.91\n",
       "4497  shut  _shu_t____  __S^t___   263.82       131.91\n",
       "5265  swim  _swi_m____  _swIm___    31.80        31.80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.word.isin(['shut', 'man', 'swim'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a human readable training set csv file for convienence reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../train.csv')  # Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: defaultdict(<class 'int'>, {'_': 6004, 'c': 4, 'p': 3, 's': 178, 't': 40})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 3772, 'b': 192, 'c': 393, 'h': 71, 'd': 80, 'f': 168, 'g': 174, 'k': 37, 'p': 191, 'q': 78, 'r': 5, 's': 735, 't': 236, 'w': 97})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 210, 'b': 305, 'l': 795, 'r': 1090, 'c': 305, 'h': 672, 'z': 24, 'd': 224, 'w': 305, 'f': 212, 'g': 174, 'n': 214, 'j': 97, 'k': 86, 'm': 289, 'p': 378, 's': 235, 'u': 83, 't': 391, 'v': 79, 'y': 61})\n",
      "Token count: defaultdict(<class 'int'>, {'a': 1663, 'e': 1145, 'i': 1130, 'o': 1493, 'u': 737, 'y': 61})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 4374, 'i': 274, 'u': 241, 'w': 237, 'y': 89, 'a': 434, 'e': 353, 'h': 3, 'o': 224})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 239, 'c': 304, 'd': 347, 'f': 158, 'g': 270, 'l': 693, 'm': 377, 'r': 791, 's': 594, 'n': 915, 'p': 357, 't': 565, 'e': 35, 'x': 35, 'b': 168, 'z': 51, 'k': 187, 'u': 4, 'v': 126, 'q': 7, 'h': 4, 'w': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 1553, 'e': 1118, 'h': 303, 't': 520, 'd': 215, 's': 936, 'z': 11, 'l': 209, 'm': 64, 'p': 200, 'g': 180, 'c': 176, 'k': 439, 'b': 59, 'n': 83, 'f': 79, 'q': 8, 'r': 13, 'u': 18, 'x': 7, 'v': 37, 'a': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 4196, 'e': 522, 's': 1086, 'd': 168, 'h': 174, 't': 70, 'u': 10, 'n': 1, 'z': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 5806, 'd': 286, 's': 89, 't': 1, 'e': 40, 'h': 6, 'g': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 6194, 'd': 27, 's': 7, 'e': 1})\n"
     ]
    }
   ],
   "source": [
    "ort_np, ort_tokenizers = ort_to_binary(train.ort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save orthographic tokenizer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenizers(ort_tokenizers, \"../tokenizer/ort_tokenizers.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_package = {\n",
    "    \"id\": train.word.index.tolist(),\n",
    "    \"item\": train.word.tolist(),\n",
    "    \"wf\": train.adjusted_wf.tolist(),\n",
    "    \"ort\": tf.constant(ort_np, dtype=tf.float32),\n",
    "    \"pho\": tf.constant(pho_to_binary(train.pho, mapping=pho_map), dtype=tf.float32),\n",
    "    \"sem\": tf.constant(sem, dtype=tf.float32),\n",
    "    \"graphem\": train.ort.tolist(),  # 10 slots, vowel at slot 3 (0-indexed)\n",
    "    \"phoneme\": train.pho.tolist(),  # 8 slots, vowel at slot 3 (0-indexed)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify dimensions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6229\n",
      "6229\n",
      "6229\n",
      "(6229, 119)\n",
      "(6229, 200)\n",
      "(6229, 2446)\n",
      "6229\n",
      "6229\n"
     ]
    }
   ],
   "source": [
    "print(len(train_package[\"id\"]))\n",
    "print(len(train_package[\"item\"]))\n",
    "print(len(train_package[\"wf\"]))\n",
    "print(train_package[\"ort\"].shape)\n",
    "print(train_package[\"pho\"].shape)\n",
    "print(train_package[\"sem\"].shape)\n",
    "print(len(train_package[\"graphem\"]))\n",
    "print(len(train_package[\"phoneme\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_package, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
