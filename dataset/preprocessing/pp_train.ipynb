{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing training set\n",
    "This notebook preprocesses the training set from the raw data files, including:\n",
    "\n",
    "- item, ort, pho: ../raw/Chang/6kdict\n",
    "- pho to encoded pho: ../raw/Chang/mapping\n",
    "- sem: ../raw/Chang/6ksem.mat\n",
    "- wf: ../raw/ELP/elp_wf.csv\n",
    "\n",
    "Outputs:\n",
    "- training set (in a custom training/testing set format): ../train.pkl.gz\n",
    "- human readable training set: ../train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, gzip\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.io import loadmat\n",
    "from data_wrangling import (\n",
    "    trim_unused_slots, \n",
    "    get_duplicates, \n",
    "    ort_to_binary,\n",
    "    pho_to_binary,\n",
    "    gen_pkey\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training set raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>sampling_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>____a_________</td>\n",
       "      <td>___^______</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ace</td>\n",
       "      <td>____a_ce______</td>\n",
       "      <td>___es_____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ache</td>\n",
       "      <td>____a_che_____</td>\n",
       "      <td>___ek_____</td>\n",
       "      <td>0.161245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ached</td>\n",
       "      <td>____a_ched____</td>\n",
       "      <td>___ekt____</td>\n",
       "      <td>0.232379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aches</td>\n",
       "      <td>____a_ches____</td>\n",
       "      <td>___eks____</td>\n",
       "      <td>0.141421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6224</th>\n",
       "      <td>zoo</td>\n",
       "      <td>___zoo________</td>\n",
       "      <td>__zu______</td>\n",
       "      <td>0.527257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6225</th>\n",
       "      <td>zoom</td>\n",
       "      <td>___zoom_______</td>\n",
       "      <td>__zum_____</td>\n",
       "      <td>0.126491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6226</th>\n",
       "      <td>zoomed</td>\n",
       "      <td>___zoomed_____</td>\n",
       "      <td>__zumd____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>zooms</td>\n",
       "      <td>___zooms______</td>\n",
       "      <td>__zumz____</td>\n",
       "      <td>0.077460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>zoos</td>\n",
       "      <td>___zoos_______</td>\n",
       "      <td>__zuz_____</td>\n",
       "      <td>0.252982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6229 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word             ort         pho  sampling_weight\n",
       "0          a  ____a_________  ___^______         1.000000\n",
       "1        ace  ____a_ce______  ___es_____         0.077460\n",
       "2       ache  ____a_che_____  ___ek_____         0.161245\n",
       "3      ached  ____a_ched____  ___ekt____         0.232379\n",
       "4      aches  ____a_ches____  ___eks____         0.141421\n",
       "...      ...             ...         ...              ...\n",
       "6224     zoo  ___zoo________  __zu______         0.527257\n",
       "6225    zoom  ___zoom_______  __zum_____         0.126491\n",
       "6226  zoomed  ___zoomed_____  __zumd____         0.077460\n",
       "6227   zooms  ___zooms______  __zumz____         0.077460\n",
       "6228    zoos  ___zoos_______  __zuz_____         0.252982\n",
       "\n",
       "[6229 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\n",
    "    \"../raw/Chang/6kdict\",\n",
    "    sep=r\"[\\t| ]\",\n",
    "    header=None,\n",
    "    names=[\"word\", \"ort\", \"pho\", \"sampling_weight\"],\n",
    "    na_filter=False,\n",
    ")\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unused slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Removing unused slots: [0, 11, 12, 13]\n",
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: [8, 9]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1360/235874300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ort'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_unused_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pho'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrim_unused_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/triangle_model/src/data_wrangling.py\u001b[0m in \u001b[0;36mtrim_unused_slots\u001b[0;34m(rep)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Removing unused slots: {list(unused_slots)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mremove_slots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_slots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/triangle_model/src/data_wrangling.py\u001b[0m in \u001b[0;36mremove_slots\u001b[0;34m(rep, remove_slots)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \"\"\"\n\u001b[1;32m     50\u001b[0m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Check that all the representation have the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mslots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train['ort'] = trim_unused_slots(train.ort)\n",
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Failed representation lenght safety check... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10, 11}\n"
     ]
    }
   ],
   "source": [
    "print(set(len(x) for x in train['pho']))  # We failed length check in phoneme column, some words has 11 slots in pho"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either 10 slots or 11 slots in pho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>sampling_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>clutched</td>\n",
       "      <td>_clu_tched</td>\n",
       "      <td>_kl^tCt____</td>\n",
       "      <td>0.232379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187</th>\n",
       "      <td>dang</td>\n",
       "      <td>__da_ng___</td>\n",
       "      <td>__d@ng_____</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word         ort          pho  sampling_weight\n",
       "900   clutched  _clu_tched  _kl^tCt____         0.232379\n",
       "1187      dang  __da_ng___  __d@ng_____         0.050000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.pho.str.len() >= 11, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, two words has len of 11 in PHO, the extra slot should be the last one. Manually trim it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[train.pho == \"_kl^tCt____\",'pho'] = \"_kl^tCt___\"\n",
    "train.loc[train.pho == \"__d@ng_____\",'pho'] = \"__d@ng____\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redo the slot trimming in pho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have these slots: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Removing unused slots: [8, 9]\n"
     ]
    }
   ],
   "source": [
    "train['pho'] = trim_unused_slots(train.pho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check slot usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'_', 's'}, 1: {'S', 'p', 'k', 't', 'f', '_', 'b', 'g', 'T', 's', 'd'}, 2: {'f', 'g', 'b', 'z', 'm', 'C', 'n', 't', 'r', '_', 'w', 's', 'y', 'd', 'l', 'S', 'p', 'T', 'v', 'h', 'k', 'D', 'J'}, 3: {'o', 'I', 'e', 'u', 'a', 'U', 'W', 'i', 'O', '@', 'Y', '^', 'A', 'E'}, 4: {'f', 'g', 'b', 'z', 'Z', 'm', 'C', 'n', 't', 'r', '_', 's', 'd', 'l', 'S', 'p', 'T', 'v', 'D', 'k', 'J'}, 5: {'m', 'C', 'S', 'p', 'n', 't', 'k', 'f', 'v', 'J', '_', 'g', 'b', 'T', 's', 'l', 'd', 'z'}, 6: {'t', 'J', '_', 'T', 's', 'd', 'z'}, 7: {'t', '_', 'z', 's'}}\n"
     ]
    }
   ],
   "source": [
    "slots_usage = {slot: set([x[slot] for x in train.pho]) for slot in range(len(train.pho[0]))}\n",
    "print(slots_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the unique token can all be found in the mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f', 'i', 'g', 'b', '@', 'z', 'Y', 'E', 'm', 'C', 'Z', 'e', 'U', 'n', 't', 'r', '_', 'w', 's', 'y', 'd', 'l', 'S', 'o', 'I', 'p', 'u', 'a', 'O', 'T', '^', 'A', 'v', 'h', 'k', 'D', 'W', 'J'}\n",
      "38 unique token in the training set\n"
     ]
    }
   ],
   "source": [
    "unique_token = {x for slot in slots_usage.values() for x in slot}\n",
    "print(unique_token)\n",
    "print(f\"{len(unique_token)} unique token in the training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_map = gen_pkey(key_file=\"../raw/Chang/mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f', 'i', 'g', 'b', '@', 'z', 'E', 'Z', 'm', 'C', 'Y', 'e', 'U', 'n', 't', 'r', '_', 's', 'w', 'y', 'd', 'l', 'S', 'o', 'I', 'p', 'a', 'u', 'O', 'T', '^', 'A', 'v', 'D', 'k', 'h', 'W', 'J', 'G'}\n",
      "39 unique tokens in the mapping file\n"
     ]
    }
   ],
   "source": [
    "token_with_mapping = set(pho_map.keys())\n",
    "print(token_with_mapping)\n",
    "print(f\"{len(token_with_mapping)} unique tokens in the mapping file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token is not used in the data, but exists in the mapping file:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'G'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"This token is not used in the data, but exists in the mapping file:\")\n",
    "set(pho_map.keys()).difference(unique_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "G is not used in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This token is used in the data, but not exists in the mapping file:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"This token is used in the data, but not exists in the mapping file:\")\n",
    "unique_token.difference(pho_map.keys()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All token can be found in pho_map. It should be safe to use this mapping dict along with the current training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get word frequency\n",
    "Since Chang did not provide the raw word frequency, we need to obtain it from elsewhere. Perhaps we can use SUBTLWF obtained from ELP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurrences</th>\n",
       "      <th>Word</th>\n",
       "      <th>Length</th>\n",
       "      <th>Freq_HAL</th>\n",
       "      <th>Log_Freq_HAL</th>\n",
       "      <th>SUBTLWF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3699</th>\n",
       "      <td>1</td>\n",
       "      <td>queer</td>\n",
       "      <td>5</td>\n",
       "      <td>2459</td>\n",
       "      <td>7.808</td>\n",
       "      <td>5.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3180</th>\n",
       "      <td>1</td>\n",
       "      <td>notes</td>\n",
       "      <td>5</td>\n",
       "      <td>42861</td>\n",
       "      <td>10.666</td>\n",
       "      <td>24.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>as</td>\n",
       "      <td>2</td>\n",
       "      <td>2642511</td>\n",
       "      <td>14.787</td>\n",
       "      <td>2,217.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4382</th>\n",
       "      <td>1</td>\n",
       "      <td>slag</td>\n",
       "      <td>4</td>\n",
       "      <td>579</td>\n",
       "      <td>6.361</td>\n",
       "      <td>0.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>4</td>\n",
       "      <td>5262331</td>\n",
       "      <td>15.476</td>\n",
       "      <td>14,111.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5680</th>\n",
       "      <td>1</td>\n",
       "      <td>weed</td>\n",
       "      <td>4</td>\n",
       "      <td>3010</td>\n",
       "      <td>8.010</td>\n",
       "      <td>11.760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>1</td>\n",
       "      <td>fro</td>\n",
       "      <td>3</td>\n",
       "      <td>1163</td>\n",
       "      <td>7.059</td>\n",
       "      <td>1.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2547</th>\n",
       "      <td>1</td>\n",
       "      <td>kilns</td>\n",
       "      <td>5</td>\n",
       "      <td>77</td>\n",
       "      <td>4.344</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>1</td>\n",
       "      <td>paid</td>\n",
       "      <td>4</td>\n",
       "      <td>50044</td>\n",
       "      <td>10.821</td>\n",
       "      <td>85.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>1</td>\n",
       "      <td>clone</td>\n",
       "      <td>5</td>\n",
       "      <td>62986</td>\n",
       "      <td>11.051</td>\n",
       "      <td>2.510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Occurrences   Word  Length  Freq_HAL  Log_Freq_HAL     SUBTLWF\n",
       "3699            1  queer       5      2459         7.808       5.800\n",
       "3180            1  notes       5     42861        10.666      24.610\n",
       "57              1     as       2   2642511        14.787   2,217.020\n",
       "4382            1   slag       4       579         6.361       0.760\n",
       "5168            1   that       4   5262331        15.476  14,111.310\n",
       "5680            1   weed       4      3010         8.010      11.760\n",
       "1779            1    fro       3      1163         7.059       1.140\n",
       "2547            1  kilns       5        77         4.344           #\n",
       "3258            1   paid       4     50044        10.821      85.670\n",
       "836             1  clone       5     62986        11.051       2.510"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wf = pd.read_csv('../raw/ELP/elp_wf.csv', index_col=None, na_filter='#', thousands=',')\n",
    "df_wf.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].str.replace(',', '')\n",
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].str.replace('#', '0')\n",
    "df_wf['SUBTLWF'] = df_wf['SUBTLWF'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.000000\n",
       "1       0.077460\n",
       "2       0.161245\n",
       "3       0.232379\n",
       "4       0.141421\n",
       "          ...   \n",
       "6224    0.527257\n",
       "6225    0.126491\n",
       "6226    0.077460\n",
       "6227    0.077460\n",
       "6228    0.252982\n",
       "Name: sampling_weight, Length: 6229, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_for_merge = df_wf[['Word', 'SUBTLWF']].rename(columns={'Word': 'word', 'SUBTLWF': 'wf'})\n",
    "train = train.merge(wf_for_merge, on='word', how='left')\n",
    "train.pop('sampling_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check semantics (Wordnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'numFeature', 'semantic_vector', 'maxFeature', 'tarWord'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn_repr = loadmat('../raw/Chang/6ksem.mat')\n",
    "wn_repr.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safety check: Is 6ksem.mat is sorted in the same order as 6kdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_word_seq = [str(wn_repr['tarWord'][0, x][0]) for x in range(6229)]\n",
    "assert wn_word_seq == train.word.tolist() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract semantic vector from wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6229, 2446)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sem = wn_repr['semantic_vector'].astype(float)\n",
    "sem.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because word frequency is counted by each word (ort form), but there are some homographs in the dataset, we need to adjust the word frequency to avoid over-sampling in the homograph case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 39 homographs in the training set\n"
     ]
    }
   ],
   "source": [
    "homographs = get_duplicates(train, 'ort')\n",
    "print(f\"There are {len(homographs)} homographs in the training set\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a homograph example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word         ort       pho      wf\n",
       "4496  shut  _shu_t____  __S^t___  263.82\n",
       "4497  shut  _shu_t____  __S^t___  263.82"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.ort=='_shu_t____']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, proceed to adjusting word frequency to avoid over sampling from the duplicated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_wf(row):\n",
    "    \"\"\"Adjust word frequencies by the number of occurance.\"\"\"\n",
    "    if row.ort in homographs.keys():\n",
    "        return(row.wf / homographs[row.ort])\n",
    "    else:\n",
    "        return(row.wf)\n",
    "        \n",
    "train['adjusted_wf'] = train.apply(adjust_wf, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some examples to make sure it is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>ort</th>\n",
       "      <th>pho</th>\n",
       "      <th>wf</th>\n",
       "      <th>adjusted_wf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3014</th>\n",
       "      <td>man</td>\n",
       "      <td>__ma_n____</td>\n",
       "      <td>__m@n___</td>\n",
       "      <td>1845.75</td>\n",
       "      <td>1845.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "      <td>131.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4497</th>\n",
       "      <td>shut</td>\n",
       "      <td>_shu_t____</td>\n",
       "      <td>__S^t___</td>\n",
       "      <td>263.82</td>\n",
       "      <td>131.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5265</th>\n",
       "      <td>swim</td>\n",
       "      <td>_swi_m____</td>\n",
       "      <td>_swIm___</td>\n",
       "      <td>31.80</td>\n",
       "      <td>31.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word         ort       pho       wf  adjusted_wf\n",
       "3014   man  __ma_n____  __m@n___  1845.75      1845.75\n",
       "4496  shut  _shu_t____  __S^t___   263.82       131.91\n",
       "4497  shut  _shu_t____  __S^t___   263.82       131.91\n",
       "5265  swim  _swi_m____  _swIm___    31.80        31.80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[train.word.isin(['shut', 'man', 'swim'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a human readable training set csv file for convienence reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('../train.csv')  # Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: defaultdict(<class 'int'>, {'_': 6004, 'c': 4, 'p': 3, 's': 178, 't': 40})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 3772, 'b': 192, 'c': 393, 'h': 71, 'd': 80, 'f': 168, 'g': 174, 'k': 37, 'p': 191, 'q': 78, 'r': 5, 's': 735, 't': 236, 'w': 97})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 210, 'b': 305, 'l': 795, 'r': 1090, 'c': 305, 'h': 672, 'z': 24, 'd': 224, 'w': 305, 'f': 212, 'g': 174, 'n': 214, 'j': 97, 'k': 86, 'm': 289, 'p': 378, 's': 235, 'u': 83, 't': 391, 'v': 79, 'y': 61})\n",
      "Token count: defaultdict(<class 'int'>, {'a': 1663, 'e': 1145, 'i': 1130, 'o': 1493, 'u': 737, 'y': 61})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 4374, 'i': 274, 'u': 241, 'w': 237, 'y': 89, 'a': 434, 'e': 353, 'h': 3, 'o': 224})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 239, 'c': 304, 'd': 347, 'f': 158, 'g': 270, 'l': 693, 'm': 377, 'r': 791, 's': 594, 'n': 915, 'p': 357, 't': 565, 'e': 35, 'x': 35, 'b': 168, 'z': 51, 'k': 187, 'u': 4, 'v': 126, 'q': 7, 'h': 4, 'w': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 1553, 'e': 1118, 'h': 303, 't': 520, 'd': 215, 's': 936, 'z': 11, 'l': 209, 'm': 64, 'p': 200, 'g': 180, 'c': 176, 'k': 439, 'b': 59, 'n': 83, 'f': 79, 'q': 8, 'r': 13, 'u': 18, 'x': 7, 'v': 37, 'a': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 4196, 'e': 522, 's': 1086, 'd': 168, 'h': 174, 't': 70, 'u': 10, 'n': 1, 'z': 2})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 5806, 'd': 286, 's': 89, 't': 1, 'e': 40, 'h': 6, 'g': 1})\n",
      "Token count: defaultdict(<class 'int'>, {'_': 6194, 'd': 27, 's': 7, 'e': 1})\n"
     ]
    }
   ],
   "source": [
    "ort_np, ort_tokenizers = ort_to_binary(train.ort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save orthographic tokenizer for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_jsons = [t.to_json() for t in ort_tokenizers]\n",
    "with open(f\"../tokenizer/ort_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer_jsons, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Package dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-14 21:49:11.359027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.365734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.365969: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.366683: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-14 21:49:11.367904: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.368109: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.368276: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.868090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.868731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.868747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-12-14 21:49:11.868920: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-12-14 21:49:11.868996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3481 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_package = {\n",
    "    \"id\": train.word.index.tolist(),\n",
    "    \"item\": train.word.tolist(),\n",
    "    \"wf\": train.adjusted_wf.tolist(),\n",
    "    \"ort\": tf.constant(ort_np, dtype=tf.float32),\n",
    "    \"pho\": tf.constant(pho_to_binary(train.pho, mapping=pho_map), dtype=tf.float32),\n",
    "    \"sem\": tf.constant(sem, dtype=tf.float32),\n",
    "    \"graphem\": train.ort.tolist(),  # 10 slots, vowel at slot 3 (0-indexed)\n",
    "    \"phoneme\": train.pho.tolist(),  # 8 slots, vowel at slot 3 (0-indexed)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify dimensions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6229\n",
      "6229\n",
      "6229\n",
      "(6229, 119)\n",
      "(6229, 200)\n",
      "(6229, 2446)\n",
      "6229\n",
      "6229\n"
     ]
    }
   ],
   "source": [
    "print(len(train_package[\"id\"]))\n",
    "print(len(train_package[\"item\"]))\n",
    "print(len(train_package[\"wf\"]))\n",
    "print(train_package[\"ort\"].shape)\n",
    "print(train_package[\"pho\"].shape)\n",
    "print(train_package[\"sem\"].shape)\n",
    "print(len(train_package[\"graphem\"]))\n",
    "print(len(train_package[\"phoneme\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../train.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump(train_package, f)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-4.mnightly-2021-02-02-debian-10-test",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-02-02-debian-10-test"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
