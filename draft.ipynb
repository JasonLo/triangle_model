{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declarator for counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(func, *args, **kwargs):\n",
    "    def wrapper():\n",
    "        counter = {}\n",
    "        words = func(*args, **kwargs)\n",
    "        for word in words:\n",
    "            if word in counter:\n",
    "                counter[word] += 1\n",
    "            else:\n",
    "                counter[word] = 1\n",
    "        return func(*args, **kwargs), counter\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per parameter effective learning rate in ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The weight update step in ADAM:\n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - lr_t * m_t / (\\sqrt{v_t} + \\epsilon)$\n",
    "\n",
    "where\n",
    "\n",
    "$lr_t = \\mathrm{learning\\_rate} * \\sqrt{1 - \\beta_2^t} / (1 - \\beta_1^t)$\n",
    "\n",
    "$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g$\n",
    "\n",
    "$v_t = \\beta_2 * v_{t-1} + (1 - \\beta_2) * g^2$\n",
    "\n",
    "The question is: will Adam affects the effective learning rate in P and S differently. In the other words, we want to compare $m_t / (\\sqrt{v_t} + \\epsilon)$ in PHO and SEM are similar or not.\n",
    "\n",
    "Let's start from OP and OS only runs. (since it is simpler, one weight only associated to one optimizer instead of n optimizers...)\n",
    "\n",
    "- PHO (during OP) weight and biases are w_hop_oh, bias_hop, w_hop_hp, w_pc, bias_cpp, w_cp, bias_p\n",
    "- SEM (during OS) weight and biases are W_hos_oh, bias_hos, w_hos_hs, w_sc, bias_css, w_cs, bias_s\n",
    "\n",
    "Steps:\n",
    "1. Get all $m_t / (\\sqrt{v_t} + \\epsilon)$ in each weights\n",
    "2. Average? Consider sparsity...\n",
    "3. Compare  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import meta, modeling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from helper import stitch_fig\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer content in OP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ELR:\n",
    "    \"\"\"Examine the effective learning rate scaling factor in ADAM.\"\"\"\n",
    "\n",
    "    def __init__(self, batch_name, code_name, tf_root='/home/jupyter/triangle_model'):\n",
    "        self.cfg = meta.Config.from_json(os.path.join('models', batch_name, code_name, 'model_config.json'))\n",
    "        self.cfg.tf_root = tf_root\n",
    "        self.model = modeling.MyModel(self.cfg)\n",
    "        self.model.build()\n",
    "        self.optimizers = {task: tf.keras.optimizers.Adam(learning_rate=self.cfg.learning_rate) for task in self.cfg.task_names}\n",
    "        self.ckpt = tf.train.Checkpoint(model=self.model, optimizers=self.optimizers)\n",
    "\n",
    "    def restore(self, epoch=int):\n",
    "        \"\"\"Restore model and optimizers from checkpoint.\"\"\"\n",
    "        self.ckpt.restore(os.path.join(self.cfg.weight_folder, f'epoch-{epoch}')).expect_partial()\n",
    "\n",
    "    @staticmethod\n",
    "    def cal_elr(m, v, e):\n",
    "        \"\"\"Calculate the effective learning rate scaling factor.\"\"\"\n",
    "        return m / (tf.sqrt(v) + e)\n",
    "\n",
    "    def get_elr(self, task:str, weight_name:str):\n",
    "        \"\"\"Calculate the effective learning rate scaling factor from an optimizer\"\"\"\n",
    "        m = [x for x in self.optimizers[task].weights if x.name == f\"{weight_name}/m:0\"]\n",
    "        v = [x for x in self.optimizers[task].weights if x.name == f\"{weight_name}/v:0\"]\n",
    "        e = self.optimizers[task].epsilon\n",
    "        return self.cal_elr(m, v, e).numpy().squeeze()\n",
    "\n",
    "    @staticmethod\n",
    "    def _cal_variance(x):\n",
    "        return x.flatten().var()\n",
    "\n",
    "    def variance(self, task:str):\n",
    "        \"\"\"Calculate the variance of effective learning rate scaling factor in each weight\"\"\"\n",
    "        return {k: self._cal_variance(self.get_elr(task, k)) for k in modeling.WEIGHTS_AND_BIASES[task]}\n",
    "\n",
    "    def mean(self, task:str):\n",
    "        \"\"\"Calculate the mean of effective learning rate scaling factor in each weight\"\"\"\n",
    "        return {k: self.get_elr(task, k).mean() for k in modeling.WEIGHTS_AND_BIASES[task]}\n",
    "\n",
    "    def make_df(self, task:str, summary_function:str='variance'):\n",
    "        df = pd.DataFrame()\n",
    "        _summary_fun = self.variance if summary_function == 'variance' else self.mean\n",
    "        for i in tqdm(self.cfg.saved_epochs):\n",
    "            self.restore(epoch=i)\n",
    "            this_epoch_data = pd.DataFrame(_summary_fun(task), index=[i])\n",
    "            df = df.append(this_epoch_data)\n",
    "        return df.reset_index().melt(id_vars='index', var_name='weight', value_name=summary_function)       \n",
    "\n",
    "    def plot_elr(self, task:str, weight_name:str, ax=None):\n",
    "        \"\"\"Plot the effective learning rate scaling factor from an optimizer\"\"\"\n",
    "        elr = self.get_elr(task, weight_name).flatten()\n",
    "        sns.kdeplot(elr, label=f'{weight_name}', ax=ax)\n",
    "        plt.legend()\n",
    "        return ax\n",
    "\n",
    "    def plot_all(self, task:str, ax=None):\n",
    "        \"\"\"Plot all effective learning rate scaling factors in each weight at current loaded epoch\"\"\"\n",
    "        [self.plot_elr(task, x, ax) for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "\n",
    "    def plot_all_over_epochs(self, task:str, xlim=None):\n",
    "        \"\"\"Plot all effective learning rate scaling factors in each weight over epochs\"\"\"       \n",
    "\n",
    "        output_folder = os.path.join(self.cfg.plot_folder, task)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        for i in tqdm(self.cfg.saved_epochs):\n",
    "            self.restore(epoch=i)\n",
    "            plt.clf() # Clear figure\n",
    "            self.plot_all(task) # All weights KDE density at given epoch \n",
    "            if xlim is not None:\n",
    "                plt.xlim(xlim)\n",
    "\n",
    "            plt.title(f'Epoch {i}')\n",
    "            plt.savefig(os.path.join(output_folder, f'epoch_{i}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_pho = ELR(batch_name='task_effect', code_name='task_effect_r0027')\n",
    "df = ort_pho.make_df('ort_pho', summary_function='mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_chart = alt.Chart(df).mark_line().encode(x='index', y='mean', color='weight')\n",
    "op_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_chart.transform_loess('index', 'mean', groupby=['weight']).mark_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ort_sem = ELR(batch_name='task_effect', code_name='task_effect_r0028')\n",
    "os_df = ort_sem.make_df('ort_sem', summary_function='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_chart = alt.Chart(os_df).mark_line().encode(x='index', y='mean', color='weight')\n",
    "os_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_chart.transform_loess('index', 'mean', groupby=['weight']).mark_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.name for x in ort_pho.optimizers['ort_pho'].weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m27 = ELR('task_effect', 'task_effect_r0027')\n",
    "# m27.plot_all_over_epochs('ort_pho', xlim=(-0.1, 0.1))\n",
    "m27_pngs = [os.path.join(m27.cfg.plot_folder, 'ort_pho', f\"epoch_{x}.png\") for x in m27.cfg.saved_epochs]\n",
    "m27_stitch = stitch_fig(m27_pngs, rows=10, columns=5)\n",
    "m27_stitch.save(m27.cfg.plot_folder + '/OP_ADAM_LR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m28 = ELR('task_effect', 'task_effect_r0028')\n",
    "# m28.plot_all_over_epochs('ort_sem', xlim=(-0.1, 0.1))\n",
    "m28_pngs = [os.path.join(m28.cfg.plot_folder, 'ort_sem', f\"epoch_{x}.png\") for x in m28.cfg.saved_epochs]\n",
    "m28_stitch = stitch_fig(m28_pngs, rows=10, columns=5)\n",
    "m28_stitch.save(m28.cfg.plot_folder + '/OS_ADAM_LR.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
