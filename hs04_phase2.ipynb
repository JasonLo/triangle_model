{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HS04 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The weights that were obtained at the end of the Phase 1 model were frozen and embedded in the larger reading model. Thus, only the connections from orthography to other units were trained in Phase 2. Freezing the weights is not strictly necessary; earlier work (Harm & Seidenberg, 1997) used a process of intermixing in which comprehension trials were used along with reading trials. Weight freezing has the same effect but is simpler and less computationally burdensome to implement. Intermixing is effective and real- istic but adds substantially to network training time.\n",
    "\n",
    "- *Pretraining is necessary, and freeze in phase 2\n",
    "\n",
    "> One set of 500 hidden units mediated the mapping from these orthographic units to semantics...\n",
    "\n",
    "- *500 sem_hidden_units*\n",
    "\n",
    "> ...a second set of 100 hidden units mediated the orth-phon pathway.\n",
    "\n",
    "- *100 pho_hidden_units*\n",
    "\n",
    "> To computationally instantiate the principle that the reading system is under pressure to perform rapidly as well as accurately, we injected error into the semantic and phonological representa- tions early, from time samples 2 to 12. \n",
    "- *11 output_ticks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modeling individual differences\n",
    "- Simulating ERPs\n",
    "- Link to reliance of OP vs OS\n",
    "- Use equation to model semantic / phonetic input to P/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext lab_black\n",
    "import pickle, os, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import meta, data_wrangling, modeling, metrics, evaluate\n",
    "\n",
    "# meta.set_gpu_mem_cap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"hs04_phase2_test3\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model architechture\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "\n",
    "hidden_os_units = 500  # P2\n",
    "hidden_op_units = 100  # P2\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "\n",
    "pho_noise_level = 0.0  # P3\n",
    "sem_noise_level = 0.0  # P3\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 11\n",
    "\n",
    "# Pretraining\n",
    "pretrained_checkpoint = (\n",
    "    \"/home/jupyter/tf/models/hs04_phase1_selected_fix_attractor/weights/ep0200\"\n",
    ")\n",
    "\n",
    "# Training\n",
    "sample_name = \"hs04\"\n",
    "\n",
    "rng_seed = 2021\n",
    "learning_rate = 0.01\n",
    "n_mil_sample = 1.5\n",
    "batch_size = 100\n",
    "save_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = meta.ModelConfig.from_json(os.path.join(\"models\", code_name, \"model_config.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {}\n",
    "\n",
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "for v in meta.CORE_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and all supporting components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "\n",
    "sampler = data_wrangling.FastSampling(cfg, data)\n",
    "generators = {\"triangle\": sampler.sample_generator(x=\"ort\", y=[\"pho\", \"sem\"])}\n",
    "optimizers = {\"triangle\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)}\n",
    "loss_fns = {\"triangle\": tf.keras.losses.BinaryCrossentropy()}\n",
    "\n",
    "# Mean loss (for TensorBoard)\n",
    "train_losses = {\n",
    "    \"triangle\": tf.keras.metrics.Mean(\"train_loss_triangle\", dtype=tf.float32)\n",
    "}\n",
    "\n",
    "# Train metrics\n",
    "train_acc = {\n",
    "    \"triangle_pho\": metrics.PhoAccuracy(\"acc_triangle_pho\"),\n",
    "    \"triangle_sem\": metrics.RightSideAccuracy(\"acc_triangle_sem\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train step for triangle model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_triangle(\n",
    "    x,\n",
    "    y,\n",
    "    model,\n",
    "    task,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    train_metric_pho,\n",
    "    train_metric_sem,\n",
    "    train_losses,\n",
    "):\n",
    "\n",
    "    train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "    train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pho_pred, sem_pred = model(x, training=True)\n",
    "        loss_value_pho = loss_fn(y[0], pho_pred)\n",
    "        loss_value_sem = loss_fn(y[1], sem_pred)\n",
    "        loss_value = loss_value_pho + loss_value_sem\n",
    "\n",
    "    grads = tape.gradient(loss_value, train_weights)\n",
    "    optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "    # Mean loss for Tensorboard\n",
    "    train_losses.update_state(loss_value)\n",
    "\n",
    "    # Metric for last time step (output first dimension is time ticks, from -cfg.output_ticks to end)\n",
    "    train_metric_pho.update_state(tf.cast(y[0][-1], tf.float32), pho_pred[-1])\n",
    "    train_metric_sem.update_state(tf.cast(y[1][-1], tf.float32), sem_pred[-1])\n",
    "\n",
    "\n",
    "train_steps = {\"triangle\": train_step_triangle}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build()\n",
    "model.load_weights(pretrained_checkpoint)\n",
    "task = \"triangle\"\n",
    "model.set_active_task(task)\n",
    "\n",
    "\n",
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(cfg.path[\"tensorboard_folder\"])\n",
    "\n",
    "for epoch in range(cfg.total_number_of_epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(cfg.steps_per_epoch):\n",
    "\n",
    "        x_batch_train, y_batch_train = next(generators[task])\n",
    "\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_acc[\"triangle_pho\"],\n",
    "            train_acc[\"triangle_sem\"],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # End of epoch operations\n",
    "\n",
    "    ## Log all scalar metrics (losses and metrics)and histogram (weights and biases) to tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "\n",
    "        [\n",
    "            tf.summary.scalar(f\"loss_{x}\", train_losses[x].result(), step=epoch)\n",
    "            for x in train_losses.keys()\n",
    "        ]\n",
    "        [\n",
    "            tf.summary.scalar(f\"acc_{x}\", train_acc[x].result(), step=epoch)\n",
    "            for x in train_acc.keys()\n",
    "        ]\n",
    "        [tf.summary.histogram(f\"{x.name}\", x, step=epoch) for x in model.weights]\n",
    "\n",
    "    ## Print status\n",
    "    compute_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1} trained for {compute_time:.0f}s\")\n",
    "    print(f\"Losses: {train_losses[task].result().numpy()}\")\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## Save weights\n",
    "    if (epoch < 10) or ((epoch + 1) % 10 == 0):\n",
    "        weight_path = cfg.path[\"weights_checkpoint_fstring\"].format(epoch=epoch + 1)\n",
    "        model.save_weights(weight_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "    ## Reset metric and loss\n",
    "    [train_losses[x].reset_states() for x in train_losses.keys()]\n",
    "    [train_acc[x].reset_states() for x in train_acc.keys()]\n",
    "\n",
    "# End of training ops\n",
    "# model.save(cfg.path[\"save_model_folder\"])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(evaluate)\n",
    "importlib.reload(data_wrangling)\n",
    "data=data_wrangling.MyData()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = evaluate.EvalReading(cfg, model, data)\n",
    "# test.eval_train()\n",
    "# test.eval_strain()\n",
    "# test.eval_grain()\n",
    "test.eval_train_cortese_img()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.plot_reading_acc(test.train_cortese_img_mean_df).encode(y=\"mean(sse)\", color=\"testset\", column=\"y\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"train_cortese_img_sse_output.html\")\n",
    ")\n",
    "\n",
    "test.plot_reading_acc(test.train_cortese_img_mean_df).encode(y=\"mean(acc)\", color=\"testset\", column=\"y\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"train_cortese_img_acc_output.html\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train ACC by OUTPUT\n",
    "test.plot_reading_acc(test.train_mean_df).encode(y=\"mean(acc)\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"train_acc_output.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain ACC by OUTPUT\n",
    "test.plot_reading_acc(test.strain_mean_df).encode(y=\"mean(acc)\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"strain_acc_output.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain PHO ACC by COND\n",
    "df = test.grain_mean_df.loc[test.grain_mean_df.y_test.isin([\"pho\"])]\n",
    "test.plot_reading_acc(df).encode(color=\"testset\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"grain_pho_acc.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grain ACC by RESP x COND\n",
    "df = test.grain_mean_df.loc[\n",
    "    test.grain_mean_df.y_test.isin([\"pho_large_grain\", \"pho_small_grain\"])\n",
    "]\n",
    "test.plot_reading_acc(df).encode(color=\"testset\", strokeDash=\"y_test\").save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"grain_pho_acc_by_resp.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HS04 experiments (Fig 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = test.strain_mean_df.loc[\n",
    "    (test.strain_mean_df.epoch >= 50)\n",
    "    & (test.strain_mean_df.timetick >= 8)\n",
    "    & (test.strain_mean_df.y == \"pho\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(exp_df).mark_line().encode(\n",
    "    x=alt.X(\"frequency:N\", sort=\"descending\"),\n",
    "    y=\"sum(sse):Q\",\n",
    "    color=\"pho_consistency:N\",\n",
    ").properties(width=180, height=180)\n",
    "\n",
    "# .save(\n",
    "#     os.path.join(cfg.path[\"plot_folder\"], \"strain_acc_output.html\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df[\"fc\"] = exp_df.frequency + \"-\" + exp_df.pho_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(exp_df).mark_bar().encode(\n",
    "    column=alt.X(\"fc:N\", sort=[\"HF-CON\", \"LF-CON\", \"HF-INC\", \"LF-INC\"]),\n",
    "    y=\"mean(sse):Q\",\n",
    "    x=\"imageability:N\",\n",
    "    color=\"imageability:N\",\n",
    ").properties(width=180, height=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive on epoch and timetick\n",
    "epoch_selection = alt.selection_single(\n",
    "    bind=alt.binding_range(min=50, max=150, step=10),\n",
    "    fields=[\"epoch\"],\n",
    "    init={\"epoch\": 150},\n",
    "    name=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "timetick_selection = alt.selection_single(\n",
    "    bind=alt.binding_range(min=0, max=cfg.n_timesteps, step=1),\n",
    "    fields=[\"timetick\"],\n",
    "    init={\"timetick\": cfg.n_timesteps},\n",
    "    name=\"timetick\",\n",
    ")\n",
    "\n",
    "\n",
    "(\n",
    "    alt.Chart(exp_df)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        column=alt.X(\"fc:N\", sort=[\"HF-CON\", \"LF-CON\", \"HF-INC\", \"LF-INC\"]),\n",
    "        y=\"sum(sse):Q\",\n",
    "        x=\"imageability:N\",\n",
    "        color=\"imageability:N\",\n",
    "    )\n",
    "    .add_selection(timetick_selection)\n",
    "    .add_selection(epoch_selection)\n",
    "    .transform_filter(timetick_selection)\n",
    "    .transform_filter(epoch_selection)\n",
    ").save(os.path.join(cfg.path[\"plot_folder\"], \"interactive_strain_sse.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum across timeticks, interactive on epoch\n",
    "(\n",
    "    alt.Chart(exp_df)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        column=alt.X(\"fc:N\", sort=[\"HF-CON\", \"LF-CON\", \"HF-INC\", \"LF-INC\"]),\n",
    "        y=\"sum(sse):Q\",\n",
    "        x=\"imageability:N\",\n",
    "        color=\"imageability:N\",\n",
    "    )\n",
    "    .add_selection(epoch_selection)\n",
    "    .transform_filter(epoch_selection)\n",
    ").save(os.path.join(cfg.path[\"plot_folder\"], \"interactive_epoch_strain_sse.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local ssh to cloud tensorboard\n",
    "# gcloud compute ssh tensorflow-2-4-20210120-000018 --zone us-east4-b -- -L 6006:localhost:6006\n",
    "# !tensorboard dev upload --logdir tensorboard_log"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
