{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triangle model\n",
    "This interactive notebook runs a triangle model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run parameters \n",
    "This block is necessary for running with [papermill](https://papermill.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"usual_pretrain\"\n",
    "batch_name = None\n",
    "\n",
    "# Model configs\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 100\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "pho_noise_level = 0.0\n",
    "sem_noise_level = 0.0\n",
    "activation = \"sigmoid\"\n",
    "\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 13\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training configs\n",
    "learning_rate = 0.001\n",
    "zero_error_radius = 0.1\n",
    "save_freq = 10\n",
    "\n",
    "# Environment configs\n",
    "wf_compression = \"log\"\n",
    "wf_clip_low = 0\n",
    "wf_clip_high = 999_999_999\n",
    "\n",
    "task_names = (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\")\n",
    "tasks_ps = (0.4, 0.4, 0.1, 0.1)\n",
    "# task_names = \"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"triangle\")\n",
    "# tasks_ps = (0.0, 0.0, 0.1, 0.1, 0.5)\n",
    "\n",
    "total_sample = 1_000_000\n",
    "batch_size = 1\n",
    "rng_seed = 2021\n",
    "which_gpu = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta import split_gpu\n",
    "\n",
    "split_gpu(which_gpu=which_gpu)  # IMPORTANT: do not import TensorFlow before this line\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "import meta, data_wrangling, metrics, modeling, training\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(rng_seed)\n",
    "tf.random.set_seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "\n",
    "load_dotenv()  # Loads .env file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `Config()`, `Data()`, and `Model()`\n",
    "- `Config()` stores all the run setting in a class. `**globals()` is used to access all global variables in the parameter block.\n",
    "- `MyData()` contains all the static data sets.\n",
    "- `MyModel()` contains the triangle model implementation on TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = meta.Config.from_json(os.path.join(tf_root, \"models\", batch_name, code_name, \"model_config.json\"))   # Load from json\n",
    "cfg = meta.Config.from_dict(**globals())\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.MyModel(cfg)\n",
    "model.build()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sample generator\n",
    "- `Experience()` defines what the model is trained on. It consists of one or more `Stage()`. \n",
    "- Each `Stage()` describes what tasks are the model trained with, and how often a task is used during training. It contains one or more `Task()`. \n",
    "- Each `Task()` contains how fast the corpus is opened (a set of word that can be sampled), defaults to full open.\n",
    "- See the docstrings in each object for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Task, Stage, Experience, Sampler\n",
    "\n",
    "stages = [\n",
    "    Stage(\n",
    "        name=\"one\",\n",
    "        tasks=[Task(x) for x in cfg.task_names],\n",
    "        stage_sample=cfg.total_sample,\n",
    "        task_probability_start=cfg.tasks_ps,\n",
    "    )\n",
    "]\n",
    "\n",
    "experience = Experience(stages)\n",
    "sampler = Sampler(cfg, data, experience)\n",
    "batch_generator = sampler.generator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimizers, loss functions, metrics, and train steps\n",
    "- Since each sub-task has its own states, it must be trained with separate optimizer.\n",
    "- Regarding to the metrics, there are two types of metrics:\n",
    "    - Stateless metrics: the average metric of all batches within an epoch (semantic acc, sse)\n",
    "    - Stateful metrics: only taking last step/batch value in an epoch (pho acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {}\n",
    "loss_fns = {}\n",
    "train_losses = {}  # Mean loss (only for TensorBoard)\n",
    "train_metrics = {}\n",
    "train_steps = {}\n",
    "\n",
    "acc = {\"pho\": metrics.PhoAccuracy, \"sem\": metrics.StatelessRightSideAccuracy}\n",
    "sse = metrics.StatelessSumSquaredError\n",
    "\n",
    "for task in cfg.task_names:\n",
    "\n",
    "    # Optimizer\n",
    "    optimizers[task] = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "\n",
    "    # Loss function\n",
    "    if cfg.zero_error_radius is not None:\n",
    "        loss_fns[task] = metrics.CustomBCE(radius=cfg.zero_error_radius)\n",
    "    else:\n",
    "        loss_fns[task] = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    train_losses[task] = tf.keras.metrics.Mean(f\"train_loss_{task}\", dtype=tf.float32)\n",
    "\n",
    "    # Metrics & train steps\n",
    "    task_output = modeling.IN_OUT[task][1]\n",
    "    if task == \"triangle\":\n",
    "        train_metrics[task] = {}\n",
    "        for output in task_output:\n",
    "            train_metrics[task][output] = [\n",
    "                acc[output](f\"{task}_{output}_acc\"),\n",
    "                sse(f\"{task}_{output}_sse\"),\n",
    "            ]\n",
    "\n",
    "        train_steps[task] = training.triangle_train_step()\n",
    "\n",
    "    else:\n",
    "\n",
    "        train_metrics[task] = [acc[task_output](f\"{task}_acc\"), sse(f\"{task}_sse\")]\n",
    "        train_steps[task] = training.basic_train_step(task)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_scalar_to_tensorboard(task, step):\n",
    "#     \"\"\"Write metrics and loss to tensorboard\"\"\"\n",
    "#     loss = train_losses[task]\n",
    "#     tf.summary.scalar(loss.name, loss.result(), step=step)\n",
    "\n",
    "#     maybe_metrics = train_metrics[task]\n",
    "#     if task == \"triangle\":\n",
    "#         [\n",
    "#             tf.summary.scalar(m.name, m.result(), step=step)\n",
    "#             for metrics in maybe_metrics.values()\n",
    "#             for m in metrics\n",
    "#         ]\n",
    "#     else:\n",
    "#         [tf.summary.scalar(m.name, m.result(), step=step) for m in maybe_metrics]\n",
    "\n",
    "\n",
    "def write_weight_histogram_to_tensorboard(step):\n",
    "    \"\"\"Weight histogram\"\"\"\n",
    "    [tf.summary.histogram(f\"{x.name}\", x, step=step) for x in model.weights]\n",
    "\n",
    "\n",
    "# def reset_metrics(task):\n",
    "#     maybe_metrics = train_metrics[task]\n",
    "#     if task == \"triangle\":\n",
    "#         [m.reset_states() for metrics in maybe_metrics.values() for m in metrics]\n",
    "#     else:\n",
    "#         [m.reset_states() for m in maybe_metrics]\n",
    "\n",
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "    os.path.join(cfg.tensorboard_folder, \"train\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_ckpt = tf.train.Checkpoint(model=model)\n",
    "# pretrain_ckpt.restore(\n",
    "#     os.path.join(cfg.tf_root, \"models\", \"pretrain_3M\", \"checkpoints\", \"epoch-300\")\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create checkpoint (save) manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = tf.Variable(0, name=\"epoch\")  # Epoch counter\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    epoch=epoch,\n",
    "    model=model,\n",
    "    optimizers=optimizers,\n",
    ")\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt,\n",
    "    cfg.checkpoint_folder,\n",
    "    max_to_keep=None,  # Keep all checkpoints\n",
    "    checkpoint_name=\"epoch\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume training from latest checkpoint\n",
    "- CAUTION: Environment will no longer be identical if resume from checkpoint (Unable to put it in checkpoint for now)\n",
    "- However, resume training is not very common, so it is not a big deal for now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(f\"Restored from {ckpt_manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(total=cfg.total_number_of_epoch, desc=\"Training\")\n",
    "progress_bar.update(epoch.numpy())\n",
    "\n",
    "while epoch.numpy() < cfg.total_number_of_epoch:\n",
    "\n",
    "    # Train an epoch\n",
    "    for step in range(cfg.steps_per_epoch):\n",
    "        # Draw task, create batch\n",
    "        task, exposed_words_idx, exposed_word, x_batch_train, y_batch_train = next(\n",
    "            batch_generator\n",
    "        )\n",
    "\n",
    "        # task switching must be done outside train_step function (will crash otherwise)\n",
    "        model.set_active_task(task)\n",
    "\n",
    "        # Run a train step\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_metrics[task],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # Post epoch Ops\n",
    "    progress_bar.update(1)\n",
    "    epoch.assign_add(1)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        ## Write log to tensorboard (Once per epoch)\n",
    "        # [write_scalar_to_tensorboard(task, step=epoch) for task in cfg.task_names]\n",
    "        write_weight_histogram_to_tensorboard(step=epoch.numpy())\n",
    "\n",
    "        #     ## Reset metric and loss\n",
    "        [train_losses[x].reset_states() for x in cfg.task_names]\n",
    "    #     [reset_metrics(x) for x in cfg.task_names]\n",
    "\n",
    "    if epoch.numpy() in cfg.saved_epochs:\n",
    "        f = ckpt_manager.save(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import benchmark_hs04\n",
    "# Basic test\n",
    "benchmark_hs04.run_test1(cfg.code_name)  # Basic accuracy test only\n",
    "\n",
    "## All benchmarks\n",
    "# benchmark_hs04.main(cfg.code_name, cfg.batch_name)  \n",
    "\n",
    "\n",
    "## Full training set test\n",
    "# import evaluate\n",
    "# test = evaluate.Test(cfg)\n",
    "# test.eval_train(\"triangle\", to_bq=True)\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
