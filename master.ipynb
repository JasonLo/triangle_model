{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model v4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HS04 model incorporating non-stationary environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *50 sem_cleanup*\n",
    "- *50 pho_cleanup*\n",
    "- *500 sem_pho_hidden_units*\n",
    "- *500 pho_sem_hidden_units*\n",
    "- *4 output_ticks* \n",
    "- *No auto-connection lock*\n",
    "- *Attractor clamped for 8 steps, free for last 4 steps*\n",
    "- implemented only in modeling.py but not the generator, it will just drop the extra generated time ticks\n",
    "- can do both phase 1 (oral) and 2 (reading) in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pickle, os, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import meta, data_wrangling, modeling, metrics, evaluate\n",
    "\n",
    "# meta.limit_gpu_memory_use(7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"interleave_003lr_test\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model configs\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 100\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "pho_noise_level = 0.0\n",
    "sem_noise_level = 0.0\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 12\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training configs\n",
    "learning_rate = 0.003\n",
    "zero_error_radius = 0.1\n",
    "save_freq = 10\n",
    "batch_name = None\n",
    "\n",
    "# Environment configs\n",
    "tasks = (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"triangle\")\n",
    "wf_clipping_edges = None\n",
    "wf_compression = \"log\"\n",
    "wf_clip_low = 0\n",
    "wf_clip_high = 999_999_999\n",
    "oral_start_pct = 0.02\n",
    "oral_end_pct = 0.5\n",
    "oral_sample = 900_000\n",
    "oral_tasks_ps = (0.4, 0.4, 0.1, 0.1, 0.0)\n",
    "transition_sample = 400_000\n",
    "reading_sample = 2_000_000\n",
    "reading_tasks_ps = (0.2, 0.2, 0.05, 0.05, 0.5)\n",
    "batch_size = 100\n",
    "rng_seed = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = meta.ModelConfig.from_json(os.path.join(tf_root, 'models', code_name, 'model_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "\n",
    "config_dict = {}\n",
    "for v in meta.CORE_CONFIGS + meta.ENV_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and all supporting components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "model.build()\n",
    "sampler = data_wrangling.Sampler(cfg, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = sampler.generator()\n",
    "\n",
    "# Full set of task specific components\n",
    "\n",
    "optimizers = {}\n",
    "loss_fns = {}\n",
    "train_losses = {}  # Mean loss (for TensorBoard)\n",
    "\n",
    "\n",
    "for task in cfg.tasks:\n",
    "    optimizers[task] = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "    loss_fns[task] = metrics.CustomBCE(radius=cfg.zero_error_radius)\n",
    "    train_losses[task] = tf.keras.metrics.Mean(f\"train_loss_{task}\", dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Training acc is output specific\n",
    "train_acc = {\n",
    "    \"pho_pho\": metrics.PhoAccuracy(\"acc_pho_pho\"),\n",
    "    \"sem_sem\": metrics.RightSideAccuracy(\"acc_sem_sem\"),\n",
    "    \"pho_sem\": metrics.RightSideAccuracy(\"acc_pho_sem\"),\n",
    "    \"sem_pho\": metrics.PhoAccuracy(\"acc_sem_pho\"),\n",
    "    \"triangle\": [\n",
    "        metrics.PhoAccuracy(\"acc_triangle_pho\"),\n",
    "        metrics.RightSideAccuracy(\"acc_triangle_sem\"),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since each sub-task has its own states, it must be trained with separate optimizer,\n",
    "# instead of sharing the same optimizer instance (https://github.com/tensorflow/tensorflow/issues/27120)\n",
    "\n",
    "\n",
    "def get_train_step():\n",
    "    \"\"\"Special train step for triangle phase with 2 outputs\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(\n",
    "        x,\n",
    "        y,\n",
    "        model,\n",
    "        task,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        train_metrics,\n",
    "        train_losses,\n",
    "    ):\n",
    "\n",
    "        train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "        train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "        if task == \"triangle\":\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x, training=True)\n",
    "                loss_value_pho = loss_fn(y[0], y_pred[0])  # Caution order matter\n",
    "                loss_value_sem = loss_fn(y[1], y_pred[1])\n",
    "                loss_value = loss_value_pho + loss_value_sem\n",
    "        else:\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x, training=True)\n",
    "                loss_value = loss_fn(y, y_pred)\n",
    "\n",
    "        grads = tape.gradient(loss_value, train_weights)\n",
    "        optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "        # Mean loss for Tensorboard\n",
    "        train_losses.update_state(loss_value)\n",
    "\n",
    "        # Metric for last time step (output first dimension is time ticks, from -cfg.output_ticks to end) for live results\n",
    "        if type(train_metrics) is list:\n",
    "            for i, x in enumerate(train_metrics):\n",
    "                x.update_state(tf.cast(y[i][-1], tf.float32), y_pred[i][-1])\n",
    "        else:\n",
    "            train_metrics.update_state(tf.cast(y[-1], tf.float32), y_pred[-1])\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "train_steps = {task: get_train_step() for task in cfg.tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(cfg.path[\"tensorboard_folder\"])\n",
    "\n",
    "for epoch in range(int(sampler.total_batches / 100)):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(100):\n",
    "        # Run 100 batches before every logging\n",
    "        # Draw task, create batch\n",
    "        task, exposed_words_idx, x_batch_train, y_batch_train = next(generator)\n",
    "        model.set_active_task(task)  # task switching must be done outside trainstep...\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_acc[task],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # End of epoch operations\n",
    "\n",
    "    ## Write log to tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "        ### Losses\n",
    "        [\n",
    "            tf.summary.scalar(f\"loss_{x}\", train_losses[x].result(), step=epoch)\n",
    "            for x in train_losses.keys()\n",
    "        ]\n",
    "\n",
    "        ### Metrics\n",
    "\n",
    "        for task in train_acc.keys():\n",
    "            if task == \"triangle\":\n",
    "                [\n",
    "                    tf.summary.scalar(acc.name, acc.result(), step=epoch)\n",
    "                    for acc in train_acc[task]\n",
    "                ]\n",
    "            else:\n",
    "                tf.summary.scalar(\n",
    "                    train_acc[task].name, train_acc[task].result(), step=epoch\n",
    "                )\n",
    "\n",
    "        ### Weight histogram\n",
    "        [tf.summary.histogram(f\"{x.name}\", x, step=epoch) for x in model.weights]\n",
    "\n",
    "    ## Print status\n",
    "    print(f\"Epoch {epoch + 1} trained for {time.time() - start_time:.0f}s\")\n",
    "    print(\n",
    "        \"Losses:\",\n",
    "        [f\"{x}: {train_losses[x].result().numpy()}\" for x in cfg.tasks],\n",
    "    )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## Save weights\n",
    "    if (epoch < 10) or ((epoch + 1) % cfg.save_freq == 0):\n",
    "        weight_path = cfg.path[\"weights_checkpoint_fstring\"].format(epoch=epoch + 1)\n",
    "        model.save_weights(weight_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "    ## Reset metric and loss\n",
    "    [train_losses[x].reset_states() for x in train_losses.keys()]\n",
    "\n",
    "    for task in train_acc.keys():\n",
    "        if task == \"triangle\":\n",
    "            [x.reset_states() for x in train_acc[task]]\n",
    "        else:\n",
    "            train_acc[task].reset_states()\n",
    "\n",
    "\n",
    "# End of training ops\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalReading:\n",
    "    \"\"\"Bundle of testsets\"\"\"\n",
    "    TESTSETS_NAME = (\"strain\", \"grain\")\n",
    "\n",
    "    def __init__(self, cfg, model, data):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "\n",
    "        self.strain_mean_df = None\n",
    "        self.grain_mean_df = None\n",
    "\n",
    "        \n",
    "        # Load eval results from file\n",
    "        for _testset_name in self.TESTSETS_NAME:\n",
    "            try:\n",
    "                _file = os.path.join(\n",
    "                    self.cfg.path[\"model_folder\"],\n",
    "                    \"eval\",\n",
    "                    f\"{_testset_name}_mean_df.csv\",\n",
    "                )\n",
    "                setattr(self, f\"{_testset_name}_mean_df\", pd.read_csv(_file))\n",
    "            except (FileNotFoundError, IOError):\n",
    "                pass\n",
    "\n",
    "        # Bundle testsets into dictionary\n",
    "        self.run_eval = {\n",
    "            \"train\": self._eval_train,\n",
    "            \"strain\": self._eval_strain,\n",
    "            \"grain\": self._eval_grain,\n",
    "            \"taraban\": self._eval_taraban,\n",
    "            \"cortese\": self._eval_cortese,\n",
    "        }\n",
    "        \n",
    "    def eval(self, testset_name):\n",
    "        \"\"\"Run eval and push to dat\"\"\"\n",
    "        if getattr(self, f\"{testset_name}_mean_df\") is None:\n",
    "            results = self.run_eval[testset_name]()\n",
    "        else:\n",
    "            print(\"Evaluation results found, loaded from file.\")\n",
    "                  \n",
    "        \n",
    "\n",
    "    def _eval_train(self):\n",
    "        testset_name = \"train\"\n",
    "        t = TestSet(\n",
    "            name=testset_name,\n",
    "            cfg=self.cfg,\n",
    "            model=self.model,\n",
    "            task=\"triangle\",\n",
    "            testitems=self.data.testsets[testset_name][\"item\"],\n",
    "            x_test=self.data.testsets[testset_name][\"ort\"],\n",
    "            y_test=[\n",
    "                self.data.testsets[testset_name][\"pho\"],\n",
    "                self.data.testsets[testset_name][\"sem\"],\n",
    "            ],\n",
    "        )\n",
    "        t.eval_all()\n",
    "        df = t.result\n",
    "        df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", f\"{testset_name}_item_df.csv\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Aggregate\n",
    "        mean_df = (\n",
    "            df.groupby([\"code_name\", \"task\", \"testset\", \"epoch\", \"timetick\", \"y\"])\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", f\"{testset_name}_mean_df.csv\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.train_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _eval_strain(self):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        testsets = (\n",
    "            \"strain_hf_con_hi\",\n",
    "            \"strain_hf_inc_hi\",\n",
    "            \"strain_hf_con_li\",\n",
    "            \"strain_hf_inc_li\",\n",
    "            \"strain_lf_con_hi\",\n",
    "            \"strain_lf_inc_hi\",\n",
    "            \"strain_lf_con_li\",\n",
    "            \"strain_lf_inc_li\"\n",
    "        )\n",
    "\n",
    "        for testset_name in testsets:\n",
    "            t = TestSet(\n",
    "                name=testset_name,\n",
    "                cfg=self.cfg,\n",
    "                model=self.model,\n",
    "                task=\"triangle\",\n",
    "                testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                x_test=self.data.testsets[testset_name][\"ort\"],\n",
    "                y_test=[\n",
    "                    self.data.testsets[testset_name][\"pho\"],\n",
    "                    self.data.testsets[testset_name][\"sem\"],\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            t.eval_all()\n",
    "            df = pd.concat([df, t.result])\n",
    "\n",
    "\n",
    "        df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", \"strain_item_df.csv\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Condition level aggregate\n",
    "        mean_df = (\n",
    "            df.groupby(\n",
    "                [\n",
    "                    \"code_name\",\n",
    "                    \"task\",\n",
    "                    \"testset\",\n",
    "                    \"epoch\",\n",
    "                    \"timetick\",\n",
    "                    \"y\",\n",
    "                ]\n",
    "            )\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", \"strain_mean_df.csv\"\n",
    "            )\n",
    "        )\n",
    "        self.strain_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _eval_grain(self):\n",
    "        df = pd.DataFrame()\n",
    "        for testset_name in (\"grain_unambiguous\", \"grain_ambiguous\"):\n",
    "            for grain_size in (\"pho_small_grain\", \"pho_large_grain\"):\n",
    "                t = TestSet(\n",
    "                    name=testset_name,\n",
    "                    cfg=self.cfg,\n",
    "                    model=self.model,\n",
    "                    task=\"triangle\",\n",
    "                    testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                    x_test=self.data.testsets[testset_name][\"ort\"],\n",
    "                    y_test=[\n",
    "                        self.data.testsets[testset_name][grain_size],\n",
    "                        self.data.testsets[testset_name][\"sem\"],\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "                t.eval_all()\n",
    "                t.result[\"y_test\"] = grain_size\n",
    "                df = pd.concat([df, t.result])\n",
    "\n",
    "        # Pho only\n",
    "        pho_df = df.loc[df.y == \"pho\"]\n",
    "\n",
    "        # Calculate pho acc by summing large and small grain response\n",
    "        pho_acc_df = (\n",
    "            pho_df.groupby(\n",
    "                [\"code_name\", \"task\", \"y\", \"testset\", \"epoch\", \"timetick\", \"item\"]\n",
    "            )\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        pho_acc_df[\"y_test\"] = \"pho\"\n",
    "\n",
    "        # Sem only (Because we have evaluated semantic twice, we need to remove the duplicates)\n",
    "        sem_df = df.loc[(df.y == \"sem\") & (df.y_test == \"pho_small_grain\")]\n",
    "        sem_df = sem_df.drop(columns=\"y_test\")\n",
    "        sem_df[\"y_test\"] = \"sem\"\n",
    "\n",
    "        df = pd.concat([pho_df, pho_acc_df, sem_df])\n",
    "        df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"grain_item_df.csv\")\n",
    "        )\n",
    "\n",
    "        mean_df = (\n",
    "            df.groupby(\n",
    "                [\"code_name\", \"task\", \"testset\", \"epoch\", \"timetick\", \"y\", \"y_test\"]\n",
    "            )\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"grain_mean_df.csv\")\n",
    "        )\n",
    "\n",
    "        self.grain_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _eval_taraban(self):\n",
    "\n",
    "        testsets = (\n",
    "            \"taraban_hf-exc\",\n",
    "            \"taraban_hf-reg-inc\",\n",
    "            \"taraban_lf-exc\",\n",
    "            \"taraban_lf-reg-inc\",\n",
    "            \"taraban_ctrl-hf-exc\",\n",
    "            \"taraban_ctrl-hf-reg-inc\",\n",
    "            \"taraban_ctrl-lf-exc\",\n",
    "            \"taraban_ctrl-lf-reg-inc\",\n",
    "        )\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for testset_name in testsets:\n",
    "\n",
    "            t = TestSet(\n",
    "                name=testset_name,\n",
    "                cfg=self.cfg,\n",
    "                model=self.model,\n",
    "                task=\"triangle\",\n",
    "                testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                x_test=self.data.testsets[testset_name][\"ort\"],\n",
    "                y_test=[\n",
    "                    self.data.testsets[testset_name][\"pho\"],\n",
    "                    self.data.testsets[testset_name][\"sem\"],\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            t.eval_all()\n",
    "            df = pd.concat([df, t.result])\n",
    "\n",
    "        df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"taraban_item_df.csv\")\n",
    "        )\n",
    "\n",
    "        mean_df = (\n",
    "            df.groupby([\"code_name\", \"task\", \"testset\", \"epoch\", \"timetick\", \"y\"])\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"taraban_mean_df.csv\")\n",
    "        )\n",
    "\n",
    "        self.taraban_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _eval_cortese(self):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for testset_name in (\"cortese_hi_img\", \"cortese_low_img\"):\n",
    "            t = TestSet(\n",
    "                name=testset_name,\n",
    "                cfg=self.cfg,\n",
    "                model=self.model,\n",
    "                task=\"triangle\",\n",
    "                testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                x_test=self.data.testsets[testset_name][\"ort\"],\n",
    "                y_test=[\n",
    "                    self.data.testsets[testset_name][\"pho\"],\n",
    "                    self.data.testsets[testset_name][\"sem\"],\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            t.eval_all()\n",
    "            df = pd.concat([df, t.result])\n",
    "\n",
    "        df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"cortese_item_df.csv\")\n",
    "        )\n",
    "\n",
    "        mean_df = (\n",
    "            df.groupby([\"code_name\", \"task\", \"testset\", \"epoch\", \"timetick\", \"y\"])\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(self.cfg.path[\"model_folder\"], \"eval\", \"cortese_mean_df.csv\")\n",
    "        )\n",
    "\n",
    "        self.cortese_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def plot_reading_acc(self, df):\n",
    "        timetick_selection = alt.selection_single(\n",
    "            bind=alt.binding_range(min=0, max=self.cfg.n_timesteps, step=1),\n",
    "            fields=[\"timetick\"],\n",
    "            init={\"timetick\": self.cfg.n_timesteps},\n",
    "            name=\"timetick\",\n",
    "        )\n",
    "\n",
    "        p = (\n",
    "            alt.Chart(df)\n",
    "            .mark_line()\n",
    "            .encode(\n",
    "                x=\"epoch:Q\",\n",
    "                y=alt.Y(\"acc:Q\", scale=alt.Scale(domain=(0, 1))),\n",
    "                color=\"y\",\n",
    "            )\n",
    "            .add_selection(timetick_selection)\n",
    "            .transform_filter(timetick_selection)\n",
    "        )\n",
    "\n",
    "        return p\n",
    "\n",
    "    def plot_grain_by_resp(self):\n",
    "        df = self.grain_mean_df.loc[\n",
    "            self.grain_mean_df.y_test.isin([\"pho_large_grain\", \"pho_small_grain\"])\n",
    "        ]\n",
    "        p = self.plot_reading_acc(df).encode(color=\"testset\", strokeDash=\"y_test\")\n",
    "        return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = evaluate.EvalReading(cfg, model, data)\n",
    "test.eval(\"strain\")\n",
    "test.eval(\"grain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.grain_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.path.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "\n",
    "def plot_reading_acc(df, cfg):\n",
    "    timetick_selection = alt.selection_single(\n",
    "        bind=alt.binding_range(min=0, max=cfg.n_timesteps, step=1),\n",
    "        fields=[\"timetick\"],\n",
    "        init={\"timetick\": cfg.n_timesteps},\n",
    "        name=\"timetick\",\n",
    "    )\n",
    "\n",
    "    y_selection = alt.selection_single(\n",
    "        bind=alt.binding_select(options=[\"pho\", \"sem\"]),\n",
    "        fields=[\"y\"],\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(df)\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=\"epoch:Q\",\n",
    "            y=alt.Y(\"mean(acc):Q\", scale=alt.Scale(domain=(0, 1))),\n",
    "            color=\"testset\",\n",
    "        )\n",
    "        .add_selection(timetick_selection)\n",
    "        .add_selection(y_selection)\n",
    "        .transform_filter(timetick_selection)\n",
    "        .transform_filter(y_selection)\n",
    "    )\n",
    "\n",
    "\n",
    "strain_plot = plot_reading_acc(test.strain_mean_df, cfg)\n",
    "strain_plot.save(os.path.join(cfg.path[\"plot_folder\"], \"strain.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grain_acceptable(df):\n",
    "\n",
    "    df = df.loc[df.y_test.isin([\"pho\", \"sem\"]) & (df.y == \"pho\")]\n",
    "\n",
    "    timetick_selection = alt.selection_single(\n",
    "        bind=alt.binding_range(min=0, max=cfg.n_timesteps, step=1),\n",
    "        fields=[\"timetick\"],\n",
    "        init={\"timetick\": cfg.n_timesteps},\n",
    "        name=\"timetick\",\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(df)\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=\"epoch:Q\",\n",
    "            y=alt.Y(\"acc:Q\", scale=alt.Scale(domain=(0, 1))),\n",
    "            color=\"testset\",\n",
    "        )\n",
    "        .add_selection(timetick_selection)\n",
    "        .transform_filter(timetick_selection)\n",
    "    )\n",
    "\n",
    "\n",
    "grain_plot_acc = plot_grain_acceptable(test.grain_mean_df)\n",
    "grain_plot_acc.save(os.path.join(cfg.path[\"plot_folder\"], \"grain1.html\"))\n",
    "\n",
    "\n",
    "def plot_grain_by_resp(df, cfg):\n",
    "    df = df.loc[df.y_test.isin([\"pho_large_grain\", \"pho_small_grain\"])]\n",
    "    p = plot_reading_acc(df, cfg).encode(color=\"testset\", strokeDash=\"y_test\")\n",
    "    return p\n",
    "\n",
    "\n",
    "grain_plot_resp = plot_grain_by_resp(test.grain_mean_df, cfg)\n",
    "grain_plot_resp.save(os.path.join(cfg.path[\"plot_folder\"], \"grain2.html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.strain_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, use_y=\"acc\", y_max=1, task=\"triangle\"):\n",
    "    df = df.loc[(df.task == task)]\n",
    "\n",
    "    timetick_selection = alt.selection_single(\n",
    "        bind=alt.binding_range(min=0, max=12, step=1),\n",
    "        fields=[\"timetick\"],\n",
    "        init={\"timetick\": cfg.n_timesteps},\n",
    "        name=\"timetick\",\n",
    "    )\n",
    "\n",
    "    y_selection = alt.selection_single(\n",
    "        bind=alt.binding_select(options=[\"pho\", \"sem\"]),\n",
    "        init={\"y\": \"pho\"},\n",
    "        fields=[\"y\"],\n",
    "    )\n",
    "\n",
    "    cond_selection = alt.selection_multi(bind=\"legend\", fields=[\"testset\"])\n",
    "\n",
    "    # Plot by condition\n",
    "    plot_by_cond = (\n",
    "        alt.Chart(df)\n",
    "        .mark_line()\n",
    "        .encode(\n",
    "            x=\"epoch:Q\",\n",
    "            y=alt.Y(f\"{use_y}:Q\", scale=alt.Scale(domain=(0, y_max))),\n",
    "            color=\"testset:N\",\n",
    "            opacity=alt.condition(cond_selection, alt.value(1), alt.value(0.1)),\n",
    "        )\n",
    "        .add_selection(timetick_selection)\n",
    "        .add_selection(y_selection)\n",
    "        .add_selection(cond_selection)\n",
    "        .transform_filter(timetick_selection)\n",
    "        .transform_filter(y_selection)\n",
    "    )\n",
    "\n",
    "    # Plot contrasts\n",
    "    contrasts = {}\n",
    "    contrasts[\n",
    "        \"F contrast\"\n",
    "    ] = \"\"\"(datum.strain_hf_con_hi + datum.strain_hf_con_li + datum.strain_hf_inc_hi + datum.strain_hf_inc_li - \n",
    "        (datum.strain_lf_con_hi + datum.strain_lf_con_li + datum.strain_lf_inc_hi + datum.strain_lf_inc_li))/4\"\"\"\n",
    "    contrasts[\n",
    "        \"CON contrast\"\n",
    "    ] = \"\"\"(datum.strain_hf_con_hi + datum.strain_hf_con_li + datum.strain_lf_con_hi + datum.strain_lf_con_li - \n",
    "        (datum.strain_hf_inc_hi + datum.strain_hf_inc_li + datum.strain_lf_inc_hi + datum.strain_lf_inc_li))/4\"\"\"\n",
    "    contrasts[\n",
    "        \"IMG contrast\"\n",
    "    ] = \"\"\"(datum.strain_hf_con_hi + datum.strain_lf_con_hi + datum.strain_hf_inc_hi + datum.strain_lf_inc_hi - \n",
    "        (datum.strain_hf_con_li + datum.strain_lf_con_li + datum.strain_hf_inc_li + datum.strain_lf_inc_li))/4\"\"\"\n",
    "\n",
    "    def create_contrast_plot(name):\n",
    "        return (\n",
    "            plot_by_cond.encode(\n",
    "                y=alt.Y(\"difference:Q\", scale=alt.Scale(domain=(-y_max, y_max)))\n",
    "            )\n",
    "            .transform_pivot(\"testset\", value=use_y, groupby=[\"epoch\"])\n",
    "            .transform_calculate(difference=contrasts[name])\n",
    "            .properties(title=name, width=100, height=100)\n",
    "        )\n",
    "\n",
    "    contrast_plots = alt.hconcat()\n",
    "    for c in contrasts.keys():\n",
    "        contrast_plots |= create_contrast_plot(c)\n",
    "\n",
    "    return plot_by_cond | contrast_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(test.strain_mean_df).save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"strain_contrast.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
