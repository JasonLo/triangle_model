{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model v4.0\n",
    "HS04 model incorporating non-stationary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext lab_black\n",
    "import os, time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import meta, data_wrangling, modeling, metrics, evaluate\n",
    "# meta.limit_gpu_memory_use(7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"fixed_default_beta1_run3\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model configs\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 100\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "pho_noise_level = 0.0\n",
    "sem_noise_level = 0.0\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 11\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training configs\n",
    "learning_rate = 0.005\n",
    "zero_error_radius = 0.1\n",
    "save_freq = 10\n",
    "batch_name = None\n",
    "\n",
    "# Environment configs\n",
    "tasks = (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"triangle\")\n",
    "wf_clipping_edges = None\n",
    "wf_compression = \"log\"\n",
    "wf_clip_low = 0\n",
    "wf_clip_high = 999_999_999\n",
    "oral_start_pct = 0.02\n",
    "oral_end_pct = 0.5\n",
    "oral_sample = 900_000\n",
    "oral_tasks_ps = (0.4, 0.4, 0.1, 0.1, 0.0)\n",
    "transition_sample = 400_000\n",
    "reading_sample = 2_000_000\n",
    "reading_tasks_ps = (0.2, 0.2, 0.05, 0.05, 0.5)\n",
    "batch_size = 100\n",
    "rng_seed = 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = meta.ModelConfig.from_json(os.path.join(tf_root, 'models', code_name, 'model_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "\n",
    "config_dict = {}\n",
    "for v in meta.CORE_CONFIGS + meta.ENV_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and all supporting components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "model.build()\n",
    "sampler = data_wrangling.Sampler(cfg, data)\n",
    "sampler.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core training modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_generator = sampler.generator()\n",
    "\n",
    "# Since each sub-task has its own states, it must be trained with separate optimizer and losses,\n",
    "# instead of sharing the same optimizer instance (https://github.com/tensorflow/tensorflow/issues/27120)\n",
    "optimizers = {}\n",
    "loss_fns = {}\n",
    "train_losses = {}  # Mean loss (only for TensorBoard)\n",
    "\n",
    "for task in cfg.tasks:\n",
    "    optimizers[task] = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "    loss_fns[task] = metrics.CustomBCE(radius=cfg.zero_error_radius)\n",
    "    train_losses[task] = tf.keras.metrics.Mean(f\"train_loss_{task}\", dtype=tf.float32) # for tensorboard only\n",
    "\n",
    "# Task specific train_metrics\n",
    "train_metrics = {\n",
    "    \"pho_pho\": [metrics.PhoAccuracy(\"pho_pho_acc\"), metrics.SumSquaredError(\"pho_pho_sse\")],\n",
    "    \"sem_sem\": [metrics.RightSideAccuracy(\"sem_sem_acc\"), metrics.SumSquaredError(\"sem_sem_sse\")],\n",
    "    \"pho_sem\": [metrics.RightSideAccuracy(\"pho_sem_acc\"), metrics.SumSquaredError(\"pho_sem_sse\")],\n",
    "    \"sem_pho\": [metrics.PhoAccuracy(\"sem_pho_acc\"), metrics.SumSquaredError(\"sem_pho_sse\")],\n",
    "    \"triangle\": {}\n",
    "}\n",
    "train_metrics[\"triangle\"][\"pho\"] = [metrics.PhoAccuracy(\"triangle_pho_acc\"), metrics.SumSquaredError(\"triangle_pho_sse\")]\n",
    "train_metrics[\"triangle\"][\"sem\"] = [metrics.RightSideAccuracy(\"triangle_sem_acc\"), metrics.SumSquaredError(\"triangle_sem_sse\")]\n",
    "\n",
    "# Train step\n",
    "train_steps = {task: modeling.get_train_step(task) for task in cfg.tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_scalar_to_tensorboard(task, step):\n",
    "    \"\"\"Write metrics and loss to tensorboard\"\"\"\n",
    "    loss = train_losses[task]\n",
    "    tf.summary.scalar(loss.name, loss.result(), step=step)\n",
    "\n",
    "    maybe_metrics = train_metrics[task]\n",
    "    if task == 'triangle':\n",
    "        [tf.summary.scalar(m.name, m.result(), step=step) for metrics in maybe_metrics.values() for m in metrics]\n",
    "    else:\n",
    "        [tf.summary.scalar(m.name, m.result(), step=step) for m in maybe_metrics]\n",
    "            \n",
    "def write_weight_histogram_to_tensorboard(step):\n",
    "    \"\"\"Weight histogram\"\"\"\n",
    "    [tf.summary.histogram(f\"{x.name}\", x, step=step) for x in model.weights]\n",
    "\n",
    "def reset_metrics(task):\n",
    "    maybe_metrics = train_metrics[task]\n",
    "    if task == 'triangle':\n",
    "        [m.reset_states() for metrics in maybe_metrics.values() for m in metrics]\n",
    "    else:\n",
    "        [m.reset_states() for m in maybe_metrics]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(cfg.path[\"tensorboard_folder\"])\n",
    "\n",
    "with train_summary_writer.as_default():\n",
    "    for epoch in tqdm(range(int(sampler.total_batches / 100))):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for step in range(100):\n",
    "            # 1 Epoch = 100 batches, each batch will run one trainstep interleaving tasks\n",
    "            # Draw task, create batch\n",
    "            task, exposed_words_idx, x_batch_train, y_batch_train = next(batch_generator)\n",
    "\n",
    "            # task switching must be done outside train_step function (will crash otherwise)\n",
    "            model.set_active_task(task)\n",
    "\n",
    "            # Run a train step\n",
    "            train_steps[task](\n",
    "                x_batch_train,\n",
    "                y_batch_train,\n",
    "                model,\n",
    "                task,\n",
    "                loss_fns[task],\n",
    "                optimizers[task],\n",
    "                train_metrics[task],\n",
    "                train_losses[task],\n",
    "            )\n",
    "\n",
    "        ## Write log to tensorboard (Once per epoch)\n",
    "        [write_scalar_to_tensorboard(task, step=epoch) for task in cfg.tasks]\n",
    "        write_weight_histogram_to_tensorboard(step=epoch)\n",
    "\n",
    "        ## Save weights\n",
    "        if (epoch < 10) or ((epoch + 1) % cfg.save_freq == 0):\n",
    "            weight_path = cfg.path[\"weights_checkpoint_fstring\"].format(epoch=epoch + 1)\n",
    "            model.save_weights(weight_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "        ## Reset metric and loss\n",
    "        [train_losses[x].reset_states() for x in cfg.tasks]\n",
    "        [reset_metrics(x) for x in cfg.tasks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "eval 3.0 under construction\n",
    "features:\n",
    "- Speed (2.0 code is easy to read but way too slow)\n",
    "- No separtion between oral and reading\n",
    "- More plots build in "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
