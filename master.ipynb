{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model v4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HS04 model incorporating non-stationary environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *50 sem_cleanup*\n",
    "- *50 pho_cleanup*\n",
    "- *500 sem_pho_hidden_units*\n",
    "- *500 pho_sem_hidden_units*\n",
    "- *4 output_ticks* \n",
    "- *No auto-connection lock*\n",
    "- *Attractor clamped for 8 steps, free for last 4 steps*\n",
    "- implemented only in modeling.py but not the generator, it will just drop the extra generated time ticks\n",
    "- can do both phase 1 (oral) and 2 (reading) in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pickle, os, time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import meta, data_wrangling, modeling, metrics, evaluate\n",
    "\n",
    "# meta.limit_gpu_memory_use(7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"tmp\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model architechture\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 100\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "\n",
    "pho_noise_level = 0.0\n",
    "sem_noise_level = 0.0\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 12\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training\n",
    "sample_name = \"flexi_rank\"\n",
    "wf_low_clip = 0\n",
    "wf_high_clip = 100_000_000_000\n",
    "wf_compression = 'log'\n",
    "sampling_plateau = 900_000\n",
    "\n",
    "rng_seed = 2021\n",
    "learning_rate = 0.005\n",
    "n_mil_sample = 1.0\n",
    "zero_error_radius = 0.1\n",
    "batch_size = 100\n",
    "save_freq = 10\n",
    "\n",
    "batch_name = None\n",
    "\n",
    "# tasks = [\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\"]\n",
    "# tasks_probability = [0.4, 0.4, 0.1, 0.1]\n",
    "\n",
    "tasks = [\"ort_pho\"]\n",
    "tasks_probability = [1.0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg = meta.ModelConfig.from_json(os.path.join(tf_root, 'models', code_name, 'model_config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "\n",
    "config_dict = {}\n",
    "for v in meta.CORE_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model and all supporting components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Model(cfg)\n",
    "model.build()\n",
    "sampler = data_wrangling.FastSampling(cfg, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full set of task specific components\n",
    "\n",
    "generators = {\n",
    "    \"ort_pho\": sampler.sample_generator(x=\"ort\", y=\"pho\"),\n",
    "    \"pho_sem\": sampler.sample_generator(x=\"pho\", y=\"sem\"),\n",
    "    \"sem_pho\": sampler.sample_generator(x=\"sem\", y=\"pho\"),\n",
    "    \"pho_pho\": sampler.sample_generator(x=\"pho\", y=\"pho\"),\n",
    "    \"sem_sem\": sampler.sample_generator(x=\"sem\", y=\"sem\"),\n",
    "    \"triangle\": sampler.sample_generator(x=\"ort\", y=[\"pho\", \"sem\"]),\n",
    "}\n",
    "\n",
    "# Instantiate optimizer for each task\n",
    "optimizers = {\n",
    "    \"ort_pho\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    \"pho_pho\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    \"sem_sem\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    \"pho_sem\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    \"sem_pho\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "    \"triangle\": tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate),\n",
    "}\n",
    "\n",
    "# Instantiate loss_fn for each task\n",
    "loss_fns = {\n",
    "    \"ort_pho\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "    \"pho_pho\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "    \"sem_sem\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "    \"pho_sem\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "    \"sem_pho\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "    \"triangle\": metrics.CustomBCE(radius=cfg.zero_error_radius),\n",
    "}\n",
    "\n",
    "# Mean loss (for TensorBoard)\n",
    "train_losses = {\n",
    "    \"ort_pho\": tf.keras.metrics.Mean(\"train_loss_ort_pho\", dtype=tf.float32),\n",
    "    \"pho_pho\": tf.keras.metrics.Mean(\"train_loss_pho_pho\", dtype=tf.float32),\n",
    "    \"sem_sem\": tf.keras.metrics.Mean(\"train_loss_sem_sem\", dtype=tf.float32),\n",
    "    \"pho_sem\": tf.keras.metrics.Mean(\"train_loss_pho_sem\", dtype=tf.float32),\n",
    "    \"sem_pho\": tf.keras.metrics.Mean(\"train_loss_sem_pho\", dtype=tf.float32),\n",
    "    \"triangle\": tf.keras.metrics.Mean(\"train_loss_triangle\", dtype=tf.float32),\n",
    "}\n",
    "\n",
    "# Train metrics\n",
    "train_acc = {\n",
    "    \"ort_pho\": metrics.PhoAccuracy(\"acc_ort_pho\"),\n",
    "    \"pho_pho\": metrics.PhoAccuracy(\"acc_pho_pho\"),\n",
    "    \"sem_sem\": metrics.RightSideAccuracy(\"acc_sem_sem\"),\n",
    "    \"pho_sem\": metrics.RightSideAccuracy(\"acc_pho_sem\"),\n",
    "    \"sem_pho\": metrics.PhoAccuracy(\"acc_sem_pho\"),\n",
    "    \"triangle_pho\": metrics.PhoAccuracy(\"acc_triangle_pho\"),\n",
    "    \"triangle_sem\": metrics.RightSideAccuracy(\"acc_triangle_sem\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train step for each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since each sub-task has its own states, it must be trained with separate optimizer,\n",
    "# instead of sharing the same optimizer instance (https://github.com/tensorflow/tensorflow/issues/27120)\n",
    "\n",
    "def get_train_step_one_output():\n",
    "    \"\"\"Wrap universal train step creator\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y, model, task, loss_fn, optimizer, train_metric, train_losses, sample_weights=None):\n",
    "\n",
    "        train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "        train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(x, training=True)\n",
    "            loss_value = loss_fn(y, y_pred)\n",
    "\n",
    "            if sample_weights is not None:\n",
    "                loss_value = sample\n",
    "\n",
    "        grads = tape.gradient(loss_value, train_weights)\n",
    "        optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "        # Mean loss for Tensorboard\n",
    "        train_losses.update_state(loss_value)\n",
    "\n",
    "        # Metric for last time step (output first dimension is time ticks, from -cfg.output_ticks to end) for live results\n",
    "        train_metric.update_state(tf.cast(y[-1], tf.float32), y_pred[-1])\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "def get_train_step_triangle():\n",
    "    \"\"\"Special train step for triangle phase with 2 outputs\"\"\"\n",
    "\n",
    "    @tf.function\n",
    "    def train_step_triangle(\n",
    "        x,\n",
    "        y,\n",
    "        model,\n",
    "        task,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        train_metric_pho,\n",
    "        train_metric_sem,\n",
    "        train_losses,\n",
    "    ):\n",
    "\n",
    "        train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "        train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pho_pred, sem_pred = model(x, training=True)\n",
    "            loss_value_pho = loss_fn(y[0], pho_pred)\n",
    "            loss_value_sem = loss_fn(y[1], sem_pred)\n",
    "            loss_value = loss_value_pho + loss_value_sem\n",
    "\n",
    "        grads = tape.gradient(loss_value, train_weights)\n",
    "        optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "        # Mean loss for Tensorboard\n",
    "        train_losses.update_state(loss_value)\n",
    "\n",
    "        # Metric for last time step (output first dimension is time ticks, from -cfg.output_ticks to end) for live results\n",
    "        train_metric_pho.update_state(tf.cast(y[0][-1], tf.float32), pho_pred[-1])\n",
    "        train_metric_sem.update_state(tf.cast(y[1][-1], tf.float32), sem_pred[-1])\n",
    "\n",
    "\n",
    "train_steps = {\n",
    "    \"ort_pho\": get_train_step_one_output(),\n",
    "    \"pho_pho\": get_train_step_one_output(),\n",
    "    \"pho_sem\": get_train_step_one_output(),\n",
    "    \"sem_sem\": get_train_step_one_output(),\n",
    "    \"sem_pho\": get_train_step_one_output(),\n",
    "    \"triangle\": get_train_step_triangle(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(cfg.path[\"tensorboard_folder\"])\n",
    "\n",
    "for epoch in range(cfg.total_number_of_epoch):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for step in range(cfg.steps_per_epoch):\n",
    "        # Intermix tasks (Draw a new task in each step)\n",
    "        task = np.random.choice(cfg.tasks, p=cfg.tasks_probability)\n",
    "        x_batch_train, y_batch_train = next(generators[task])\n",
    "        model.set_active_task(task)  # task switching must be done outside trainstep...\n",
    "\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_acc[task],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # End of epoch operations\n",
    "\n",
    "    ## Write log to tensorboard\n",
    "    with train_summary_writer.as_default():\n",
    "        ### Losses\n",
    "        [\n",
    "            tf.summary.scalar(f\"loss_{x}\", train_losses[x].result(), step=epoch)\n",
    "            for x in train_losses.keys()\n",
    "        ]\n",
    "\n",
    "        ### Metrics\n",
    "        [\n",
    "            tf.summary.scalar(f\"acc_{x}\", train_acc[x].result(), step=epoch)\n",
    "            for x in train_acc.keys()\n",
    "        ]\n",
    "\n",
    "        ### Weight histogram\n",
    "        [tf.summary.histogram(f\"{x.name}\", x, step=epoch) for x in model.weights]\n",
    "\n",
    "    ## Print status\n",
    "    compute_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch + 1} trained for {compute_time:.0f}s\")\n",
    "    print(\n",
    "        \"Losses:\",\n",
    "        [f\"{x}: {train_losses[x].result().numpy()}\" for x in cfg.tasks],\n",
    "    )\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## Save weights\n",
    "    if (epoch < 10) or ((epoch + 1) % cfg.save_freq == 0):\n",
    "        weight_path = cfg.path[\"weights_checkpoint_fstring\"].format(epoch=epoch + 1)\n",
    "        model.save_weights(weight_path, overwrite=True, save_format=\"tf\")\n",
    "\n",
    "    ## Reset metric and loss\n",
    "    [train_losses[x].reset_states() for x in train_losses.keys()]\n",
    "    [train_acc[x].reset_states() for x in train_acc.keys()]\n",
    "\n",
    "\n",
    "# End of training ops\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = evaluate.EvalOral(cfg, model, data)\n",
    "# test.eval('strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OP eval\n",
    "ts_strain = data.load_testset(os.path.join(tf_root, 'dataset', 'testsets', 'strain.pkl.gz'))\n",
    "\n",
    "test = evaluate.TestSet(\n",
    "    name = 'strain', \n",
    "    cfg=cfg, \n",
    "    model=model,\n",
    "    task='ort_pho', \n",
    "    testitems=ts_strain['item'], \n",
    "    x_test=ts_strain['ort'],\n",
    "    y_test=ts_strain['pho'])\n",
    "\n",
    "test.eval_all()\n",
    "\n",
    "# post-processing\n",
    "merge_data = data.df_strain[['word', 'frequency', 'pho_consistency', 'imageability']].copy()\n",
    "merge_data['cond'] = merge_data.frequency + '_' +  merge_data.pho_consistency\n",
    "item_df = test.result.merge(merge_data, how='left', left_on='item', right_on='word')\n",
    "item_df.to_csv(os.path.join(cfg.path['model_folder'], 'eval', 'strain_item_df.csv'))\n",
    "mean_df = item_df.groupby(['epoch', 'timetick', 'cond', 'frequency', 'pho_consistency']).mean() .reset_index()\n",
    "mean_df.to_csv(os.path.join(cfg.path['model_folder'], 'eval', 'strain_mean_df.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m68"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
