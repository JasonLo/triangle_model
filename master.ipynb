{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters block (for papermill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"_boo\"\n",
    "batch_name = None\n",
    "\n",
    "# Model configs\n",
    "ort_units = 119\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "hidden_os_units = 500\n",
    "hidden_op_units = 100\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "pho_noise_level = 0.\n",
    "sem_noise_level = 0.\n",
    "activation = \"sigmoid\"\n",
    "\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 13\n",
    "inject_error_ticks = 11\n",
    "\n",
    "# Training configs\n",
    "learning_rate = 0.001\n",
    "zero_error_radius = 0.1\n",
    "save_freq = 10\n",
    "\n",
    "# Environment configs\n",
    "wf_compression = \"log\"\n",
    "wf_clip_low = 0\n",
    "wf_clip_high = 999_999_999\n",
    "\n",
    "task_names = (\"pho_sem\", \"sem_pho\", \"pho_pho\", \"sem_sem\", \"triangle\")\n",
    "tasks_ps = (0.2, 0.2, 0.05, 0.05, 0.5)\n",
    "# tasks_ps = (0.4, 0.4, 0.1, 0.1)\n",
    "\n",
    "total_sample = 5_000_000\n",
    "batch_size = 4\n",
    "rng_seed = 2021\n",
    "which_gpu = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocate GPU resources\n",
    "IMPORTANT: DO NOT IMPORT OTHER LIBS BEFORE THIS BLOCK!!!\n",
    "Otherwise, the GPU will not be instantiate correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta import split_gpu\n",
    "split_gpu(which_gpu=which_gpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "\n",
    "# NOTE: determinism is only available in TF nightly (2.8)\n",
    "# tf.keras.utils.set_random_seed(rng_seed)\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "tf.random.set_seed(rng_seed)\n",
    "np.random.seed(rng_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import meta, data_wrangling, metrics, modeling\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config, Data, and Model modules\n",
    "- About Config():\n",
    "    - We build a Config() object from the parameters described in the parameter block above\n",
    "    - As long as there is no change in the underlying source code (./src), we can use the same Config to producce a somewhat similar model in TF2.6\n",
    "    - It is \"somewhat\" similar because running model on GPU is not deterministic (and thus the results are not identical)\n",
    "    - Deterministic mode will be available in TF 2.8 release\n",
    "- About MyData():\n",
    "    - It is a class that contains all the data that is used by the model training\n",
    "- About MyModel():\n",
    "    - It is a class that contains the triangle model implementation on TensorFlow using subclass-level API\n",
    "    - It is a subclass of tf.keras.Model\n",
    "    - It contains multiple tasks (e.g., triangle, PS, SP, PP, SS, etc.)\n",
    "    - The behavior of the model is defined by the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = meta.Config.from_global(**globals())\n",
    "# cfg = meta.Config.from_json(os.path.join(tf_root, \"models\", batch_name, code_name, \"model_config.json\"))\n",
    "\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.MyModel(cfg)\n",
    "model.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Training) Environment module\n",
    "- Environment defines what the model is trained on. It consists of one or more stages. \n",
    "- Each stage describe what the model training tasks are and their probability (how often a task is used during training). It contains one or more tasks. \n",
    "- Each task contains how fast the corpus (actual word that can be sampled) is opened, default to full open (that follows the word frequency compression sampling stragegy defined in config). \n",
    "- See the docstrings in each object for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import Task, Stage, Experience, Sampler\n",
    "\n",
    "stages = [\n",
    "    Stage(\n",
    "        name=\"one\",\n",
    "        tasks=[Task(x) for x in cfg.task_names], \n",
    "        stage_sample=cfg.total_sample, \n",
    "        task_probability_start=cfg.tasks_ps\n",
    "        )\n",
    "]\n",
    "\n",
    "experience = Experience(stages)\n",
    "sampler = Sampler(cfg, data, experience)\n",
    "batch_generator = sampler.generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots to visualize environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience.plot_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experience.plot_task_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_ckpt = tf.train.Checkpoint(model=model)\n",
    "pretrain_ckpt.restore(os.path.join(cfg.tf_root, \"models\", \"pretrain_3M\", \"checkpoints\", \"epoch-300\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create optimizers, loss functions, and metrics\n",
    "- Since each sub-task has its own states, it must be trained with separate optimizer and losses, instead of sharing the same optimizer instance, see this [issue](https://github.com/tensorflow/tensorflow/issues/27120)\n",
    "- Regarding to the metrics, there are two types of metrics:\n",
    "    - Stateless metrics: the average metric of all batches within an epoch (semantic acc, sse)\n",
    "    - Stateful metrics: only taking last step/batch value in an epoch (pho acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = {}\n",
    "loss_fns = {}\n",
    "train_losses = {}  # Mean loss (only for TensorBoard)\n",
    "train_metrics = {}\n",
    "\n",
    "acc = {\"pho\": metrics.PhoAccuracy, \"sem\": metrics.StatelessRightSideAccuracy}\n",
    "sse = metrics.StatelessSumSquaredError\n",
    "\n",
    "for task in cfg.task_names:\n",
    "    # optimizers[task] = tf.keras.optimizers.SGD(learning_rate=cfg.learning_rate)\n",
    "    optimizers[task] = tf.keras.optimizers.Adam(learning_rate=cfg.learning_rate)\n",
    "    if cfg.zero_error_radius is not None:\n",
    "        loss_fns[task] = metrics.CustomBCE(radius=cfg.zero_error_radius)\n",
    "    else:\n",
    "        loss_fns[task] = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "    train_losses[task] = tf.keras.metrics.Mean(\n",
    "        f\"train_loss_{task}\", dtype=tf.float32\n",
    "    )  # for tensorboard only\n",
    "\n",
    "    task_output = modeling.IN_OUT[task][1]\n",
    "\n",
    "    if type(task_output) is list:\n",
    "        train_metrics[task] = {}\n",
    "\n",
    "        for out in task_output:\n",
    "            train_metrics[task][out] = [\n",
    "                acc[out](f\"{task}_{out}_acc\"),\n",
    "                sse(f\"{task}_{out}_sse\"),\n",
    "            ]\n",
    "    else:\n",
    "        train_metrics[task] = [acc[task_output](f\"{task}_acc\"), sse(f\"{task}_sse\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_step(task):\n",
    "    input_name, output_name = modeling.IN_OUT[task]\n",
    "\n",
    "    if task == \"triangle\":\n",
    "\n",
    "        @tf.function()\n",
    "        def train_step(\n",
    "            x, y, model, task, loss_fn, optimizer, train_metrics, train_losses\n",
    "        ):\n",
    "            \"\"\"Train a batch, log loss and metrics (last time step only)\"\"\"\n",
    "\n",
    "            train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "            train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "            # TF Automatic differentiation\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x, training=True)\n",
    "                # training flag can be access within model by K.in_train_phase()\n",
    "                # it can change the behavior in model() (e.g., turn on/off noise)\n",
    "\n",
    "                loss_value_pho = loss_fn(y[\"pho\"], y_pred[\"pho\"])\n",
    "                loss_value_sem = loss_fn(y[\"sem\"], y_pred[\"sem\"])\n",
    "                loss_value = loss_value_pho + loss_value_sem\n",
    "\n",
    "            grads = tape.gradient(loss_value, train_weights)\n",
    "\n",
    "            # Weight update\n",
    "            optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "            # Calculate mean loss and metrics for tensorboard\n",
    "            # Metrics update (Only last time step)\n",
    "            # for y_name, metrics in train_metrics.items():\n",
    "            #     if y_name == \"pho\":\n",
    "            #         # y[0] is pho, y[0][-1] is last time step in pho\n",
    "            #         [\n",
    "            #             m.update_state(\n",
    "            #                 tf.cast(y[\"pho\"][-1], tf.float32), y_pred[\"pho\"][-1]\n",
    "            #             )\n",
    "            #             for m in metrics\n",
    "            #         ]\n",
    "            #     else:\n",
    "            #         # y[1] is sem, y[0][-1] is last time step in sem\n",
    "            #         [\n",
    "            #             m.update_state(\n",
    "            #                 tf.cast(y[\"sem\"][-1], tf.float32), y_pred[\"sem\"][-1]\n",
    "            #             )\n",
    "            #             for m in metrics\n",
    "            #         ]\n",
    "\n",
    "            # Mean loss\n",
    "            train_losses.update_state(loss_value)\n",
    "\n",
    "    else:  # Single output tasks\n",
    "\n",
    "        @tf.function()\n",
    "        def train_step(\n",
    "            x, y, model, task, loss_fn, optimizer, train_metrics, train_losses\n",
    "        ):\n",
    "            train_weights_name = [x + \":0\" for x in modeling.WEIGHTS_AND_BIASES[task]]\n",
    "            train_weights = [x for x in model.weights if x.name in train_weights_name]\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(x, training=True)\n",
    "                loss_value = loss_fn(y, y_pred[output_name])\n",
    "\n",
    "            grads = tape.gradient(loss_value, train_weights)\n",
    "            optimizer.apply_gradients(zip(grads, train_weights))\n",
    "\n",
    "            # [\n",
    "            #     m.update_state(tf.cast(y[-1], tf.float32), y_pred[output_name][-1])\n",
    "            #     for m in train_metrics\n",
    "            # ]\n",
    "            train_losses.update_state(loss_value)\n",
    "\n",
    "    return train_step\n",
    "\n",
    "\n",
    "train_steps = {task: get_train_step(task) for task in cfg.task_names}\n",
    "# train_steps = {task: modeling.get_train_step(task) for task in cfg.tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def write_scalar_to_tensorboard(task, step):\n",
    "#     \"\"\"Write metrics and loss to tensorboard\"\"\"\n",
    "#     loss = train_losses[task]\n",
    "#     tf.summary.scalar(loss.name, loss.result(), step=step)\n",
    "\n",
    "#     maybe_metrics = train_metrics[task]\n",
    "#     if task == \"triangle\":\n",
    "#         [\n",
    "#             tf.summary.scalar(m.name, m.result(), step=step)\n",
    "#             for metrics in maybe_metrics.values()\n",
    "#             for m in metrics\n",
    "#         ]\n",
    "#     else:\n",
    "#         [tf.summary.scalar(m.name, m.result(), step=step) for m in maybe_metrics]\n",
    "\n",
    "\n",
    "def write_weight_histogram_to_tensorboard(step):\n",
    "    \"\"\"Weight histogram\"\"\"\n",
    "    [tf.summary.histogram(f\"{x.name}\",   x, step=step) for x in model.weights]\n",
    "\n",
    "\n",
    "# def reset_metrics(task):\n",
    "#     maybe_metrics = train_metrics[task]\n",
    "#     if task == \"triangle\":\n",
    "#         [m.reset_states() for metrics in maybe_metrics.values() for m in metrics]\n",
    "#     else:\n",
    "#         [m.reset_states() for m in maybe_metrics]\n",
    "\n",
    "# TensorBoard writer\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "    os.path.join(cfg.tensorboard_folder, \"train\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoint (save) module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = tf.Variable(0, name='epoch')\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    epoch=epoch,\n",
    "    model=model, \n",
    "    optimizers=optimizers,\n",
    "    )\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    cfg.checkpoint_folder, \n",
    "    max_to_keep=None,  # Keep all checkpoints\n",
    "    checkpoint_name=\"epoch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resume from latest checkpoint\n",
    "- Environment will no longer be identical if resume from checkpoint (Unable to put it in checkpoint for now)\n",
    "- However, resume training is not very common, so it is not a big deal for now... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore from checkpoint\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Restored from {}\".format(ckpt_manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(total=cfg.total_number_of_epoch, desc=\"Training\")\n",
    "progress_bar.update(epoch.numpy())\n",
    "\n",
    "while epoch.numpy() < cfg.total_number_of_epoch:\n",
    "\n",
    "    # Train an epoch\n",
    "    for step in range(cfg.steps_per_epoch):\n",
    "        # Draw task, create batch\n",
    "        task, exposed_words_idx, exposed_word, x_batch_train, y_batch_train = next(batch_generator)\n",
    "\n",
    "        # task switching must be done outside train_step function (will crash otherwise)\n",
    "        model.set_active_task(task)\n",
    "\n",
    "        # Run a train step\n",
    "        train_steps[task](\n",
    "            x_batch_train,\n",
    "            y_batch_train,\n",
    "            model,\n",
    "            task,\n",
    "            loss_fns[task],\n",
    "            optimizers[task],\n",
    "            train_metrics[task],\n",
    "            train_losses[task],\n",
    "        )\n",
    "\n",
    "    # Post epoch Ops\n",
    "    progress_bar.update(1)\n",
    "    epoch.assign_add(1)\n",
    "\n",
    "    with train_summary_writer.as_default():\n",
    "        ## Write log to tensorboard (Once per epoch)\n",
    "        # [write_scalar_to_tensorboard(task, step=epoch) for task in cfg.tasks]\n",
    "        write_weight_histogram_to_tensorboard(step=epoch.numpy())\n",
    "\n",
    "    #     ## Reset metric and loss\n",
    "        [train_losses[x].reset_states() for x in cfg.task_names]\n",
    "    #     [reset_metrics(x) for x in cfg.tasks]\n",
    "\n",
    "    # In training loop testset eval\n",
    "    # Kind of fast, but hard to integrate and customize... maybe use offline eval again.\n",
    "    # [strain(task, step=epoch) for task in strain.tasks]\n",
    "    # [grain(task, step=epoch) for task in grain.tasks]\n",
    "\n",
    "    ## Save weights every save_freq epochs\n",
    "    if (epoch.numpy() in cfg.saved_epochs):\n",
    "        f=ckpt_manager.save(epoch) \n",
    "        # model.save_weights(cfg.saved_weights_fstring.format(epoch=epoch.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run usual evals\n",
    "import benchmark_hs04\n",
    "benchmark_hs04.main(cfg.code_name, cfg.batch_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just test1 (acc in triangle and oral)\n",
    "# import benchmark_hs04\n",
    "# benchmark_hs04.run_test1(cfg.code_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluate\n",
    "test = evaluate.TestSet(cfg)\n",
    "test.eval_train(\"triangle\", to_bq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to storage bucket and shutdown\n",
    "# !gsutil -m rsync -d -r models/{code_name} gs://tf_mirror/{code_name}\n",
    "# sleep(30)\n",
    "# !sudo shutdown -P +0"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m74"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
