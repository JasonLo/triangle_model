{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import h5py, pickle, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src import meta, data_wrangling, modeling, evaluate\n",
    "from IPython.display import clear_output\n",
    "\n",
    "meta.gpu_mem_cap(2048)  # Put memory cap to allow parallel runs\n",
    "meta.check_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters block for Papermill\n",
    "- Instead of using model_cfg directly, this extra step is needed for batch run using Papermill\n",
    "- Consider carefully the variable type in each cfg setting (Probably automatically check it later...)\n",
    "    - Do not use integer (e.g., 1, 2, 3, 0) in variables that can be float32 (e.g., w_oh_noise, tau...)\n",
    "    - Use integer with a dot instead (e.g., 1., 2., 3., 0.) to indicate a float32 value\n",
    "- To use attractor, two params must be config, \n",
    "    - 1) embed_attractor_cfg --> json cfg file of the pretrain attractor\n",
    "        - e.g.: f'models/Attractor_{cleanup_units:02d}/model_config.json'\n",
    "    - 2) embed_attractor_h5 --> h5 file of the exact weight (e.g. ep0500.h5 #epoch or c90.h5 #correct rate)\n",
    "        - e.g.: 'c00.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"foo\"\n",
    "\n",
    "# Dataset\n",
    "sample_name = \"experimental\"\n",
    "rng_seed = 53797\n",
    "use_semantic = False\n",
    "\n",
    "# Model architechture\n",
    "input_dim = 119\n",
    "output_dim = 250\n",
    "hidden_units = 100\n",
    "cleanup_units = 50\n",
    "\n",
    "rnn_activation = \"sigmoid\"\n",
    "regularizer_const = None\n",
    "w_initializer = 0.1\n",
    "zero_error_radius = 0.1\n",
    "\n",
    "p_noise = 0.0\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 2\n",
    "\n",
    "# Pre-Training\n",
    "pretrain_attractor = False\n",
    "embed_attractor_cfg = None\n",
    "embed_attractor_h5 = None\n",
    "\n",
    "# Training\n",
    "optimizer = \"adam\"\n",
    "n_mil_sample = 1.0\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "save_freq = 10\n",
    "\n",
    "# MISC\n",
    "show_plots_in_notebook = True\n",
    "bq_dataset = None\n",
    "batch_unique_setting_string = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "# Load global cfg variables into a dictionary\n",
    "for v in meta.model_cfg.minimal_cfgs:\n",
    "    d[v] = globals()[v]\n",
    "\n",
    "for v in meta.model_cfg.aux_cfgs:\n",
    "    try:\n",
    "        d[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct model_cfg object\n",
    "cfg = meta.model_cfg(**d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IwxbE29_0yx"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Input, concatenate, multiply, RepeatVector\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from src.metrics import ZERCount, OutputOfZeroTarget, OutputOfOneTarget\n",
    "\n",
    "tf.random.set_seed(cfg.rng_seed)\n",
    "data = data_wrangling.MyData()\n",
    "\n",
    "my_metrics = [\n",
    "    \"BinaryAccuracy\",  # Stock metric\n",
    "    \"mse\",  # Stock metric\n",
    "    OutputOfZeroTarget(),\n",
    "    OutputOfOneTarget(),\n",
    "    ZERCount(),\n",
    "]\n",
    "\n",
    "\n",
    "def build_model(training=True):\n",
    "    \"\"\"\n",
    "    Create Keras model\n",
    "    Note that:\n",
    "    For structural things, such as repeat vector, should build within the model\n",
    "    For Static calculation of input, it is easier to modify, should build within sample generator\n",
    "    \"\"\"\n",
    "\n",
    "    cfg.noise_on() if training else cfg.noise_off()\n",
    "\n",
    "    input_o = Input(shape=(cfg.input_dim,), name=\"Input_O\")\n",
    "    input_o_t = RepeatVector(cfg.n_timesteps, name=\"Input_Ot\")(input_o)\n",
    "\n",
    "    # Construct semantic input\n",
    "    if cfg.use_semantic == True:\n",
    "        raw_s_t = Input(shape=(cfg.n_timesteps, cfg.output_dim), name=\"Plaut_St\")\n",
    "\n",
    "        input_p = Input(shape=(cfg.output_dim,), name=\"input_P\")\n",
    "        input_p_t = RepeatVector(cfg.n_timesteps, name=\"Teaching_Pt\")(input_p)\n",
    "\n",
    "        input_s_t = multiply([raw_s_t, input_p_t], name=\"Input_St\")\n",
    "\n",
    "        combined = concatenate([input_o_t, input_s_t], name=\"Combined_input\")\n",
    "        rnn_model = modeling.rnn(cfg)(combined)\n",
    "        model = Model([input_o, raw_s_t, input_p], rnn_model)\n",
    "\n",
    "    else:\n",
    "        rnn_model = modeling.rnn(cfg)(input_o_t)\n",
    "        model = Model(input_o, rnn_model)\n",
    "\n",
    "    # Select optimizer\n",
    "    if cfg.optimizer == \"adam\":\n",
    "        op = Adam(\n",
    "            learning_rate=cfg.learning_rate, beta_1=0.0, beta_2=0.999, amsgrad=False\n",
    "        )\n",
    "\n",
    "    elif cfg.optimizer == \"sgd\":\n",
    "        op = SGD(cfg.learning_rate)\n",
    "\n",
    "    # Select zero error radius (by chossing custom loss function zer_bce())\n",
    "\n",
    "    if cfg.zero_error_radius is not None:\n",
    "        print(f\"Using zero-error-radius of {cfg.zero_error_radius}\")\n",
    "        model.compile(\n",
    "            loss=modeling.CustomBCE(radius=cfg.zero_error_radius),\n",
    "            optimizer=op,\n",
    "            metrics=my_metrics,\n",
    "        )\n",
    "\n",
    "    elif cfg.zero_error_radius is None:\n",
    "        print(f\"No zero-error-radius\")\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=op, metrics=me)\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arm attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if cfg.pretrain_attractor is True:\n",
    "    print(\"Found attractor info in config (cfg), arming attractor...\")\n",
    "    attractor_cfg = meta.model_cfg(cfg.embed_attractor_cfg, bypass_chk=True)\n",
    "    attractor_obj = modeling.attractor(attractor_cfg, cfg.embed_attractor_h5)\n",
    "    model = modeling.arm_attractor(model, attractor_obj)\n",
    "    evaluate.plot_variables(model)\n",
    "else:\n",
    "    print(\"Config indicates no attractor, I have do nothing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKUOoZkP8QiA"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create sampling instance\n",
    "my_sampling = data_wrangling.Sampling(cfg, data)\n",
    "\n",
    "checkpoint = modeling.ModelCheckpoint_custom(\n",
    "    cfg.path_weights_checkpoint, save_weights_only=True, period=cfg.save_freq,\n",
    ")\n",
    "\n",
    "tboard = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=f\"tensorboard_log/{cfg.code_name}\", histogram_freq=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    my_sampling.sample_generator(),\n",
    "    steps_per_epoch=cfg.steps_per_epoch,\n",
    "    epochs=cfg.nEpo,\n",
    "    verbose=0,\n",
    "    callbacks=[checkpoint, tboard],\n",
    ")\n",
    "\n",
    "# Saving history and model\n",
    "pickle_out = open(cfg.path_history_pickle, \"wb\")\n",
    "pickle.dump(history.history, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "clear_output()\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMmNcbJdcPMh"
   },
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must turn training mode off before evaluation\n",
    "model = build_model(training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse item level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain\n",
    "strain = evaluate.strain_eval(cfg, data, model)\n",
    "strain.start_evaluate(\n",
    "    test_use_semantic=cfg.use_semantic, output=cfg.path_model_folder + \"result_strain_item.csv\"\n",
    ")\n",
    "\n",
    "# Grain\n",
    "grain = evaluate.grain_eval(cfg, data, model)\n",
    "grain.start_evaluate(output=cfg.path_model_folder + \"result_grain_item.csv\")\n",
    "\n",
    "# Taraban\n",
    "taraban = evaluate.taraban_eval(cfg, data, model)\n",
    "taraban.start_evaluate(\n",
    "    test_use_semantic=cfg.use_semantic, output=cfg.path_model_folder + \"result_taraban_item.csv\"\n",
    ")\n",
    "\n",
    "# Glushko\n",
    "glushko = evaluate.glushko_eval(cfg, data, model)\n",
    "glushko.start_evaluate(test_use_semantic=cfg.use_semantic, output=cfg.path_model_folder + \"result_glushko_item.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = evaluate.vis(cfg.path_model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.training_hist().save(cfg.path_plot_folder + \"training_history.html\")\n",
    "if show_plots_in_notebook:\n",
    "    vis.training_hist().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KnxCNzMyWah"
   },
   "source": [
    "### Strain and Grain plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = vis.plot_dev_interactive(\"acc\", [\"strain\", \"grain\"]).properties(\n",
    "    title=\"Accuracy in Strain and Grain\"\n",
    ")\n",
    "sg.save(cfg.path_plot_folder + \"development_sg.html\")\n",
    "if show_plots_in_notebook:\n",
    "    sg.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taraban and Glushko plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = vis.plot_dev_interactive(\"acc\", [\"taraban\", \"glushko\"]).properties(\n",
    "    title=\"Accuracy in Taraban and Glushko by condition\"\n",
    ")\n",
    "tb.save(cfg.path_plot_folder + \"development_tg.html\")\n",
    "\n",
    "if show_plots_in_notebook:\n",
    "    tb.display()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = vis.plot_dev_interactive(\"acc_small_grain\", exp=[\"grain\"]).properties(\n",
    "    title=\"Small Grain Response\"\n",
    ")\n",
    "large = vis.plot_dev_interactive(\"acc_large_grain\", exp=[\"grain\"]).properties(\n",
    "    title=\"Large Grain Response\"\n",
    ")\n",
    "grain_plot = (small | large).properties(\n",
    "    title=\"Accuracy of Grain by response and condition\"\n",
    ")\n",
    "grain_plot.save(cfg.path_plot_folder + \"development_grain_by_response.html\")\n",
    "if show_plots_in_notebook:\n",
    "    grain_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words vs. Nonwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnw_zr = vis.plot_wnw([\"INC_HF\"], [\"unambiguous\"]).properties(\n",
    "    title=\"Strain (INC_HF) vs. Grain (UN)\"\n",
    ")\n",
    "\n",
    "all_taraban_conds = list(vis.cdf.loc[vis.cdf.exp == \"taraban\", \"cond\"].unique())\n",
    "wnw_tg = vis.plot_wnw(all_taraban_conds, [\"Exception\", \"Regular\"]).properties(\n",
    "    title=\"Taraban (all) vs. Glushko (NW)\"\n",
    ")\n",
    "wnw_plot = wnw_zr | wnw_tg\n",
    "wnw_plot.save(cfg.path_plot_folder + \"wnw.html\")\n",
    "\n",
    "if show_plots_in_notebook:\n",
    "    wnw_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.weight(cfg.path_weights_list[-1]).violinplot(\n",
    "    cfg.path_plot_folder + \"weight_violin.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate.weight(cfg.path_weights_list[-1]).heatmap(\n",
    "    cfg.path_plot_folder + \"weight_heatmap.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir $cfg.path_model_folder --to html_toc OSP_master.ipynb\n",
    "# !tensorboard --logdir tensor_board_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "basicO2P_master.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
