{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbgouQwZ1Y1f"
   },
   "source": [
    "# O2S model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters block for Papermill\n",
    "- Instead of using model_cfg directly, this extra step is needed for batch run using Papermill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = 'O2S_bert_H500_L1_S10M'\n",
    "\n",
    "embedding = 'bert'\n",
    "sample_name = 'hal'  #hal log frequency with clipping like HS04 (did not use it for control...)\n",
    "sample_rng_seed = 1234\n",
    "tf_rng_seed = 4321\n",
    "\n",
    "# Model architechture\n",
    "o_input_dim = 119\n",
    "hidden_units = 500\n",
    "cleanup_units = 50  # useless\n",
    "rnn_activation = 'sigmoid'\n",
    "regularizer_const = 0.\n",
    "\n",
    "p_noise = 0.  # i.e. w_pp, w_pc, and w_cp noise\n",
    "tau = 1.\n",
    "max_unit_time = 2.\n",
    "\n",
    "# Training\n",
    "n_mil_sample = 10.\n",
    "batch_size = 128\n",
    "learning_rate = 0.005\n",
    "save_freq = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_name = 'x_train_{}.npz'.format(embedding)\n",
    "y_name = 'y_train_{}.npz'.format(embedding)\n",
    "csv_name = 'df_train_{}.csv'.format(embedding)\n",
    "\n",
    "if embedding == 'tasa':\n",
    "    sem_units = 300\n",
    "if embedding == 'bert':\n",
    "    sem_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packing parameters into model_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meta import model_cfg\n",
    "\n",
    "cfg = model_cfg(\n",
    "    code_name=code_name,\n",
    "    x_name=x_name,\n",
    "    y_name=y_name,\n",
    "    csv_name=csv_name,\n",
    "    sample_name=sample_name,\n",
    "    sample_rng_seed=sample_rng_seed,\n",
    "    tf_rng_seed=tf_rng_seed,\n",
    "    use_semantic=False,\n",
    "    sem_param_gf=0,\n",
    "    sem_param_gi=0,\n",
    "    sem_param_kf=0,\n",
    "    sem_param_ki=0,\n",
    "    sem_param_hf=0,\n",
    "    sem_param_hi=0,\n",
    "    o_input_dim=o_input_dim,\n",
    "    hidden_units=hidden_units,\n",
    "    pho_units=sem_units,  # Output become semantic embedding vector\n",
    "    cleanup_units=None,\n",
    "    embed_attractor_cfg=None,\n",
    "    embed_attractor_h5=None,\n",
    "    w_oh_noise=0.,\n",
    "    w_hp_noise=0.,\n",
    "    w_pp_noise=p_noise,\n",
    "    w_pc_noise=p_noise,\n",
    "    w_cp_noise=p_noise,\n",
    "    tau=tau,\n",
    "    max_unit_time=max_unit_time,\n",
    "    n_mil_sample=n_mil_sample,\n",
    "    batch_size=batch_size,\n",
    "    rnn_activation=rnn_activation,\n",
    "    regularizer_const=regularizer_const,\n",
    "    learning_rate=learning_rate,\n",
    "    save_freq=save_freq,\n",
    "    bq_dataset=None\n",
    ")\n",
    "\n",
    "# TF random seed (Sampling is out of TF scope... change sample_rng_seed instead)\n",
    "tf.random.set_seed(cfg.tf_rng_seed)\n",
    "\n",
    "# Preload data\n",
    "from data_wrangling import sample_generator, my_data\n",
    "data = my_data(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IwxbE29_0yx"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFTsfceXUGIh",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(training=True):\n",
    "    # Organization principal:\n",
    "    # Structure things, such as repeat vector should build within the model\n",
    "    # Static calculation of input --> Easier to modify --> build within sample generator\n",
    "\n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.layers import Layer, Input, concatenate, multiply, RepeatVector, Dense\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "    from modeling import rnn\n",
    "    #     from modeling_without_cleanup import rnn_no_cleanup_no_pp\n",
    "\n",
    "    # Train/test mode checking\n",
    "    cfg.noise_on() if training is True else cfg.noise_off()\n",
    "\n",
    "    input_o = Input(shape=(cfg.o_input_dim, ), name=\"Orthography\")\n",
    "    hidden1 = Dense(\n",
    "        cfg.hidden_units, name=\"Hidden1\", activation=cfg.rnn_activation\n",
    "    )(input_o)\n",
    "    #     hidden2 = Dense(cfg.hidden_units, name=\"Hidden2\",\n",
    "    #                     activation='sigmoid')(hidden1)\n",
    "    #     hidden3 = Dense(cfg.hidden_units, name=\"Hidden3\",\n",
    "    #                     activation='sigmoid')(hidden2)\n",
    "    output = Dense(cfg.pho_units, name=\"Semantics\")(\n",
    "        hidden1\n",
    "    )  # Should rename pho_unit to output unit later... a bit confusing\n",
    "    model = Model(input_o, output)\n",
    "\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=Adam(\n",
    "            learning_rate=cfg.learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            amsgrad=False\n",
    "        ),\n",
    "        metrics=['accuracy', 'mse']\n",
    "    )\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKUOoZkP8QiA"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, pickle, os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from data_wrangling import sample_generator\n",
    "from IPython.display import clear_output\n",
    "\n",
    "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    cfg.path_weights_checkpoint,\n",
    "    verbose=1,\n",
    "    save_freq=cfg.save_freq_sample,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    sample_generator(cfg, data),\n",
    "    steps_per_epoch=cfg.steps_per_epoch,\n",
    "    epochs=cfg.nEpo,\n",
    "    verbose=2,\n",
    "    callbacks=[checkpoint],\n",
    "    #     callbacks=[checkpoint, es],\n",
    ")\n",
    "\n",
    "# Saving history and model\n",
    "pickle_out = open(cfg.path_history_pickle, \"wb\")\n",
    "pickle.dump(history.history, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "clear_output()\n",
    "print('Training done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMmNcbJdcPMh"
   },
   "source": [
    "\n",
    "# Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BqpDOQStugK"
   },
   "source": [
    "### Training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import training_history\n",
    "\n",
    "hist = training_history(cfg.path_history_pickle)\n",
    "hist.plot_mse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse item level stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import eval_O2S\n",
    "model = build_model(training=False)\n",
    "model.load_weights(cfg.path_weight_folder + 'ep1000.h5')\n",
    "eval_embeddings = eval_O2S(model, data)\n",
    "eval_embeddings.df.to_csv(cfg.path_model_folder + 'results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_embeddings.plot_heatmap(save_file=cfg.path_plot_folder + 'heatmap.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_embeddings.plot_scatter(\n",
    "    'model_mse', 'lwf', save_file=cfg.path_plot_folder + 'scatter_lwf.html'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import plot_variables\n",
    "plot_variables(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir=$cfg.path_model_folder --to html basicO2S_master.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "basicO2P_master.ipynb",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
