{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "%%writefile data_wrangling.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def gen_pkey(p_file=\"../common/patterns/mappingv2.txt\"):\n",
    "    \"\"\"\n",
    "    Read phonological patterns from the mapping file\n",
    "    See Harm & Seidenberg PDF file\n",
    "    \"\"\"\n",
    "\n",
    "    mapping = pd.read_table(p_file, header=None, delim_whitespace=True)\n",
    "    m_dict = mapping.set_index(0).T.to_dict('list')\n",
    "    return m_dict\n",
    "\n",
    "\n",
    "class wf_manager():\n",
    "    # Note: the probability must sum to 1 when passing it to np.random.choice()\n",
    "    def __init__(self, wf):\n",
    "\n",
    "        self.wf = np.array(wf)\n",
    "\n",
    "    def wf(self):\n",
    "        return self.wf\n",
    "\n",
    "    def to_p(self, x):\n",
    "        return x / np.sum(x)\n",
    "\n",
    "    def samp_termf(self):\n",
    "        return self.to_p(self.wf)\n",
    "\n",
    "    def samp_log(self):\n",
    "        log = np.log(1 + self.wf)\n",
    "        return self.to_p(log)\n",
    "\n",
    "    def samp_hs04(self):\n",
    "        root = np.sqrt(self.wf) / np.sqrt(30000)\n",
    "        clip = root.clip(0.05, 1.0)\n",
    "        return self.to_p(clip)\n",
    "\n",
    "    def samp_jay(self):\n",
    "        cap = self.wf.clip(0, 10000)\n",
    "        root = np.sqrt(cap)\n",
    "        return self.to_p(root)\n",
    "    \n",
    "    def samp_fromhallf_clip(self):\n",
    "        ratio = self.wf / np.log(30000)  # Apply hs04-like sampling stregegy\n",
    "        clip = ratio.clip(0.05, 1.0)                  \n",
    "        return self.to_p(clip)\n",
    "\n",
    "# Input for training set\n",
    "def sample_generator(cfg, data):\n",
    "    # Dimension guide: (batch_size, timesteps, nodes)\n",
    "    from modeling import input_s\n",
    "\n",
    "    np.random.seed(cfg.sample_rng_seed)\n",
    "    epoch = 0\n",
    "    batch = 0\n",
    "\n",
    "    while True:\n",
    "        batch += 1\n",
    "        idx = np.random.choice(\n",
    "            range(len(data.sample_p)), cfg.batch_size, p=data.sample_p\n",
    "        )\n",
    "\n",
    "        # Preallocate for easier indexing: (batch_size, time_step, input_dim)\n",
    "        batch_s = np.zeros((cfg.batch_size, cfg.n_timesteps, cfg.pho_units))\n",
    "        \n",
    "        batch_y = []\n",
    "        batch_y = data.y_train[idx]\n",
    "\n",
    "#         for t in range(cfg.n_timesteps):\n",
    "#             batch_y.append(data.y_train[idx])\n",
    "\n",
    "        if batch % cfg.steps_per_epoch == 0:\n",
    "            epoch += 1  # Counting epoch for ramping up input S\n",
    "\n",
    "        if cfg.use_semantic == True:\n",
    "            yield (\n",
    "                [data.x_train[idx], batch_s, 2 * data.y_train[idx] - 1], batch_y\n",
    "            )  # With negative input signal\n",
    "        else:\n",
    "            yield (data.x_train[idx], batch_y)\n",
    "\n",
    "\n",
    "def test_set_input(\n",
    "    x_test, x_test_wf, x_test_img, y_test, epoch, cfg, test_use_semantic\n",
    "):\n",
    "    # We need to separate whether the model use semantic by cfg.use_semantic\n",
    "    # And whether the test set has semantic input by test_use_semantic\n",
    "    # If model use semantic, we need to return a list of 3 inputs (x, s[time step varying], y), otherwise (x) is enough\n",
    "    from modeling import input_s\n",
    "\n",
    "    if cfg.use_semantic:\n",
    "        batch_s = np.zeros(\n",
    "            (len(x_test), cfg.n_timesteps, cfg.pho_units)\n",
    "        )  # Fill batch_s with Plaut S formula\n",
    "        batch_y = np.zeros_like(y_test)\n",
    "\n",
    "        if test_use_semantic:\n",
    "            for t in range(cfg.n_timesteps):\n",
    "                s_cell = input_s(\n",
    "                    e=epoch,\n",
    "                    t=t * cfg.tau,\n",
    "                    f=x_test_wf,\n",
    "                    i=x_test_img,\n",
    "                    gf=cfg.sem_param_gf,\n",
    "                    gi=cfg.sem_param_gi,\n",
    "                    kf=cfg.sem_param_kf,\n",
    "                    ki=cfg.sem_param_ki,\n",
    "                    hf=cfg.sem_param_hf,\n",
    "                    hi=cfg.sem_param_hi,\n",
    "                    tmax=cfg.max_unit_time - cfg.tau\n",
    "                )  # Because of zero indexing\n",
    "                batch_s[:, t, :] = np.tile(\n",
    "                    np.expand_dims(s_cell, 1), [1, cfg.pho_units]\n",
    "                )\n",
    "\n",
    "            batch_y = 2 * y_test - 1  # With negative teaching signal\n",
    "\n",
    "        return [x_test, batch_s, batch_y]  # With negative teaching signal\n",
    "\n",
    "    else:\n",
    "        return x_test\n",
    "\n",
    "\n",
    "class my_data():\n",
    "    def __init__(self, cfg):\n",
    "\n",
    "        self.sample_name = cfg.sample_name\n",
    "        self.csv_name = cfg.csv_name\n",
    "        self.x_name = cfg.x_name\n",
    "        self.y_name = cfg.y_name\n",
    "\n",
    "        input_path = './input/'\n",
    "\n",
    "        self.df_train = pd.read_csv(input_path + self.csv_name, index_col=0)\n",
    "        self.x_train = np.load(input_path + self.x_name)['data']\n",
    "        self.y_train = np.load(input_path + self.y_name)['data']\n",
    "\n",
    "        from data_wrangling import gen_pkey\n",
    "        self.phon_key = gen_pkey()\n",
    "\n",
    "        print('==========Orthographic representation==========')\n",
    "        print('x_train shape:', self.x_train.shape)\n",
    "        print('y_train shape:', self.y_train.shape)\n",
    "        \n",
    "        # Select WF  \n",
    "        self.sample_p = wf_manager(self.df_train.lwf).samp_fromhallf_clip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy source files to keys csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read training file\n",
    "train_file = '../common/patterns/6ktraining_v2.dict'\n",
    "\n",
    "strain_file = '../common/patterns/strain.txt'\n",
    "strain_key_file = '../common/patterns/strain_key.txt'\n",
    "\n",
    "grain_file = '../common/patterns/grain_nws.dict'\n",
    "grain_key_file = '../common/patterns/grain_key.txt'\n",
    "\n",
    "# Imageability\n",
    "cortese = pd.read_csv('../common/patterns/cortese2004norms.csv', skiprows=9)\n",
    "img_map = cortese[['item', 'rating']]\n",
    "img_map.columns = ['word', 'img']\n",
    "\n",
    "train = pd.read_csv(\n",
    "    train_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho', 'wf'],\n",
    "    na_filter=\n",
    "    False  # Bug fix: incorrectly treated null as missing value in the corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge files from Noam, TASA, and BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "naming = pd.read_csv('from_noam/naming.dol.jason.csv', na_filter=False)\n",
    "naming = naming.rename(\n",
    "    columns={\n",
    "        \"D_word\": \"word\",\n",
    "        \"Log_Freq_HAL\": \"lwf\",\n",
    "        \"mean.acc\": \"mean.acc.naming\",\n",
    "        \"mean.RT\": \"mean.rt.naming\"\n",
    "    }\n",
    ")\n",
    "naming = naming[[\n",
    "    'word', 'lwf', 'mean.cosine', 'mean.cosine.p2s', 'uncond.surprisal',\n",
    "    'neigh.size.plus.1', 'p.neigh.size.plus.1', 'Length', 'mean.acc.naming',\n",
    "    'mean.rt.naming'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexd = pd.read_csv('from_noam/lexDec.dol.jason.csv', na_filter=False)\n",
    "lexd = lexd.rename(\n",
    "    columns={\n",
    "        \"D_word\": \"word\",\n",
    "        \"mean.acc.LD\": \"mean.acc.ld\",\n",
    "        \"mean.RT.LD\": \"mean.rt.ld\"\n",
    "    }\n",
    ")\n",
    "lexd = lexd[['word', 'mean.acc.ld', 'mean.rt.ld']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['og_id'] = train.index\n",
    "tr_na = train.merge(naming)\n",
    "tr_na_ld = tr_na.merge(lexd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASA Cleanup\n",
    "\n",
    "tasa = pd.read_csv('embeddings/LSA_TASA/TASA.csv')\n",
    "tasa['tasa_id'] = tasa.index\n",
    "tasa = tasa.rename(columns={tasa.columns[0]: 'word'})\n",
    "\n",
    "# Make wordlist for merging\n",
    "tasa_wordlist = tasa.loc[:, ['word', 'tasa_id']]\n",
    "\n",
    "# Actual embedding\n",
    "tasa_embedding = tasa.drop(columns=['word', 'tasa_id'])\n",
    "tasa_embedding = tasa_embedding.to_numpy()\n",
    "\n",
    "# Save for easier loading\n",
    "np.savez_compressed('embeddings/LSA_TASA/TASA.npz', data=tasa_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASA version (n=4396)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master data set\n",
    "tr_na_ld_tasa = tr_na_ld.merge(tasa_wordlist)\n",
    "tr_na_ld_tasa.to_csv('input/df_train_tasa.csv')\n",
    "\n",
    "# x-train\n",
    "x_train = np.load('../common/input/x_train.npz')['data']\n",
    "x_train_tasa = x_train[tr_na_ld_tasa.og_id]\n",
    "np.savez_compressed('input/x_train_tasa.npz', data=x_train_tasa)\n",
    "\n",
    "# y-train\n",
    "tasa = np.load('embeddings/LSA_TASA/TASA.npz')['data']\n",
    "y_train_tasa = tasa[tr_na_ld_tasa.tasa_id]\n",
    "np.savez_compressed('input/y_train_tasa.npz', data=y_train_tasa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT version (n=3262)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master data set\n",
    "bert_words = pd.read_csv(\n",
    "    'embeddings/bert/bert_wordlist.txt', header=None, names=['word']\n",
    ")\n",
    "bert_words['bert_id'] = bert_words.index\n",
    "tr_na_ld_bert = tr_na_ld.merge(bert_words)\n",
    "tr_na_ld_bert.to_csv('input/df_train_bert.csv')\n",
    "\n",
    "# x-train\n",
    "x_train = np.load('../common/input/x_train.npz')['data']\n",
    "x_train_bert = x_train[tr_na_ld_bert.og_id]\n",
    "np.savez_compressed('input/x_train_bert.npz', data=x_train_bert)\n",
    "\n",
    "# y-train\n",
    "bert = np.load('embeddings/bert/bert.npz')['data']\n",
    "y_train_bert = bert[tr_na_ld_bert.bert_id]\n",
    "np.savez_compressed('input/y_train_bert.npz', data=y_train_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Older compiling scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "df_train = pd.merge(train, img_map, on='word', how='left')\n",
    "\n",
    "strain = pd.read_csv(\n",
    "    strain_file, sep='\\t', header=None, names=['word', 'ort', 'pho', 'wf']\n",
    ")\n",
    "\n",
    "strain_key = pd.read_table(\n",
    "    strain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'frequency', 'pho_consistency', 'imageability']\n",
    ")\n",
    "\n",
    "df_strain = pd.merge(strain, strain_key)\n",
    "df_strain = pd.merge(df_strain, img_map, on='word', how='left')\n",
    "\n",
    "grain = pd.read_csv(\n",
    "    grain_file,\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=['word', 'ort', 'pho_large', 'pho_small']\n",
    ")\n",
    "\n",
    "grain_key = pd.read_table(\n",
    "    grain_key_file,\n",
    "    header=None,\n",
    "    delim_whitespace=True,\n",
    "    names=['word', 'condition']\n",
    ")\n",
    "\n",
    "grain_key['condition'] = np.where(\n",
    "    grain_key['condition'] == 'critical', 'ambiguous', 'unambiguous'\n",
    ")\n",
    "\n",
    "df_grain = pd.merge(grain, grain_key)\n",
    "\n",
    "df_grain['img'] = 0\n",
    "df_grain['wf'] = 0\n",
    "\n",
    "\n",
    "def prepDF(t):\n",
    "    # The first bit and last 3 bits are empty in this source dataset (6ktraining.dict)\n",
    "    t['ort'] = t.ort.apply(lambda x: x[1:11])\n",
    "    return t\n",
    "\n",
    "\n",
    "df_train = prepDF(df_train)\n",
    "df_strain = prepDF(df_strain)\n",
    "df_grain = prepDF(df_grain)\n",
    "\n",
    "# Fill missing value to mean img rating\n",
    "mean_img = df_train.img.mean()\n",
    "df_train = df_train.fillna(mean_img)\n",
    "df_strain = df_strain.fillna(mean_img)\n",
    "\n",
    "df_train.to_csv('../common/input/df_train.csv')\n",
    "df_strain.to_csv('../common/input/df_strain.csv')\n",
    "df_grain.to_csv('../common/input/df_grain.csv')\n",
    "\n",
    "print(df_train.head)\n",
    "print(df_strain.head)\n",
    "print(df_grain.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Encode orthographic representation\n",
    "def ort2bin(o_col, trimMode=True, verbose=True):\n",
    "    # Replicating support.py (o_char)\n",
    "    # This function wrap tokenizer.texts_to_matrix to fit on multiple\n",
    "    # independent slot-based input\n",
    "    # i.e. one-hot encoding per each slot with independent dictionary\n",
    "\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "    nSlot = len(o_col[0])\n",
    "    nWord = len(o_col)\n",
    "\n",
    "    slotData = nWord * [None]\n",
    "    binData = pd.DataFrame()\n",
    "\n",
    "    for slotId in range(nSlot):\n",
    "        for wordId in range(nWord):\n",
    "            slotData[wordId] = o_col[wordId][slotId]\n",
    "\n",
    "        t = Tokenizer(filters='', lower=False)\n",
    "        t.fit_on_texts(slotData)\n",
    "        seqData = t.texts_to_sequences(\n",
    "            slotData\n",
    "        )  # Maybe just use sequence data later\n",
    "\n",
    "        # Triming first bit in each slot\n",
    "        if trimMode == True:\n",
    "            tmp = t.texts_to_matrix(slotData)\n",
    "            thisSlotBinData = tmp[:, 1::\n",
    "                                 ]  # Remove the first bit which indicate a separate slot (probably useful in recurrent network)\n",
    "        elif trimMode == False:\n",
    "            thisSlotBinData = t.texts_to_matrix(slotData)\n",
    "\n",
    "        # Print dictionary details\n",
    "        if verbose == True:\n",
    "            print('In slot ', slotId, '\\t')\n",
    "            print('token count:', t.word_counts)\n",
    "            print('word count:', t.document_count)\n",
    "            print('dictionary:', t.word_index)\n",
    "            print('token appear in how many words:', t.word_docs)\n",
    "\n",
    "        # Put binary data into a dataframe\n",
    "        binData = pd.concat(\n",
    "            [binData, pd.DataFrame(thisSlotBinData)], axis=1, ignore_index=True\n",
    "        )\n",
    "\n",
    "    return binData\n",
    "\n",
    "\n",
    "def ort2bin_v2(o_col):\n",
    "    # Use tokenizer instead to acheive same thing, but with extra zeros columns\n",
    "    # Will be useful for letter level recurrent model\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(filters='', lower=False, char_level=True)\n",
    "    t.fit_on_texts(o_col)\n",
    "    print('dictionary:', t.word_index)\n",
    "    return t.texts_to_matrix(o_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlVWxyFWHNNN"
   },
   "outputs": [],
   "source": [
    "# Merge all 3 ortho representation\n",
    "all_ort = pd.concat(\n",
    "    [df_train.ort, df_strain.ort, df_grain.ort], ignore_index=True\n",
    ")\n",
    "\n",
    "# Encoding orthographic representation\n",
    "all_ort_bin = ort2bin(all_ort, verbose=False)\n",
    "splitId_strain = len(df_train)\n",
    "splitId_grain = len(df_train) + len(df_strain)\n",
    "\n",
    "x_train = np.array(all_ort_bin[0:splitId_strain])\n",
    "x_strain = np.array(all_ort_bin[splitId_strain:splitId_grain])\n",
    "x_grain = np.array(all_ort_bin[splitId_grain::])\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/x_train.npz', data=x_train)\n",
    "np.savez_compressed('../common/input/x_strain.npz', data=x_strain)\n",
    "np.savez_compressed('../common/input/x_grain.npz', data=x_grain)\n",
    "\n",
    "print('==========Orthographic representation==========')\n",
    "print('all shape:', all_ort_bin.shape)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_strain shape:', x_strain.shape)\n",
    "print('x_grain shape:', x_grain.shape)\n",
    "\n",
    "\n",
    "def pho2bin_v2(p_col, p_key):\n",
    "    # Vectorize for performance (that no one ask for... )\n",
    "    binLength = len(p_key['_'])\n",
    "    n = len(p_col)\n",
    "    nPhoChar = len(p_col[0])\n",
    "\n",
    "    p_output = np.empty([n, binLength * nPhoChar])\n",
    "\n",
    "    for slot in range(len(p_col[0])):\n",
    "        slotSeries = p_col.str.slice(start=slot, stop=slot + 1)\n",
    "        outSeries = slotSeries.map(p_key)\n",
    "        p_output[:, range(slot * 25, (slot + 1) * 25)] = outSeries.to_list()\n",
    "    return p_output\n",
    "\n",
    "\n",
    "from data_wrangling import gen_pkey\n",
    "phon_key = gen_pkey()\n",
    "y_train = pho2bin_v2(train.pho, phon_key)\n",
    "y_strain = pho2bin_v2(strain.pho, phon_key)\n",
    "y_large_grain = pho2bin_v2(grain.pho_large, phon_key)\n",
    "y_small_grain = pho2bin_v2(grain.pho_small, phon_key)\n",
    "\n",
    "# Save to disk\n",
    "np.savez_compressed('../common/input/y_train.npz', data=y_train)\n",
    "np.savez_compressed('../common/input/y_strain.npz', data=y_strain)\n",
    "np.savez_compressed('../common/input/y_large_grain.npz', data=y_large_grain)\n",
    "np.savez_compressed('../common/input/y_small_grain.npz', data=y_small_grain)\n",
    "\n",
    "print('\\n==========Phonological representation==========')\n",
    "print(len(phon_key), ' phonemes: ', phon_key.keys())\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_strain shape:', y_strain.shape)\n",
    "print('y_large_grain shape:', y_large_grain.shape)\n",
    "print('y_small_grain shape:', y_small_grain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "db82aUKwQTxk"
   },
   "source": [
    "# Testing and evaluating new sampling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SZ5NSFbVHuch"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_csv('../common/input/df_train.csv', index_col=0)\n",
    "\n",
    "# Plot sampling conversion graph\n",
    "import matplotlib.pyplot as plt\n",
    "from data_wrangling import wf_manager\n",
    "\n",
    "plot_f = df_train.sort_values('wf')\n",
    "sortwf = wf_manager(plot_f['wf'])\n",
    "\n",
    "fig, ax = plt.subplots(facecolor=\"w\")\n",
    "line1, = ax.plot(sortwf.wf, sortwf.samp_log(), label='HS04')\n",
    "line2, = ax.plot(sortwf.wf, sortwf.samp_hs04(), label='JAY')\n",
    "line3, = ax.plot(sortwf.wf, sortwf.samp_jay(), label='LOG')\n",
    "ax.legend(loc='lower right')\n",
    "plt.xlabel('Word frequency')\n",
    "plt.ylabel('Sampling probability')\n",
    "# plt.xlim([0,100])\n",
    "plt.title('Tested sampling p vs. word frequency')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMKrJRa8jqmw3arQb7qw4Ja",
   "collapsed_sections": [],
   "name": "data_wrangling.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
