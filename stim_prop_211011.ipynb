{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine stimulus properties effects\n",
    "On PHO:\n",
    "- F + OP + IMG + OP x F + OP x IMG + F x IMG \n",
    "\n",
    "On SEM:\n",
    "- IMG x F\n",
    "\n",
    "Steps:\n",
    "1. Get output at tick 12\n",
    "2. Run lm on each rng_seed\n",
    "    - Logistic regression for accuracy\n",
    "    - Linear regression for SSE\n",
    "3. Extract all betas\n",
    "4. Average the betas over rng_seed\n",
    "5. Plot developmental and performance space\n",
    "    - add zero horizontal line\n",
    "    - add epoch info\n",
    "    - add sem in pho output plot, vice versa...\n",
    "6. Make interactive heat if I have enough time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "%load_ext google.cloud.bigquery\n",
    "import sqlite3\n",
    "import json\n",
    "import meta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tidy and visualize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats.mstats import zscore\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get PHO beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery df\n",
    "# SELECT\n",
    "#   code_name, epoch, word, acc, sse \n",
    "# FROM\n",
    "#   `majestic-camp-303620.station_3.train`\n",
    "# WHERE\n",
    "#   timetick = 12\n",
    "#   AND output_name = 'pho'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"models/station_3/pho_lasttick.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"models/station_3/pho_lasttick.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = \"models/station_3/batch_config.json\"\n",
    "\n",
    "with open(json_file) as f:\n",
    "    batch_cfgs = json.load(f)\n",
    "\n",
    "all_params = [pd.DataFrame(cfg[\"params\"]) for cfg in batch_cfgs if type(cfg[\"params\"].values()) is not list]\n",
    "cfgs = pd.concat(all_params, ignore_index=True)\n",
    "cfgs = cfgs.groupby(['code_name', 'batch_size', 'learning_rate']).mean().reset_index()\n",
    "cfgs = cfgs[['code_name', 'batch_size', 'learning_rate', 'rng_seed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surprisal = pd.read_csv(\"/home/jupyter/triangle_model/corpus/noam_surprisal.csv\")\n",
    "word2op_dict = {word: op for word, op in zip(surprisal.word, surprisal[\"uncond.surprisal\"])}\n",
    "\n",
    "df_train = pd.read_csv(\"/home/jupyter/triangle_model/dataset/df_train.csv\")\n",
    "word2wf_dict = {word: wf for word, wf in zip(df_train.word, df_train.wf)}\n",
    "\n",
    "img_replacement_value = df_train.img[0] # Mean replacement in the dataset, get rid of it. \n",
    "word2img_dict = {word: img for word, img in zip(df_train.word, df_train.img) if not img == img_replacement_value}\n",
    "\n",
    "\n",
    "def word2op(word):\n",
    "    try:\n",
    "        return word2op_dict[word]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def word2wf(word):\n",
    "    try:\n",
    "        return np.log10(word2wf_dict[word] + 1)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def word2img(word):\n",
    "    try:\n",
    "        return word2img_dict[word]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "selected_words = set.intersection(set(word2op_dict.keys()), set(word2wf_dict.keys()), set(word2img_dict.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine correlations between stimulus properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_df = df_train.loc[df_train.word.isin(selected_words)]\n",
    "cor_df = cor_df[['word', 'wf', 'img']].copy()\n",
    "cor_df['op'] = cor_df.word.apply(word2op)\n",
    "cor_df['lwf'] = cor_df.wf.apply(lambda x: np.log10(x + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cor_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_df[['lwf', 'op', 'img']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate csse\n",
    "df = df[df.word.isin(selected_words)]\n",
    "\n",
    "df['csse'] = df.sse.loc[df.acc == 1]\n",
    "\n",
    "# Get wf and op for each word\n",
    "df['wf'] = df.word.apply(word2wf)\n",
    "df['op'] = df.word.apply(word2op)\n",
    "df['img'] = df.word.apply(word2img)\n",
    "\n",
    "# Get batch size and learning rate\n",
    "df = df.merge(cfgs, on=['code_name'], how='left')\n",
    "\n",
    "# checkpoint\n",
    "df.to_csv(\"models/station_3/parsed_pho_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pho_beta(df: pd.DataFrame, code_name:str, epoch:int, metric:str) -> pd.DataFrame:\n",
    "    \"\"\"Run one GLM and get one row of beta\"\"\"\n",
    "    sdf = df.loc[(df.epoch == epoch) & (df.code_name == code_name)]\n",
    "    batch_size = sdf.batch_size.unique()[0]\n",
    "    learning_rate = sdf.learning_rate.unique()[0]\n",
    "\n",
    "    assert metric in ('acc', 'sse', 'csse')\n",
    "    sdf = sdf[['word', metric, 'op', 'wf', 'img']].dropna()\n",
    "\n",
    "    try:\n",
    "        rhs = \"zscore(op) * zscore(wf) + zscore(op) * zscore(img) + zscore(wf) * zscore(img) + 0\"\n",
    "\n",
    "        if metric == 'acc':\n",
    "            m = smf.glm(formula=f\"acc ~ {rhs}\", family=sm.families.Binomial(), data=sdf).fit()\n",
    "        else:\n",
    "            m = smf.glm(formula=f\"zscore(csse) ~ {rhs}\", data=sdf).fit()\n",
    "\n",
    "        p = m.params\n",
    "        p['epoch'] = epoch\n",
    "        p['code_name'] = code_name\n",
    "        p['batch_size'] = batch_size\n",
    "        p['learning_rate'] = learning_rate\n",
    "        p['metric'] = metric\n",
    "\n",
    "        return pd.DataFrame(p).T\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_beta_df(df, func, acc_label:str):\n",
    "    \"\"\"Make a dataframe of all the betas in each code_name, epoch, and metric (acc, csse)\n",
    "    df: item level raw data dataframe\n",
    "    func: function to get the beta for each row (e.g., get_pho_beta, get_sem_beta)\n",
    "    acc_label: label for the acc column (mean accuracy at a given epoch)\n",
    "    \"\"\"\n",
    "    \n",
    "    epoch_acc_map = df.groupby(['code_name', 'epoch']).mean().reset_index()[['code_name', 'epoch', 'acc']]\n",
    "    epoch_acc_map.columns = ['code_name', 'epoch', acc_label]\n",
    "\n",
    "    code_names = sorted(df.code_name.unique())\n",
    "    epochs = sorted(df.epoch.unique())\n",
    "    metrics = ['acc', 'csse']\n",
    "\n",
    "    # Do the job\n",
    "    beta_df = pd.concat([func(df, code_name, epoch, metric) for code_name in tqdm(code_names) for epoch in epochs for metric in metrics], ignore_index=True)\n",
    "\n",
    "    beta_df = beta_df.melt(id_vars=['code_name', 'epoch', 'batch_size', 'learning_rate', 'metric'], var_name='param', value_name='beta')\n",
    "    beta_df = pd.merge(beta_df, epoch_acc_map, on=['code_name', 'epoch'], how='left').dropna()\n",
    "\n",
    "    return beta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_beta = make_beta_df(df, get_pho_beta, acc_label='pho_acc')\n",
    "pho_beta.to_csv(\"models/station_3/pho_beta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get SEM betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bigquery df\n",
    "# SELECT\n",
    "#   code_name, epoch, word, acc, sse \n",
    "# FROM\n",
    "#   `majestic-camp-303620.station_3.train`\n",
    "# WHERE\n",
    "#   timetick = 12\n",
    "#   AND output_name = 'sem'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"models/station_3/sem_lasttick.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate csse\n",
    "df = df[df.word.isin(selected_words)]\n",
    "\n",
    "df['csse'] = df.sse.loc[df.acc == 1]\n",
    "\n",
    "# Get wf and op for each word\n",
    "df['wf'] = df.word.apply(word2wf)\n",
    "df['op'] = df.word.apply(word2op)\n",
    "df['img'] = df.word.apply(word2img)\n",
    "\n",
    "# Get batch size and learning rate\n",
    "df = df.merge(cfgs, on=['code_name'], how='left')\n",
    "\n",
    "# checkpoint\n",
    "df.to_csv(\"models/station_3/parsed_sem_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sem_beta(df: pd.DataFrame, code_name:str, epoch:int, metric:str) -> pd.DataFrame:\n",
    "    \"\"\"Run one GLM and get one row of beta\"\"\"\n",
    "    sdf = df.loc[(df.epoch == epoch) & (df.code_name == code_name)]\n",
    "    batch_size = sdf.batch_size.unique()[0]\n",
    "    learning_rate = sdf.learning_rate.unique()[0]\n",
    "\n",
    "    assert metric in ('acc', 'sse', 'csse')\n",
    "    sdf = sdf[['word', metric, 'op', 'wf', 'img']].dropna()\n",
    "\n",
    "    try:\n",
    "        rhs = \"zscore(wf) * zscore(img) + 0\"\n",
    "\n",
    "        if metric == 'acc':\n",
    "            m = smf.glm(formula=f\"acc ~ {rhs}\", family=sm.families.Binomial(), data=sdf).fit()\n",
    "        else:\n",
    "            m = smf.glm(formula=f\"zscore(csse) ~ {rhs}\", data=sdf).fit()\n",
    "\n",
    "        p = m.params\n",
    "        p['epoch'] = epoch\n",
    "        p['code_name'] = code_name\n",
    "        p['batch_size'] = batch_size\n",
    "        p['learning_rate'] = learning_rate\n",
    "        p['metric'] = metric\n",
    "\n",
    "        return pd.DataFrame(p).T\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_beta = make_beta_df(df, get_sem_beta)\n",
    "sem_beta.to_csv(\"models/station_3/sem_beta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exchange mean accuracy between PHO and SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_beta = pd.read_csv(\"models/station_3/sem_beta.csv\", index_col=0)\n",
    "pho_beta = pd.read_csv(\"models/station_3/pho_beta.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_acc_map = sem_beta.groupby(['code_name', 'epoch']).mean().reset_index()[['code_name', 'epoch', 'sem_acc']]\n",
    "pho_acc_map = pho_beta.groupby(['code_name', 'epoch']).mean().reset_index()[['code_name', 'epoch', 'pho_acc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_beta = pho_beta.merge(sem_acc_map, on=['code_name', 'epoch'], how='left')\n",
    "sem_beta = sem_beta.merge(pho_acc_map, on=['code_name', 'epoch'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_beta.to_csv(\"models/station_3/pho_beta.csv\")\n",
    "sem_beta.to_csv(\"models/station_3/sem_beta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beta(df, x:str, metric:str, additional_acc: str):\n",
    "    \"\"\"Plot beta and save developmental and performance space.\"\"\"\n",
    "    df = df.loc[(df.metric == metric)]\n",
    "\n",
    "    selection = alt.selection_multi(fields=['param'], bind='legend')\n",
    "\n",
    "    # Line of betas\n",
    "    b = alt.Chart().mark_line(point=True).encode(\n",
    "        x=f\"{x}:Q\",\n",
    "        y=\"beta:Q\",\n",
    "        color=\"param:N\",\n",
    "        opacity=alt.condition(selection, alt.value(1), alt.value(0.))\n",
    "    ).add_selection(selection)\n",
    "\n",
    "    # Line of additional accuracy\n",
    "    a = alt.Chart().mark_line(color='black').encode(\n",
    "        x=f\"{x}:Q\",\n",
    "        y=f\"mean({additional_acc}):Q\",\n",
    "    )\n",
    "    \n",
    "    # Color point to indicate 50 epoch \n",
    "    p = (\n",
    "        alt.Chart()\n",
    "        .transform_filter(alt.datum.epoch == 50)\n",
    "        .mark_rule(color='red')\n",
    "        .encode(x=f\"{x}:Q\")\n",
    "    )\n",
    "\n",
    "    # h-line for easier reference\n",
    "    l = alt.Chart().mark_rule().encode(y='zero:Q')\n",
    "\n",
    "    return (\n",
    "        alt.layer(l, b, p, a, data=df)\n",
    "        .transform_calculate(zero=\"0\")\n",
    "        .facet(row=\"batch_size:O\", column=\"learning_rate:O\")\n",
    "        .interactive()\n",
    "    ).properties(title=f\"{metric}_by_{x}. Red vertical line indicate epoch == 50\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot PHO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pho_beta = pd.read_csv(\"models/station_3/pho_beta.csv\", index_col=0)\n",
    "pho_beta = pho_beta.groupby(['epoch', 'batch_size', 'learning_rate', 'metric', 'param']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta(pho_beta, x='epoch', metric='acc', additional_acc='sem_acc').save(\"models/station_3/pho_beta_dev_acc.html\")\n",
    "plot_beta(pho_beta, x='epoch', metric='csse', additional_acc='sem_acc').save(\"models/station_3/pho_beta_dev_csse.html\")\n",
    "plot_beta(pho_beta, x='pho_acc', metric='acc', additional_acc='sem_acc').save(\"models/station_3/pho_beta_per_acc.html\")\n",
    "plot_beta(pho_beta, x='pho_acc', metric='csse', additional_acc='sem_acc').save(\"models/station_3/pho_beta_per_csse.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_beta = pd.read_csv(\"models/station_3/sem_beta.csv\", index_col=0)\n",
    "sem_beta = sem_beta.groupby(['epoch', 'batch_size', 'learning_rate', 'metric', 'param']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_beta(sem_beta, x='epoch', metric='acc', additional_acc='pho_acc').save(\"models/station_3/sem_beta_dev_acc.html\")\n",
    "plot_beta(sem_beta, x='epoch', metric='csse', additional_acc='pho_acc').save(\"models/station_3/sem_beta_dev_csse.html\")\n",
    "plot_beta(sem_beta, x='sem_acc', metric='acc', additional_acc='pho_acc').save(\"models/station_3/sem_beta_per_acc.html\")\n",
    "plot_beta(sem_beta, x='sem_acc', metric='csse', additional_acc='pho_acc').save(\"models/station_3/sem_beta_per_csse.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_name = 'station_3'\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT code_name, batch_size, learning_rate, epoch, acc, sse, csse FROM LEXICALITY\n",
    "WHERE testset = 'glushko' AND timetick = 12\n",
    "\"\"\"\n",
    "\n",
    "with sqlite3.connect(f\"models/{batch_name}/results.db\") as c:\n",
    "    nonword = pd.read_sql(query, con=c)\n",
    "\n",
    "nonword['cond'] = 'nonword'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrow setting from Glushko\n",
    "settings = nonword[[\"code_name\", \"batch_size\", \"learning_rate\"]].groupby(['code_name']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = pd.read_csv(\"models/station_3/pho_lasttick.csv\", index_col=0)\n",
    "word = word.merge(settings, on='code_name', how='left')\n",
    "word['csse'] = word.sse.loc[word.acc == 1]\n",
    "word = word[['code_name', 'batch_size', 'learning_rate', 'epoch', 'acc', 'sse', 'csse']]\n",
    "word['cond'] = 'word'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([word, nonword], axis=0)\n",
    "del word, nonword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby(['code_name', 'batch_size', 'learning_rate', 'epoch', 'cond']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach mean accuracy on word by epoch\n",
    "word_acc = df.loc[df.cond == 'word'].groupby(['code_name', 'epoch']).mean().reset_index()[['code_name', 'epoch', 'acc']]\n",
    "word_acc.columns = ['code_name', 'epoch', 'word_acc']\n",
    "df = df.merge(word_acc, on=['code_name', 'epoch'], how='left')\n",
    "df['lex_num'] = df.cond.apply(lambda x: 1 if x == 'word' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicality over epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(df).mark_line(point=True).encode(\n",
    "    x='epoch:Q',\n",
    "    y='mean(acc):Q',\n",
    "    column='learning_rate:Q',\n",
    "    row='batch_size:Q',\n",
    "    color='cond:N'\n",
    ").save('lexicality_over_epoch_211012.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Betas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lex_beta(df: pd.DataFrame, batch_size:int, learning_rate:float, epoch:int, metric='acc', standardize=False) -> pd.DataFrame:\n",
    "    # Parse the dataframe to get the parameters\n",
    "    df = df.loc[(df.batch_size == batch_size) & (df.learning_rate == learning_rate) & (df.epoch == epoch)]\n",
    "    y = f'zscore({metric})' if standardize else metric  # pick y\n",
    "\n",
    "    try:\n",
    "        m = smf.glm(formula=f\"{y} ~ lex_num\", data=df).fit()\n",
    "        p = m.params\n",
    "        p['batch_size'] = batch_size\n",
    "        p['learning_rate'] = learning_rate\n",
    "        p['epoch'] = epoch\n",
    "        p['word_acc'] = df.word_acc.mean()\n",
    "\n",
    "        return pd.DataFrame(p).T\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(df.epoch.unique())\n",
    "batch_sizes = list(df.batch_size.unique())\n",
    "learning_rates = list(df.learning_rate.unique())\n",
    "\n",
    "def run_lex_dev(metric, standardize):\n",
    "\n",
    "    beta_lex = pd.DataFrame()\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "        for batch_size in batch_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                beta_lex = beta_lex.append(get_lex_beta(df, batch_size, learning_rate, epoch, metric, standardize))\n",
    "\n",
    "    mdf = beta_lex.melt(id_vars=['batch_size', 'learning_rate', 'epoch'], \n",
    "        value_vars=['Intercept', 'lex_num'], var_name='param', value_name='beta')\n",
    "\n",
    "    return alt.Chart(mdf).mark_line(point=True).encode(\n",
    "        y='beta:Q',\n",
    "        x='epoch:Q',\n",
    "        column='learning_rate:O',\n",
    "        row='batch_size:O',\n",
    "        color='param:N'\n",
    "    ).properties(title=f'Beta in Taraban (z:{standardize}, y:{metric}) ')\n",
    "\n",
    "def run_lex_per(metric, standardize):\n",
    "\n",
    "    beta_lex = pd.DataFrame()\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "        for batch_size in batch_sizes:\n",
    "            for learning_rate in learning_rates:\n",
    "                beta_lex = beta_lex.append(get_lex_beta(df, batch_size, learning_rate, epoch, metric, standardize))\n",
    "\n",
    "    mdf = beta_lex.melt(id_vars=['batch_size', 'learning_rate', 'word_acc'], \n",
    "        value_vars=['Intercept', 'lex_num'], var_name='param', value_name='beta')\n",
    "\n",
    "    return alt.Chart(mdf).mark_line(point=True).encode(\n",
    "        y='beta:Q',\n",
    "        x='word_acc:Q',\n",
    "        column='learning_rate:O',\n",
    "        row='batch_size:O',\n",
    "        color='param:N'\n",
    "    ).properties(title=f'Beta in Taraban (z:{standardize}, y:{metric})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lex_dev('acc', False).save('models/station_3/Lex_beta_dev_acc.html')\n",
    "run_lex_dev('csse', True).save('models/station_3/Lex_zbeta_dev_csse.html')\n",
    "run_lex_per('acc', False).save('models/station_3/Lex_beta_per_acc.html')\n",
    "run_lex_per('csse', True).save('models/station_3/Lex_zbeta_per_csse.html')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
