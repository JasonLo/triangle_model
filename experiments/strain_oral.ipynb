{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import sqlite3\n",
    "import os, json\n",
    "import evaluate, meta, modeling, data_wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import TestSet\n",
    "class EvalOral:\n",
    "    \"\"\"Bundle of testsets\"\"\"\n",
    "\n",
    "    TESTSETS_NAME = (\"strain\", \"grain\", \"taraban\")\n",
    "\n",
    "    def __init__(self, cfg, model, data):\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "\n",
    "        self.train_mean_df = None\n",
    "        self.strain_mean_df = None\n",
    "        self.grain_mean_df = None\n",
    "        self.taraban_mean_df = None\n",
    "        self.cortese_mean_df = None\n",
    "        \n",
    "        # Setup database\n",
    "        if self.cfg.batch_name is not None:\n",
    "            \n",
    "            sqlite_file = os.path.join(self.cfg.path[\"batch_folder\"], \"batch_results.sqlite\")\n",
    "            self.con = sqlite3.connect(sqlite_file)\n",
    "            self.cur = self.con.cursor()\n",
    "        \n",
    "        # Load eval results from file\n",
    "        for _testset_name in self.TESTSETS_NAME:\n",
    "            try:\n",
    "                _file = os.path.join(\n",
    "                    self.cfg.path[\"model_folder\"],\n",
    "                    \"eval\",\n",
    "                    f\"{_testset_name}_mean_df.csv\",\n",
    "                )\n",
    "                setattr(self, f\"{_testset_name}_mean_df\", pd.read_csv(_file))\n",
    "            except (FileNotFoundError, IOError):\n",
    "                pass\n",
    "\n",
    "        # Bundle testsets into dictionary\n",
    "        self.run_eval = {\n",
    "            \"strain\": self._eval_strain,\n",
    "        }\n",
    "        \n",
    "    def eval(self, testset_name):\n",
    "        \"\"\"Run eval and push to dat\"\"\"\n",
    "        if getattr(self, f\"{testset_name}_mean_df\") is None:\n",
    "            results = self.run_eval[testset_name]()\n",
    "            try:\n",
    "                results.to_sql(testset_name, self.con, if_exists=\"append\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(\"Evaluation results found, loaded from file.\")\n",
    "\n",
    "\n",
    "    def _eval_strain(self):\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        testsets = (\n",
    "            \"strain_hf_con_hi\",\n",
    "            \"strain_hf_inc_hi\",\n",
    "            \"strain_hf_con_li\",\n",
    "            \"strain_hf_inc_li\",\n",
    "            \"strain_lf_con_hi\",\n",
    "            \"strain_lf_inc_hi\",\n",
    "            \"strain_lf_con_li\",\n",
    "            \"strain_lf_inc_li\"\n",
    "        )\n",
    "\n",
    "        for testset_name in testsets:\n",
    "            t_ps = TestSet(\n",
    "                name=testset_name,\n",
    "                cfg=self.cfg,\n",
    "                model=self.model,\n",
    "                task=\"pho_sem\",\n",
    "                testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                x_test=self.data.testsets[testset_name][\"pho\"],\n",
    "                y_test=self.data.testsets[testset_name][\"sem\"],\n",
    "            )\n",
    "            \n",
    "            t_ps.eval_all()\n",
    "            df = pd.concat([df, t_ps.result])\n",
    "            \n",
    "            t_sp = TestSet(\n",
    "                name=testset_name,\n",
    "                cfg=self.cfg,\n",
    "                model=self.model,\n",
    "                task=\"sem_pho\",\n",
    "                testitems=self.data.testsets[testset_name][\"item\"],\n",
    "                x_test=self.data.testsets[testset_name][\"sem\"],\n",
    "                y_test=self.data.testsets[testset_name][\"pho\"],\n",
    "            )\n",
    "            \n",
    "            t_sp.eval_all()\n",
    "            df = pd.concat([df, t_sp.result])\n",
    "\n",
    "\n",
    "        df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", \"oral_strain_item_df.csv\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Condition level aggregate\n",
    "        mean_df = (\n",
    "            df.groupby(\n",
    "                [\n",
    "                    \"code_name\",\n",
    "                    \"task\",\n",
    "                    \"testset\",\n",
    "                    \"epoch\",\n",
    "                    \"timetick\",\n",
    "                    \"y\",\n",
    "                ]\n",
    "            )\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        mean_df.to_csv(\n",
    "            os.path.join(\n",
    "                self.cfg.path[\"model_folder\"], \"eval\", \"strain_mean_df.csv\"\n",
    "            )\n",
    "        )\n",
    "        self.strain_mean_df = mean_df\n",
    "        \n",
    "        return df\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from /home/jupyter/tf/models/high_stress_long_pretraining/model_config.json\n"
     ]
    }
   ],
   "source": [
    "code_name = 'high_stress_long_pretraining'\n",
    "tf_root = '/home/jupyter/tf'\n",
    "\n",
    "cfg = meta.ModelConfig.from_json(os.path.join(tf_root, \"models\", code_name, \"model_config.json\"))\n",
    "model = modeling.HS04Model(cfg)\n",
    "data = data_wrangling.MyData()\n",
    "cfg.batch_name = None\n",
    "oral_test = EvalOral(cfg, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating strain_hf_con_hi: 100%|██████████| 59/59 [00:03<00:00, 16.31it/s]\n",
      "Evaluating strain_hf_con_hi: 100%|██████████| 59/59 [00:06<00:00,  9.32it/s]\n",
      "Evaluating strain_hf_inc_hi: 100%|██████████| 59/59 [00:03<00:00, 16.48it/s]\n",
      "Evaluating strain_hf_inc_hi: 100%|██████████| 59/59 [00:06<00:00,  9.27it/s]\n",
      "Evaluating strain_hf_con_li: 100%|██████████| 59/59 [00:03<00:00, 16.29it/s]\n",
      "Evaluating strain_hf_con_li: 100%|██████████| 59/59 [00:06<00:00,  9.35it/s]\n",
      "Evaluating strain_hf_inc_li: 100%|██████████| 59/59 [00:03<00:00, 16.49it/s]\n",
      "Evaluating strain_hf_inc_li: 100%|██████████| 59/59 [00:06<00:00,  9.13it/s]\n",
      "Evaluating strain_lf_con_hi: 100%|██████████| 59/59 [00:03<00:00, 16.72it/s]\n",
      "Evaluating strain_lf_con_hi: 100%|██████████| 59/59 [00:06<00:00,  9.26it/s]\n",
      "Evaluating strain_lf_inc_hi: 100%|██████████| 59/59 [00:03<00:00, 16.08it/s]\n",
      "Evaluating strain_lf_inc_hi: 100%|██████████| 59/59 [00:06<00:00,  9.23it/s]\n",
      "Evaluating strain_lf_con_li: 100%|██████████| 59/59 [00:03<00:00, 16.53it/s]\n",
      "Evaluating strain_lf_con_li: 100%|██████████| 59/59 [00:06<00:00,  9.28it/s]\n",
      "Evaluating strain_lf_inc_li: 100%|██████████| 59/59 [00:03<00:00, 16.33it/s]\n",
      "Evaluating strain_lf_inc_li: 100%|██████████| 59/59 [00:06<00:00,  9.24it/s]\n"
     ]
    }
   ],
   "source": [
    "oral_test.eval('strain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_name</th>\n",
       "      <th>task</th>\n",
       "      <th>testset</th>\n",
       "      <th>epoch</th>\n",
       "      <th>timetick</th>\n",
       "      <th>y</th>\n",
       "      <th>acc</th>\n",
       "      <th>conditional_sse</th>\n",
       "      <th>sse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>pho_sem</td>\n",
       "      <td>strain_hf_con_hi</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>sem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9.137530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>pho_sem</td>\n",
       "      <td>strain_hf_con_hi</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>sem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9.073186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>pho_sem</td>\n",
       "      <td>strain_hf_con_hi</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>sem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9.021046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>pho_sem</td>\n",
       "      <td>strain_hf_con_hi</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>sem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9.024216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>pho_sem</td>\n",
       "      <td>strain_hf_con_hi</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>sem</td>\n",
       "      <td>0.0</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>9.107266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10379</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>sem_pho</td>\n",
       "      <td>strain_lf_inc_li</td>\n",
       "      <td>500</td>\n",
       "      <td>8</td>\n",
       "      <td>pho</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.001732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>sem_pho</td>\n",
       "      <td>strain_lf_inc_li</td>\n",
       "      <td>500</td>\n",
       "      <td>9</td>\n",
       "      <td>pho</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10381</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>sem_pho</td>\n",
       "      <td>strain_lf_inc_li</td>\n",
       "      <td>500</td>\n",
       "      <td>10</td>\n",
       "      <td>pho</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10382</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>sem_pho</td>\n",
       "      <td>strain_lf_inc_li</td>\n",
       "      <td>500</td>\n",
       "      <td>11</td>\n",
       "      <td>pho</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10383</th>\n",
       "      <td>high_stress_long_pretraining</td>\n",
       "      <td>sem_pho</td>\n",
       "      <td>strain_lf_inc_li</td>\n",
       "      <td>500</td>\n",
       "      <td>12</td>\n",
       "      <td>pho</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10384 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          code_name     task           testset  epoch  \\\n",
       "0      high_stress_long_pretraining  pho_sem  strain_hf_con_hi      1   \n",
       "1      high_stress_long_pretraining  pho_sem  strain_hf_con_hi      1   \n",
       "2      high_stress_long_pretraining  pho_sem  strain_hf_con_hi      1   \n",
       "3      high_stress_long_pretraining  pho_sem  strain_hf_con_hi      1   \n",
       "4      high_stress_long_pretraining  pho_sem  strain_hf_con_hi      1   \n",
       "...                             ...      ...               ...    ...   \n",
       "10379  high_stress_long_pretraining  sem_pho  strain_lf_inc_li    500   \n",
       "10380  high_stress_long_pretraining  sem_pho  strain_lf_inc_li    500   \n",
       "10381  high_stress_long_pretraining  sem_pho  strain_lf_inc_li    500   \n",
       "10382  high_stress_long_pretraining  sem_pho  strain_lf_inc_li    500   \n",
       "10383  high_stress_long_pretraining  sem_pho  strain_lf_inc_li    500   \n",
       "\n",
       "       timetick    y  acc  conditional_sse       sse  \n",
       "0             2  sem  0.0             <NA>  9.137530  \n",
       "1             3  sem  0.0             <NA>  9.073186  \n",
       "2             4  sem  0.0             <NA>  9.021046  \n",
       "3             5  sem  0.0             <NA>  9.024216  \n",
       "4             6  sem  0.0             <NA>  9.107266  \n",
       "...         ...  ...  ...              ...       ...  \n",
       "10379         8  pho  1.0         0.001732  0.001732  \n",
       "10380         9  pho  1.0         0.000911  0.000911  \n",
       "10381        10  pho  1.0         0.000594  0.000594  \n",
       "10382        11  pho  1.0         0.000520  0.000520  \n",
       "10383        12  pho  1.0         0.000512  0.000512  \n",
       "\n",
       "[10384 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oral_test.strain_mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_bigquery(csv_file, dataset_name, table_name):\n",
    "    from google.cloud import bigquery\n",
    "    import json, os\n",
    "    import pandas as pd\n",
    "\n",
    "    # Create connection to BQ and push data\n",
    "    client = bigquery.Client()\n",
    "    dataset = client.create_dataset(dataset_name, exists_ok=True)\n",
    "    table_ref = dataset.table(table_name)\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        source_format=bigquery.SourceFormat.CSV, skip_leading_rows=1, autodetect=True\n",
    "    )\n",
    "\n",
    "    with open(csv_file, \"rb\") as f:\n",
    "        job = client.load_table_from_file(f, table_ref, job_config=job_config)\n",
    "\n",
    "    job.result()\n",
    "    print(f\"Loaded {job.output_rows} rows into {dataset_name}:{table_ref.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 207680 rows into triangle_oral:/projects/mimetic-core-276919/datasets/triangle_oral/tables/strain\n"
     ]
    }
   ],
   "source": [
    "csv_to_bigquery(os.path.join(cfg.path['model_folder'], 'eval', 'oral_strain_item_df.csv'), dataset_name=\"triangle_oral\", table_name=\"strain\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
