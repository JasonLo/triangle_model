{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HS04 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The semantic component consisted of the 1,989 semantic features described above. These units were all connected to 50 units in the semantic cleanup apparatus...\n",
    "\n",
    "- *50 sem_cleanup*\n",
    "\n",
    "> The phonological representation consisted of the 200 phonolog-ical units (eight slots of 25 units each), which projected onto a set of 50 phonological cleanup units. These...\n",
    "\n",
    "- *50 pho_cleanup*\n",
    "\n",
    "> The semantic component mapped onto the phonological component via a set of 500 hidden units. There was feedback in both directions. \n",
    "\n",
    "- *500 sem_pho_hidden_units*\n",
    "- *500 pho_sem_hidden_units*\n",
    "\n",
    "> The phonological form of the target word was clamped on the phonological units for 2.66 units of time. Then a target signal was provided for the next 1.33 units of time, in which the network was required to retain the phonological pattern in the absence of external clamping. \n",
    "\n",
    "- *4 output_ticks* \n",
    "\n",
    "> In Harm and Seidenberg (1999), auto-connections were used to give the units a tendency to retain their value but gradually decay. To accomplish the task, the network had to learn enough of the statistical regularities of the representations to prevent this decay. In the current simulations, the idea is the same, but because continuous time units were used, auto-connections were not necessary to provide the units with a tendency to gradually decay; this was part of the unitsâ€™ normal processing dynamics.\n",
    "\n",
    "> HS99: This makes it easier to read weights as correlations between units. Each phonological unit has an auto-connection: a weight set to 0.75 and frozen to that value.\n",
    "\n",
    "- *No auto-connection lock*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The weights that were obtained at the end of the Phase 1 model were frozen and embedded in the larger reading model. Thus, only the connections from orthography to other units were trained in Phase 2. Freezing the weights is not strictly necessary; earlier work (Harm & Seidenberg, 1997) used a process of intermixing in which comprehension trials were used along with reading trials. Weight freezing has the same effect but is simpler and less computationally burdensome to implement. Intermixing is effective and real- istic but adds substantially to network training time.\n",
    "\n",
    "- *Pretraining is necessary (skip for now)*\n",
    "\n",
    "> One set of 500 hidden units mediated the mapping from these orthographic units to semantics...\n",
    "\n",
    "- *500 sem_hidden_units*\n",
    "\n",
    "> ...a second set of 100 hidden units mediated the orth-phon pathway.\n",
    "\n",
    "- *100 pho_hidden_units*\n",
    "\n",
    "> To computationally instantiate the principle that the reading system is under pressure to perform rapidly as well as accurately, we injected error into the semantic and phonological representa- tions early, from time samples 2 to 12. \n",
    "- *11 output_ticks*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modeling individual differences\n",
    "- Simulating ERPs\n",
    "- Link to reliance of OP vs OS\n",
    "- Use equation to model semantic / phonetic input to P/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext lab_black\n",
    "import pickle, os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from importlib import reload\n",
    "import meta, data_wrangling, modeling\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "code_name = \"hs04_ps\"\n",
    "tf_root = \"/home/jupyter/tf\"\n",
    "\n",
    "# Model architechture\n",
    "ort_units = 119  # Phase 2 param (P2)\n",
    "pho_units = 250\n",
    "sem_units = 2446\n",
    "\n",
    "hidden_os_units = 500  # P2\n",
    "hidden_op_units = 100  # P2\n",
    "hidden_ps_units = 500\n",
    "hidden_sp_units = 500\n",
    "\n",
    "pho_cleanup_units = 50\n",
    "sem_cleanup_units = 50\n",
    "\n",
    "pho_noise_level = 0.0  # P3\n",
    "sem_noise_level = 0.0  # P3\n",
    "\n",
    "activation = \"sigmoid\"\n",
    "tau = 1 / 3\n",
    "max_unit_time = 4.0\n",
    "output_ticks = 4\n",
    "\n",
    "# Training\n",
    "sample_name = \"hs04\"\n",
    "rng_seed = 53797\n",
    "learning_rate = 0.005\n",
    "n_mil_sample = 0.1\n",
    "batch_size = 100\n",
    "save_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {}\n",
    "\n",
    "# Load global cfg variables into a dictionary for feeding into ModelConfig()\n",
    "for v in meta.CORE_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "for v in meta.OPTIONAL_CONFIGS:\n",
    "    try:\n",
    "        config_dict[v] = globals()[v]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Construct ModelConfig object\n",
    "cfg = meta.ModelConfig(**config_dict)\n",
    "cfg.save()\n",
    "del config_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(cfg.rng_seed)\n",
    "\n",
    "data = data_wrangling.MyData()\n",
    "model = modeling.HS04Phase1PS(cfg)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=cfg.learning_rate, beta_1=0.0, beta_2=0.999, amsgrad=False\n",
    "    ),\n",
    "    metrics=[\"BinaryAccuracy\", \"mse\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sampling = data_wrangling.FastSampling(cfg, data)\n",
    "\n",
    "history = model.fit(\n",
    "    my_sampling.sample_generator(x=\"pho\", y=\"sem\"),\n",
    "    steps_per_epoch=cfg.steps_per_epoch,\n",
    "    epochs=cfg.total_number_of_epoch,\n",
    "    verbose=1,\n",
    "    # callbacks=[checkpoint],\n",
    ")\n",
    "\n",
    "with open(cfg.path[\"history_pickle\"], \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "model.save(cfg.path[\"save_model_folder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "evaluate.training_history(cfg.path[\"history_pickle\"]).plot_all(\n",
    "    save_file=os.path.join(cfg.path[\"plot_folder\"], \"training_history.html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strain\n",
    "strain = evaluate.strain_eval(cfg, data, model)\n",
    "strain.start_evaluate(\n",
    "    output=os.path.join(cfg.path[\"model_folder\"], \"result_strain_item.csv\")\n",
    ")\n",
    "\n",
    "# Grain\n",
    "grain = evaluate.grain_eval(cfg, data, model)\n",
    "grain.start_evaluate(\n",
    "    output=os.path.join(cfg.path[\"model_folder\"], \"result_grain_item.csv\")\n",
    ")\n",
    "\n",
    "# Taraban\n",
    "taraban = evaluate.taraban_eval(cfg, data, model)\n",
    "taraban.start_evaluate(\n",
    "    output=os.path.join(cfg.path[\"model_folder\"], \"result_taraban_item.csv\")\n",
    ")\n",
    "\n",
    "# Glushko\n",
    "glushko = evaluate.glushko_eval(cfg, data, model)\n",
    "glushko.start_evaluate(\n",
    "    output=os.path.join(cfg.path[\"model_folder\"], \"result_glushko_item.csv\")\n",
    ")\n",
    "\n",
    "# Create vis class for visualisation\n",
    "vis = evaluate.vis(cfg.path[\"model_folder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strain and Grain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg = vis.plot_dev_interactive(\"acc\", [\"strain\", \"grain\"]).properties(\n",
    "    title=\"Accuracy in Strain and Grain\"\n",
    ")\n",
    "sg.save(os.path.join(cfg.path[\"plot_folder\"], \"development_sg.html\"))\n",
    "sg.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grain response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = vis.plot_dev_interactive(\"acc_small_grain\", exp=[\"grain\"]).properties(\n",
    "    title=\"Small Grain Response\"\n",
    ")\n",
    "large = vis.plot_dev_interactive(\"acc_large_grain\", exp=[\"grain\"]).properties(\n",
    "    title=\"Large Grain Response\"\n",
    ")\n",
    "grain_plot = (small | large).properties(\n",
    "    title=\"Accuracy of Grain by response and condition\"\n",
    ")\n",
    "grain_plot.save(\n",
    "    os.path.join(cfg.path[\"plot_folder\"], \"development_grain_by_response.html\")\n",
    ")\n",
    "grain_plot.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
