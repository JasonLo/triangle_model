{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, target):\n",
    "    return 1 * np.array(output == target)\n",
    "\n",
    "\n",
    "def get_slot_sse(output, target, slot_len=25):\n",
    "    \"\"\" Get slot based SSE    \n",
    "    \"\"\"\n",
    "    segments = target.shape[-1] / slot_len\n",
    "    return np.sum(\n",
    "        np.array_split(np.square(output - target), segments, axis=-1), axis=-1\n",
    "    )\n",
    "\n",
    "\n",
    "def gen_pkey(p_file=\"../common/patterns/mappingv2.txt\"):\n",
    "    # read phonological patterns from the mapping file\n",
    "    # See Harm & Seidenberg PDF file\n",
    "    mapping = pd.read_table(p_file, header=None, delim_whitespace=True)\n",
    "    m_dict = mapping.set_index(0).T.to_dict(\"list\")\n",
    "    return m_dict\n",
    "\n",
    "\n",
    "def get_pronunciation_fast(act, phon_key):\n",
    "    phonemes = list(phon_key.keys())\n",
    "    act10 = np.tile([v for k, v in phon_key.items()], 10)\n",
    "\n",
    "    d = np.abs(act10 - act)\n",
    "    d_mat = np.reshape(d, (38, 10, 25))\n",
    "    sumd_mat = np.squeeze(np.sum(d_mat, 2))\n",
    "    map_idx = np.argmin(sumd_mat, 0)\n",
    "    out = str()\n",
    "    for x in map_idx:\n",
    "        out += phonemes[x]\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_all_pronunciations_fast(act, phon_key):\n",
    "    return np.apply_along_axis(get_pronunciation_fast, 1, act, phon_key)\n",
    "\n",
    "\n",
    "p_key = gen_pkey()\n",
    "df_strain = pd.read_csv(\"../common/input/df_strain.csv\", index_col=0)\n",
    "y_strain = np.load(\"../common/input/y_strain.npz\")[\"data\"]\n",
    "y_true = get_all_pronunciations_fast(y_strain, p_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New function for getting output matrix from JZ export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model_id):\n",
    "    \"\"\" Fast way to read data with numpy\n",
    "    \"\"\"\n",
    "    a = np.genfromtxt(f\"sse/{model_id}_literal.txt\", usecols=1)\n",
    "    return a.reshape((160, 250))\n",
    "\n",
    "\n",
    "# Safer way to read data with pandas with first key lookup (for checking, PASS)\n",
    "\n",
    "# df = pd.read_csv(f\"sse/{model_id}_literal.txt\", delimiter=\"\\t\", header=None)\n",
    "# df.columns = [\"item\", \"output\", \"trash\"]\n",
    "# item_dict = {k: df.loc[df.item == k, \"output\"] for k in df.item.unique()}\n",
    "\n",
    "# c = np.empty((160, 250))\n",
    "# for i, key in enumerate(df.item.unique()):\n",
    "#     c[i,] = item_dict[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all output from JZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"64195158\", \"64195283\", \"64195408\", \"64195658\"]\n",
    "noise_level = [0, 1, 2, 4]\n",
    "\n",
    "all_out = pd.DataFrame()\n",
    "\n",
    "for i, m in enumerate(model_list):\n",
    "\n",
    "    item_eval = df_strain\n",
    "    x = get_output(m)\n",
    "    y_pred = get_all_pronunciations_fast(x, p_key)\n",
    "\n",
    "    item_eval[\"output\"] = y_pred\n",
    "    item_eval[\"acc\"] = get_accuracy(y_pred, y_true)\n",
    "\n",
    "    # SSE related\n",
    "    slot_sse = get_slot_sse(x, y_strain)\n",
    "    item_eval[\"sse_slot1\"] = slot_sse[0]\n",
    "    item_eval[\"sse_slot2\"] = slot_sse[1]\n",
    "    item_eval[\"sse_slot3\"] = slot_sse[2]\n",
    "    item_eval[\"sse_slot4\"] = slot_sse[3]\n",
    "    item_eval[\"sse_slot5\"] = slot_sse[4]\n",
    "    item_eval[\"sse_slot6\"] = slot_sse[5]\n",
    "    item_eval[\"sse_slot7\"] = slot_sse[6]\n",
    "    item_eval[\"sse_slot8\"] = slot_sse[7]\n",
    "    item_eval[\"sse_slot9\"] = slot_sse[8]\n",
    "    item_eval[\"sse_slot10\"] = slot_sse[9]\n",
    "    item_eval[\"sse\"] = slot_sse.sum(axis=0)\n",
    "\n",
    "    item_eval[\"model\"] = m\n",
    "    item_eval[\"noise\"] = noise_level[i]\n",
    "\n",
    "    all_out = pd.concat([all_out, item_eval])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_out.to_csv(\"sse_parsed.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
